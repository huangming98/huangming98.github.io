<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Mathematical Foundation of Reinforcement Learning - Algorithms (model-based &amp; non-incremental)</title>
    <url>/2024/08/08/Mathmatical-Foundation-of-Reinforcement-Learning-Algorithms-model-based-non-incremental/</url>
    <content><![CDATA[<p>[toc]</p>
<h2 id="Reference-Sources"><a href="#Reference-Sources" class="headerlink" title="Reference Sources:"></a>Reference Sources:</h2><ol>
<li><a href="https://github.com/MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning"><em>Shiyu Zhao</em>. 《Mathematical Foundation of Reinforcement Learning》Chapter 4-5</a>.</li>
<li><a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#">OpenAI Spinning Up</a></li>
<li><a href="https://lyk-love.cn/tags/reinforcement-learning/">Lu yukuan’s notes</a></li>
</ol>
<h2 id="4-Value-iteration-and-policy-iteration-dynamic-programming-algorithms"><a href="#4-Value-iteration-and-policy-iteration-dynamic-programming-algorithms" class="headerlink" title="4. Value iteration and policy iteration(dynamic programming algorithms)"></a>4. Value iteration and policy iteration(dynamic programming algorithms)</h2><h3 id="4-1-Value-iteration"><a href="#4-1-Value-iteration" class="headerlink" title="4.1 Value iteration"></a>4.1 Value iteration</h3><div align="center"><img src="/2024/08/08/Mathmatical-Foundation-of-Reinforcement-Learning-Algorithms-model-based-non-incremental/grid%20world.jpg" width="50%" alt="Figure 1.2: The grid world example is used throughout the book."></div>


<p>The value iteration algorithm is <strong>exactly</strong> the algorithm suggested by the contraction mapping theorem for solving the Bellman optimality equation (BOE).</p>
<p>The BOE Solution algorithm is</p>
<script type="math/tex; mode=display">
\begin{equation} 
v_{k+1}=\max _{\pi \in \Pi}\left(r_\pi+\gamma P_\pi v_k\right), \quad k=0,1,2, \ldots
\end{equation}</script><p>It is guaranteed  that $v_k$ and $\pi_k$ converge to the optimal state value and an optimal policy as $k \rightarrow \infty$, respectively.</p>
<p>value iteration is an iterative algorithm. Each iteration has two steps.<br><strong>Step 1</strong>: policy update($v_k$ → $\pi_{k+1}$). This step is to solve</p>
<script type="math/tex; mode=display">
\begin{equation} 
\pi_{k+1}=\arg \max _\pi\left(r_\pi+\gamma P_\pi v_k\right)
\end{equation}</script><p>where $v_k$ is obtained in the previous iteration.$v_0$ can be random setting.</p>
<p><strong>Step 2</strong>: value update($v_k$,$\pi_{k+1}$ → $v_{k+1}$). </p>
<script type="math/tex; mode=display">
\begin{equation} 
v_{k+1}=r_{\pi_{k+1}}+\gamma P_{\pi_{k+1}} v_k,
\end{equation}</script><p>where $v_{k+1}$ will be used in the next iteration.</p>
<p>The value iteration algorithm introduced above is in a matrix-vector form. It’s useful for understanding the core idea of the algorithm. To implement this algorithm, we need to further examine its elementwise form.</p>
<hr>
<p><strong>Step 1</strong>: Policy update(elementwise form)</p>
<p>The elementwise form of $\pi_{k+1}$ is</p>
<script type="math/tex; mode=display">
\pi_{k+1}(s)=\arg \max _\pi \sum_a \pi(a \mid s) \underbrace{\left(\sum_r p(r \mid s, a) r+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right) v_k\left(s^{\prime}\right)\right)}_{q_k(s, a)}, \quad s \in \mathcal{S}</script><p>The optimal policy solving the above optimization problem is</p>
<script type="math/tex; mode=display">
\pi_{k+1}(a \mid s)=\left\{\begin{array}{cc}
1 & a=a_k^*(s) \\
0 & a \neq a_k^*(s)
\end{array}\right.</script><p>where $a_k^<em>(s)=\arg \max _a q_k(a, s).<br>\pi_{k+1}$ is called a <em>*greedy</em></em> policy, since it simply selects the greatest q-value.</p>
<p><strong>Step 2</strong>: Value update(elementwise form)</p>
<p>The elementwise form of $v_{k+1}$ is</p>
<script type="math/tex; mode=display">
v_{k+1}(s)=\sum_a \pi_{k+1}(a \mid s) \underbrace{\left(\sum_r p(r \mid s, a) r+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right) v_k\left(s^{\prime}\right)\right)}_{q_k(s, a)}, \quad s \in \mathcal{S}</script><p>Since $\pi_{k+1}$ is greedy, the above equation is simply</p>
<script type="math/tex; mode=display">
v_{k+1}(s)=\max _a q_k(a, s)</script><p><u><strong>Procedure summary:</strong></u></p>
<p><strong>$v_k(s) \rightarrow q_k(s, a) \rightarrow$ new greedy policy $\pi_{k+1}(a \mid s) \rightarrow$ new value $v_{k+1}=\max _a q_k(s, a)$</strong></p>
<div align="center"><img src="/2024/08/08/Mathmatical-Foundation-of-Reinforcement-Learning-Algorithms-model-based-non-incremental/2024-04-19-14-56-27.png" width="90%" alt="Algorithm 4.1: Value iteration algorithm"></div>


<p>Note: Although $v_k$ eventually converges to the optimal state value , it is <strong>not</strong> a state value, it doesn’t ensure to satisfy the Bellman equation($ v_k=r_{\pi_{k+1}}+\gamma P_{\pi_{k+1}} v_k$ or $v_k=r_{\pi_{k}}+\gamma P_{\pi_{k}} v_k$). It is merely an intermediate value generated by the algorithm. In addition, since $v_k$ is not a state value,$q_k$ is not an action value. <strong> $v_k$ can be seen as State value that has not yet converged, which be calculated only once</strong>.</p>
<h3 id="4-2-Policy-iteration"><a href="#4-2-Policy-iteration" class="headerlink" title="4.2 Policy iteration"></a>4.2 Policy iteration</h3><p>Policy iteration is an iterative algorithm. Each iteration has two steps.</p>
<p>Given a random initial policy $\pi_0$, in each policy iteration we do</p>
<ol>
<li><p><strong>policy evaluation (PE)</strong><br>That is to solve the following Bellman equation:</p>
<script type="math/tex; mode=display">
\begin{equation} 
v_{\pi_k}=r_{\pi_k}+\gamma P_{\pi_k} v_{\pi_k}
\end{equation}</script><p><strong>$v_{\pi_k}$ is a state value function</strong>. So we need to get the state values for <strong>all states</strong>, not for one specific state, in PE.</p>
</li>
<li><p><strong>policy improvement (PI)</strong>:</p>
<script type="math/tex; mode=display">
\pi_{k+1}=\arg \max _\pi\left(r_\pi+\gamma P_\pi v_{\pi_k}\right)</script><p>The <em>maximization</em> is componentwise.</p>
</li>
</ol>
<p>The policy iteration algorithm leads to a sequence</p>
<script type="math/tex; mode=display">
\pi_0 \stackrel{P E}{\longrightarrow} v_{\pi_0} \stackrel{P I}{\longrightarrow} \pi_1 \stackrel{P E}{\longrightarrow} v_{\pi_1} \stackrel{P I}{\longrightarrow} \pi_2 \stackrel{P E}{\longrightarrow} v_{\pi_2} \stackrel{P I}{\longrightarrow} \ldots \nonumber</script><p>Interestingly, policy iteration is an iterative algorithm with another iterative algorithm  embedded in the policy evaluation step. </p>
<p>In theory, this embedded iterative algorithm requires an infinite number of steps (that is, $j \rightarrow \infty$ ) to converge to the true state value $v_{\pi_k}$. This is, however, <strong>impossible to realize</strong>. In practice, the iterative process terminates when a certain criterion is satisfied.</p>
<p><strong>Theorem</strong> (Convergence of policy iteration). The state value sequence $\left\{v_{\pi_k}\right\}_{k=0}^{\infty}$ generated by the policy iteration algorithm <strong>converges</strong> to the optimal state value $v^*$. As a result, the policy sequence $\left\{\pi_k\right\}_{k=0}^{\infty}$ converges to an optimal policy.</p>
<p><strong>Elementwise form and implementation</strong></p>
<p><strong>Step 1</strong>: Policy evaluation(elementwise form)<br>The elementwise form is:</p>
<script type="math/tex; mode=display">
v_{\pi_k}^{(j+1)}(s)=\sum_a \pi_k(a \mid s)\left(\sum_r p(r \mid s, a) r+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right) v_{\pi_k}^{(j)}\left(s^{\prime}\right)\right), \quad s \in \mathcal{S}, \nonumber</script><p>Stop when $j \rightarrow \infty$ or $j$ is sufficiently large or $\left|v_{\pi_k}^{(j+1)}-v_{\pi_k}^{(j)}\right|$ is sufficiently small.</p>
<p><strong>Step 2</strong>: Policy improvement(elementwise form)<br>Recalling the solution to do policy improvement with matrix-vector form: $\pi_{k+1}=\arg \max _\pi\left(r_\pi+\gamma P_\pi v_{\pi_k}\right)$</p>
<p>The elementwise form is:</p>
<script type="math/tex; mode=display">
\pi_{k+1}(s)=\arg \max _\pi \sum_a \pi(a \mid s) \underbrace{\left(\sum_r p(r \mid s, a) r+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right) v_{\pi_k}\left(s^{\prime}\right)\right)}_{q_{\pi_k}(s, a)}, \quad s \in \mathcal{S} . \nonumber</script><p>Here, $q_{\pi_k}(s, a)$ is the action value under policy $\pi_k$. Let</p>
<script type="math/tex; mode=display">
a_k^*(s)=\arg \max _a q_{\pi_k}(a, s) \nonumber</script><p>Then, the greedy policy is</p>
<script type="math/tex; mode=display">
\pi_{k+1}(a \mid s)= \begin{cases}1 & a=a_k^*(s), \\ 0 & a \neq a_k^*(s) .\end{cases} \nonumber</script><p><u><strong>Procedure summary:</strong></u><br><img src="/2024/08/08/Mathmatical-Foundation-of-Reinforcement-Learning-Algorithms-model-based-non-incremental/2024-04-19-15-30-58.png" alt="Algorithm 4.2: Policy iteration algorithm."></p>
<h3 id="4-3-Truncated-policy-iteration"><a href="#4-3-Truncated-policy-iteration" class="headerlink" title="4.3 Truncated policy iteration"></a>4.3 Truncated policy iteration</h3><p>We will see that the value iteration and policy iteration algorithms are two special cases of the truncated policy iteration algorithm.</p>
<p>The above steps of the two algorithms can be illustrated as</p>
<p>Policy iteration: $\pi_0 \xrightarrow{P E} v_{\pi_0} \xrightarrow{P I} \pi_1 \xrightarrow{P E} v_{\pi_1} \xrightarrow{P I} \pi_2 \xrightarrow{P E} v_{\pi_2} \xrightarrow{P I} \ldots$.</p>
<p>Value iteration: $\quad \quad  \quad  v_0 \xrightarrow{P U} \pi_1^{\prime} \xrightarrow{V U} v_1 \xrightarrow{P U} \pi_2^{\prime} \xrightarrow{V U} v_2 \xrightarrow{P U} \ldots$</p>
<p>It can be seen that the procedures of the two algorithms are very similar.<br>We examine their value steps more closely to see the difference between the two algorithms. In particular, let both algorithms start from the same initial condition: $v_0=v_{\pi_0}$. </p>
<p><img src="/2024/08/08/Mathmatical-Foundation-of-Reinforcement-Learning-Algorithms-model-based-non-incremental/2024-04-19-15-35-44.png" alt="Table 4.6: A comparison between the implementation steps of policy iteration and value iteration."></p>
<p>The procedures of the two algorithms are listed in Table 4.6. </p>
<ul>
<li><p>In the first three steps, the two algorithms generate the same results since $v_0=v_{\pi_0}$. <strong>They become different in the fourth step</strong>. </p>
</li>
<li><p>During the fourth step, the value iteration algorithm executes $v_1=r_{\pi_1}+\gamma P_{\pi_1} v_0$, which is a <u> one-step calculation </u> , whereas the policy iteration algorithm solves $v_{\pi_1}=r_{\pi_1}+\gamma P_{\pi_1} v_{\pi_1}$, which requires <u> an infinite number of iterations</u>.</p>
</li>
</ul>
<p>If we explicitly write out the iterative process for solving $v_{\pi_1}=r_{\pi_1}+\gamma P_{\pi_1} v_{\pi_1}$ in the fourth step, everything becomes clear. By letting $v_{\pi_1}^{(0)}=v_0$, we have</p>
<p><img src="/2024/08/08/Mathmatical-Foundation-of-Reinforcement-Learning-Algorithms-model-based-non-incremental/2024-04-19-15-38-37.png" alt="Comparisons"></p>
<p>The following observations can be obtained from the above process.</p>
<ul>
<li>If the iteration is run <em>only once</em>, then $v_{\pi_1}^{(1)}$ is actually $v_1$, as calculated in the value iteration algorithm.</li>
<li>iteration is run <em>an infinite number of times</em>, then $v_{\pi_1}^{(\infty)}$ is actually $v_{\pi_1}$, as calculated in the policy iteration algorithm.</li>
<li>If the iteration is run <em>a finite number of times (denoted as $j_{\text {truncate }}$ )</em>, then such an algorithm is called <strong>truncated policy iteration</strong>. It is called truncated because the remaining iterations from $j_{\text {truncate }}$ to $\infty$ are truncated.</li>
</ul>
<p><strong>As a result, the value iteration and policy iteration algorithms can be viewed as two extreme cases of the truncated policy iteration algorithm: value iteration terminates at $j_{\text {truncate }}=1$, and policy iteration terminates at $j_{\text {truncate }}=\infty$.</strong> </p>
<p>It should be noted that, although the above comparison is illustrative, it is based on the condition that $v_{\pi_1}^{(0)}=v_0=v_{\pi_0}$. The two algorithms cannot be directly compared without this condition.</p>
<p><strong>Truncated policy iteration algorithm</strong></p>
<p>In a nutshell, the truncated policy iteration algorithm is <strong>the same as the policy iteration algorithm except that it merely runs a finite number of iterations in the policy evaluation step.</strong> </p>
<p><img src="/2024/08/08/Mathmatical-Foundation-of-Reinforcement-Learning-Algorithms-model-based-non-incremental/2024-04-19-15-40-35.png" alt="Algorithm 4.3: Truncated policy iteration algorithm."></p>
<p><strong>Lemma: Value Improvement</strong></p>
<p>Consider the iterative algorithm for solving the policy evaluation step:</p>
<script type="math/tex; mode=display">
v_{\pi_k}^{(j+1)}=r_{\pi_k}+\gamma P_{\pi_k} v_{\pi_k}^{(j)}, \quad j=0,1,2, \ldots</script><p>If the initial guess is selected as $v_{\pi_k}^{(0)}=v_{\pi_{k-1}}$, it holds that</p>
<script type="math/tex; mode=display">
v_{\pi_k}^{(j+1)} \geq v_{\pi_k}^{(j)}</script><p>for every $j=0,1,2, \ldots$.</p>
<p><strong>Proof</strong>:<br>First, since $v_{\pi_k}^{(j)}=r_{\pi_k}+\gamma P_{\pi_k} v_{\pi_k}^{(j-1)}$ and $v_{\pi_k}^{(j+1)}=r_{\pi_k}+\gamma P_{\pi_k} v_{\pi_k}^{(j)}$, we have</p>
<script type="math/tex; mode=display">
v_{\pi_k}^{(j+1)}-v_{\pi_k}^{(j)}=\gamma P_{\pi_k}\left(v_{\pi_k}^{(j)}-v_{\pi_k}^{(j-1)}\right)=\cdots=\gamma^j P_{\pi_k}^j\left(v_{\pi_k}^{(1)}-v_{\pi_k}^{(0)}\right)</script><p>Second, since $v_{\pi_k}^{(0)}=v_{\pi_{k-1}}$, we have</p>
<script type="math/tex; mode=display">
v_{\pi_k}^{(1)}=r_{\pi_k}+\gamma P_{\pi_k} v_{\pi_k}^{(0)}=r_{\pi_k}+\gamma P_{\pi_k} v_{\pi_{k-1}} \geq r_{\pi_{k-1}}+\gamma P_{\pi_{k-1}} v_{\pi_{k-1}}=v_{\pi_{k-1}}=v_{\pi_k}^{(0)},</script><p>where the inequality is due to $\pi_k=\arg \max _\pi\left(r_\pi+\gamma P_\pi v_{\pi_{k-1}}\right)$. Substituting $v_{\pi_k}^{(1)} \geq v_{\pi_k}^{(0)}$ into (4.5) yields $v_{\pi_k}^{(j+1)} \geq v_{\pi_k}^{(j)}$.</p>
<p><strong>Relationships between the three algorithms</strong></p>
<div align="center"><img src="/2024/08/08/Mathmatical-Foundation-of-Reinforcement-Learning-Algorithms-model-based-non-incremental/2024-04-19-15-42-50.png" width="60%" alt="Figure 4.5: An illustration of the relationships between the value iteration, policy iteration, and truncated policy iteration algorithms."></div>


<p>Define $||v_k-v^\ast||$ as the state value error at time $k$. The stop criterion is $||v_k-v^\ast||&lt;0.01$, Truncated policy iteration-$x$.<br><img src="/2024/08/08/Mathmatical-Foundation-of-Reinforcement-Learning-Algorithms-model-based-non-incremental/2024-04-22-16-46-51.png" alt="Truncated policy iteration- Example"></p>
<ul>
<li>The greater the value of $x$ is, the faster the value estimate converges.</li>
<li>However, the benefit of increasing $x$ drops quickly when $x$ is large.</li>
<li>In practice, run a few number of iterations in the policy evaluation step.</li>
</ul>
<h3 id="4-4-Summary"><a href="#4-4-Summary" class="headerlink" title="4.4 Summary"></a>4.4 Summary</h3><ul>
<li><p>1 <strong>Value iteration</strong>: The value iteration algorithm is the same as the algorithm suggested by the contraction mapping theorem for solving the Bellman optimality equation. It can be decomposed into two steps: <strong>policy update and value update.</strong></p>
</li>
<li><p>2 <strong>Policy iteration</strong>: The policy iteration algorithm is slightly more complicated than the value iteration algorithm. It also contains two steps: <strong>policy evaluation and policy improvement</strong>.In the policy evaluation step , an iterative algorithm is required to solve the Bellman equation of the current policy.</p>
</li>
<li><p>3 <strong>Truncated policy iteration</strong>: The value iteration and policy iteration algorithms can be viewed as <strong>two extreme cases</strong> of the truncated policy iteration algorithm.  The intermediate values generated by the truncated policy iteration algorithm are not state values? Only if we run an infinite number of iterations in the policy evaluation step, can we obtain true state values. If we run a nite number of iterations, we can only obtain approximates of the true state values.</p>
</li>
</ul>
<p>A common property of the three algorithms is that every iteration has two steps. One step is to update the value, and the other step is to update the policy. <strong>The idea of interaction between value and policy updates</strong> widely exists in reinforcement learning algorithms. This idea is also called <strong>generalized policy iteration</strong>.</p>
<p>Generalized policy iteration is not a specific algorithm. Instead, it refers to the general idea of the interaction between value and policy updates. This idea is rooted in the policy iteration algorithm. Most of the reinforcement learning algorithms introduced in this book fall into the scope of generalized policy iteration.</p>
<p>Finally, the algorithms introduced in this chapter <strong>require the system model</strong>. Starting in Chapter 5, we will study model-free reinforcement learning algorithms. We will see that the model-free can be obtained by extending the algorithms introduced in this chapter.</p>
<p>Q: What are model-based and model-free reinforcement learning?<br>A: Although the algorithms introduced in this chapter can find optimal policies, they are usually called <strong>dynamic programming algorithms</strong> rather than reinforcement learning algorithms because they require the system model. Reinforcement learning algorithms can be classified into two categories: model-based and model-free. Here, <strong>model-based does not refer to the requirement of the system model. Instead, model-based reinforcement learning uses data to estimate the system model and uses this model during the learning process.</strong> By contrast, model-free reinforcement learning does not involve model estimation during the learning process.</p>
<h2 id="5-Monte-Carlo-Methods"><a href="#5-Monte-Carlo-Methods" class="headerlink" title="5 Monte Carlo Methods"></a>5 Monte Carlo Methods</h2><p><em>If we do not have a <strong>model</strong>, we must have some <strong>data</strong>. If we do not have data, we must have a model. If we have neither, then we are not able to find optimal policies.</em> The <strong>data</strong> in reinforcement learning usually refers to <strong>the agent’s interaction experiences with the environment.</strong><br>Model-based reinforcement learning algorithms that need do presume system models.<br>Model-free reinforcement learning algorithms that do not presume system models.</p>
<h3 id="5-1-Mean-estimation-problem"><a href="#5-1-Mean-estimation-problem" class="headerlink" title="5.1 Mean estimation problem"></a>5.1 Mean estimation problem</h3><p>we start this chapter by introducing the mean estimation problem, where the expected value of a random variable is estimated from some samples. Understanding this problem is crucial for understanding the fundamental idea of <u> learning from data </u>.<br><strong>Why we care about the mean estimation problem? It is simply because state and action values are both defined as the means of returns. Estimating a state or action value is actually a mean estimation problem.</strong></p>
<p>Consider a random variable $X$ that can take values from a finite set of real numbers denoted as $\mathcal{X}$, suppose that our task is to calculate the mean or expected value of $X$, i.e., $\mathbb{E}[X]$.</p>
<p>Two approaches can be used to calculate $\mathbb{E}[X]$.</p>
<p><strong>Model based case</strong></p>
<p>The first approach is model-based. Here, the model refers to the probability distribution of $X$. If the model is known, then the mean can be directly calculated based on the definition of the expected value:</p>
<script type="math/tex; mode=display">
\mathbb{E}[X]=\sum_{x \in \mathcal{X}} p(x) x</script><p><strong>Model free case</strong></p>
<p>The second approach is model-free. When the probability distribution (i.e., the model) of $X$ is unknown, suppose that we have some samples $\left\{x_1, x_2, \ldots, x_n\right\}$ of $X$. Then, the mean can be approximated as</p>
<script type="math/tex; mode=display">
\mathbb{E}[X] \approx \bar{x}=\frac{1}{n} \sum_{j=1}^n x_j .</script><p>When $n$ is small, this approximation may not be accurate. However, as $n$ increases, the approximation becomes increasingly accurate. When $n \rightarrow \infty$, we have $\bar{x} \rightarrow \mathbb{E}[X]$.</p>
<p>This is guaranteed by the <em>law of large numbers</em>: the average of a large number of samples is close to the expected value.</p>
<p>It is worth mentioning that the samples used for mean estimation must be <strong>independent and identically distributed(i.i.d. or iid)</strong>. Otherwise, if the sampling values correlate, it may be impossible to correctly estimate the expected value. An extreme case is that all the sampling values are the same as the <em>r</em> st one, whatever the <em>r</em> st one is. In this case, the average of the samples is always equal to the <em>r</em> st sample, no matter how many samples we use.</p>
<p><strong>Law of large numbers</strong><br>For a random variable $X$, suppose that $\left\{x_i\right\}_{i=1}^n$ are some i.i.d. samples. Let $\bar{x}=$ $\frac{1}{n} \sum_{i=1}^n x_i$ be the average of the samples. Then,</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbb{E}[\bar{x}] & =\mathbb{E}[X], \\
\operatorname{var}[\bar{x}] & =\frac{1}{n} \operatorname{var}[X] .
\end{aligned}</script><p>The above two equations indicate that $\bar{x}$ is an unbiased estimate of $\mathbb{E}[X]$, and its variance decreases to zero as $n$ increases to infinity.<br>The proof is given below.</p>
<p>First, $\mathbb{E}[\bar{x}]=\mathbb{E}\left[\sum_{i=1}^n x_i / n\right]=\sum_{i=1}^n \mathbb{E}\left[x_i\right] / n=\mathbb{E}[X]$, where the last equability is due to the fact that the samples are identically distributed (that is, $\mathbb{E}\left[x_i\right]=\mathbb{E}[X]$ ).</p>
<p>Second, $\operatorname{var}(\bar{x})=\operatorname{var}\left[\sum_{i=1}^n x_i / n\right]=\sum_{i=1}^n \operatorname{var}\left[x_i\right] / n^2=(n \cdot \operatorname{var}[X]) / n^2=$ $\operatorname{var}[X] / n$, where the second equality is due to the fact that the samples are independent, and the third equability is a result of the samples being identically distributed (that is, $\operatorname{var}\left[x_i\right]=\operatorname{var}[X]$ ).</p>
<h3 id="5-2-MC-Basic-The-simplest-MC-based-algorithm"><a href="#5-2-MC-Basic-The-simplest-MC-based-algorithm" class="headerlink" title="5.2 MC Basic: The simplest MC-based algorithm"></a>5.2 MC Basic: The simplest MC-based algorithm</h3><h4 id="5-2-1-introduction"><a href="#5-2-1-introduction" class="headerlink" title="5.2.1 introduction"></a>5.2.1 introduction</h4><p>Recalling that the policy iteration algorithm has two steps in each iteration:</p>
<ol>
<li>Policy evaluation(PE): $v_{\pi_k}=r_{\pi_k}+\gamma P_{\pi_k} v_{\pi_k}$.</li>
<li>Policy improvement(PI): $\pi_{k+1}=\arg \max _\pi\left(r_\pi+\gamma P_\pi v_{\pi_k}\right)$.</li>
</ol>
<p>The PE step is calculated through solving the Bellman equation.</p>
<p>The elementwise form of the (PE,PI) step are:</p>
<script type="math/tex; mode=display">
\begin{aligned}
v_{\pi_{k}} & =\sum_a \pi(a \mid s)\left[\sum_r p(r \mid s, a) r+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right) v_{\pi_k}\left(s^{\prime}\right)\right] \\
& =\sum_a \pi(a \mid s) q_{\pi_k}(s, a), \quad s \in \mathcal{S}
\end{aligned}</script><script type="math/tex; mode=display">
\begin{aligned}
\pi_{k+1}(s) & =\arg \max _\pi \sum_a \pi(a \mid s)\left[\sum_r p(r \mid s, a) r+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right) v_{\pi_k}\left(s^{\prime}\right)\right] \\
& =\arg \max _\pi \sum_a \pi(a \mid s) q_{\pi_k}(s, a), \quad s \in \mathcal{S}
\end{aligned}</script><p><strong>Under the model free case —— The calculated key is $q_{\pi_k}(s, a)$ ! So how can we calculate $q_{\pi_k}(s, a)$?</strong></p>
<p><strong>Model based case</strong></p>
<p>The policy iteration algorithm is a model based algorithm, the model (or dynamic) is given.  a MDP is composed of two parts:</p>
<ol>
<li>State transition probability: $p\left(s^{\prime} \mid s, a\right)$. </li>
<li>Reward probability: $p(r \mid s, a)$.</li>
</ol>
<p>Thus, we can calculate $q_{\pi_k}(s, a)$ via following equation</p>
<script type="math/tex; mode=display">
q_{\pi_k}(s, a)=\sum_r p(r \mid s, a) r+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right) v_{\pi_k}\left(s^{\prime}\right)</script><p>where  $p\left(s^{\prime} \mid s, a\right)$ and  $p(r \mid s, a)$ are given, and every $v_{\pi_k}\left(s^{\prime}\right)$ is calculated in the PE step.</p>
<p><strong>Model free case</strong></p>
<p>But what if we don’t know the model, Please recalling the definition of action value.</p>
<script type="math/tex; mode=display">
\begin{equation} 
q_\pi(s, a) \triangleq \mathbb{E}\left[G_t \mid S_t=s, A_t=a\right]
\end{equation}</script><p>We can  to calculate $q_{\pi_k}(s, a)$ <strong>based on data</strong> (samples or experiences). This is the key idea of MCRL: If we don’t have a model, we estimate one based on data (or experience).</p>
<p><strong>The procedure of Monte Carlo estimation of action values</strong></p>
<p>Starting from $(s, a)$, following policy $\pi_k$, generate an episode. The return of this episode is $g(s, a)$. $g(s, a)$ is a sample of $G_t$ in</p>
<script type="math/tex; mode=display">
q_{\pi_k}(s, a)=\mathbb{E}\left[G_t \mid S_t=s, A_t=a\right]</script><p>Suppose we have a set of episodes and hence $\left\{g^{(j)}(s, a)\right\}$. Then,</p>
<script type="math/tex; mode=display">
\textcolor{red} {q_{\pi_k}(s, a)=\mathbb{E}\left[G_t \mid S_t=s, A_t=a\right] \approx \frac{1}{N} \sum_{i=1}^N g^{(i)}(s, a)}</script><p>The idea of estimate the mean based on data is called Monte Carlo estimation. This is why this method is called “<strong>MC (Monte Carlo)</strong> Basic”.</p>
<h4 id="5-2-2-The-MC-Basic-algorithm"><a href="#5-2-2-The-MC-Basic-algorithm" class="headerlink" title="5.2.2 The MC Basic algorithm"></a>5.2.2 The MC Basic algorithm</h4><p>The MC Basic algorithm is exactly the same as the policy iteration algorithm except: In policy evaluation (PI), <strong>we don’t solve $v_{\pi_k}(s)$, instead we estimate $q_{\pi_k}(s, a)$ directly</strong>.</p>
<p>Why we don’t compute $v_{\pi_k}(s)$? Because if we calculate the state value in PE, in PI step we still need to calculate action value. So we can directly calculate action value in PE.</p>
<p><img src="/2024/08/08/Mathmatical-Foundation-of-Reinforcement-Learning-Algorithms-model-based-non-incremental/2024-04-23-14-11-53.png" alt="Algorithm 5.1: MC Basic (a model-free variant of policy iteration)"></p>
<p>Since policy iteration is convergent, MC Basic is also convergent when given sufficient samples.However, the MC Basic algorithm is not practical due its <strong>low sample efficiency</strong> (we need to calculate $N$ episodes for every $q_{\pi_k}(s, a)$, it is a large work time ).</p>
<p><strong>A simple example</strong></p>
<p>An initial policy is shown in the figure (as you can see, it’s a deterministic policy). Use MC Basic to find the optimal policy. The env setting is: </p>
<script type="math/tex; mode=display">
r_{\text {boundary }}=-1, r_{\text {forbidden }}=-1, r_{\text {target }}=1, \gamma=0.9 .</script><div align="center"><img src="/2024/08/08/Mathmatical-Foundation-of-Reinforcement-Learning-Algorithms-model-based-non-incremental/2024-04-23-14-21-24.png" width="50%" alt="Figure 5.3: An example for illustrating the MC Basic algorithm"></div>

<p>Outline: given the current policy $\pi_k$, in each iteration:</p>
<ol>
<li>policy evaluation: calculate $q_{\pi_k}(s, a)$. Sicne there’re 9 states and 5 actions. We need to calculate 45 state-action pairs.</li>
<li>policy improvement: select the greedy action</li>
</ol>
<script type="math/tex; mode=display">
a^*(s)=\arg \max _{a_i} q_{\pi_k}(s, a)</script><p>For each state-action pair, we need to roll out $N$ episodes to estimate the action value. However, since it’s a deterministic policy,  running multiple times would generate the same trajectory. So we only need to rollout  a single episode.</p>
<p>For space limitation, we only illustrate for the part of action value for $s_1$ in the first iteration.</p>
<p><strong>Step 1: policy evaluation</strong></p>
<ol>
<li>Starting from $\left(s_1, a_1\right)$, the episode is $s_1 \xrightarrow{a_1} s_1 \xrightarrow{a_1} s_1 \xrightarrow{a_1} \ldots$ Hence, the action value is</li>
</ol>
<script type="math/tex; mode=display">
q_{\pi_0}\left(s_1, a_1\right)=-1+\gamma(-1)+\gamma^2(-1)+\ldots</script><ol>
<li>Starting from $\left(s_1, a_2\right)$, the episode is $s_1 \xrightarrow{a_2} s_2 \xrightarrow{a_3} s_5 \xrightarrow{a_3} \ldots$ Hence, the action value is</li>
</ol>
<script type="math/tex; mode=display">
q_{\pi_0}\left(s_1, a_2\right)=0+\gamma 0+\gamma^2 0+\gamma^3(1)+\gamma^4(1)+\ldots</script><ol>
<li>Starting from $\left(s_1, a_3\right)$, the episode is $s_1 \xrightarrow{a_3} s_4 \xrightarrow{a_2} s_5 \xrightarrow{a_3} \ldots$ Hence, the action value is</li>
</ol>
<script type="math/tex; mode=display">
q_{\pi_0}\left(s_1, a_3\right)=0+\gamma 0+\gamma^2 0+\gamma^3(1)+\gamma^4(1)+\ldots</script><ol>
<li>Starting from $\left(s_1, a_4\right)$, the episode is $s_1 \xrightarrow{a_4} s_1 \xrightarrow{a_1} s_1 \xrightarrow{a_1} \ldots$ Hence, the action value is</li>
</ol>
<script type="math/tex; mode=display">
q_{\pi_0}\left(s_1, a_4\right)=-1+\gamma(-1)+\gamma^2(-1)+\ldots</script><ol>
<li>Starting from $\left(s_1, a_5\right)$, the episode is $s_1 \xrightarrow{a_5} s_1 \xrightarrow{a_1} s_1 \xrightarrow{a_1} \ldots$ Hence, the action value is</li>
</ol>
<script type="math/tex; mode=display">
q_{\pi_0}\left(s_1, a_5\right)=0+\gamma(-1)+\gamma^2(-1)+\ldots</script><p><strong>Step2: policy improvement</strong></p>
<p>By observing the action values, we see that $q_{\pi_0}\left(s_1, a_2\right)=q_{\pi_0}\left(s_1, a_3\right)$ are the maximum.</p>
<p>As a result, the policy can be improved as</p>
<script type="math/tex; mode=display">
\pi_1\left(a_2 \mid s_1\right)=1 \text { or } \pi_1\left(a_3 \mid s_1\right)=1 \text {. }</script><p>In either way, the new policy for $s_1$ becomes optimal. In this simple example, the initial policy is already optimal for all the states except $s_1$ and $s_3$. Therefore, the policy can become optimal after merely a single iteration. <u>When the policy is nonoptimal for other states, more iterations are needed.</u></p>
<p><strong>A comprehensive example: Episode length and sparse rewards</strong></p>
<p>In MC Basic, if the episode length is too short, the algorithm won’t converge. Its convergence comes from the policy iteration algorithm. But if the episode length is too short, each action value won’t be correct, and the premise of the reasoning falls apart.</p>
<p><strong>So how should we know the right length? Isn’t MC Basic too fragile?</strong></p>
<p>We can see from the following figures that, the episode length greatly impacts the final optimal policies.</p>
<p><strong>When the length of each episode is too short, neither the policy nor the value estimate is optimal</strong> (see Figures 5.4(a)-(d)). In the extreme case where the episode length is one, only the states that are adjacent to the target have nonzero values, and all the other states have zero values since each episode is too short to reach the target or get positive rewards (see Figure 5.4(a)). </p>
<p>As the episode length increases, the policy and value estimates gradually approach the optimal ones (see Fig- ure 5.4(h)).</p>
<p>While the above analysis suggests that each episode must be sufficiently long, the episodes are not necessarily infinitely long. As shown in Figure 5.4(g), when the length is 30, <strong>the algorithm can find an optimal policy, although the value estimate is not yet optimal.</strong></p>
<p><img src="/2024/08/08/Mathmatical-Foundation-of-Reinforcement-Learning-Algorithms-model-based-non-incremental/2024-04-23-14-43-23.png" alt="Figure 5.4: a, b, c, d"><br><img src="/2024/08/08/Mathmatical-Foundation-of-Reinforcement-Learning-Algorithms-model-based-non-incremental/2024-04-23-14-43-51.png" alt="Figure 5.4: e, f, g, h"></p>
<p>The above analysis is related to an important reward design problem, <strong>sparse reward</strong>, which refers to the scenario in which no positive rewards can be obtained unless the target is reached. <u>The sparse reward setting requires long episodes that can reach the target.</u> This requirement is challenging to satisfy when the state space is large. As a result, the sparse reward problem downgrades the learning efficiency.</p>
<p>One simple technique for solving this problem is to design nonsparse rewards. For instance, in the above grid world example, we can redesign the reward setting so that <strong>the agent can obtain a small positive reward when reaching the states near the target.</strong> In this way, an “attractive field” can be formed around the target so that the agent can find the target more easily<sup><a href="#fn_1" id="reffn_1">1</a></sup>.</p>
<blockquote id="fn_1">
<sup>1</sup>. M. Riedmiller, R. Hafner, T. Lampe, M. Neunert, J. Degrave, T. Wiele, V. Mnih, N. Heess, and J. T. Springenberg, Learning by playing solving sparse reward tasks from scratch, in International Conference on Machine Learning, pp. 43444353, 2018.<a href="#reffn_1" title="Jump back to footnote [1] in the text."> ↩</a>
</blockquote>
<h3 id="5-3-MC-Exploring-Starts"><a href="#5-3-MC-Exploring-Starts" class="headerlink" title="5.3 MC Exploring Starts"></a>5.3 MC Exploring Starts</h3><h4 id="5-3-1-Utilizing-samples-more-efficiently"><a href="#5-3-1-Utilizing-samples-more-efficiently" class="headerlink" title="5.3.1 Utilizing samples more efficiently"></a>5.3.1 Utilizing samples more efficiently</h4><p>$s_1 \xrightarrow{a_2} s_2 \xrightarrow{a_4} s_1 \xrightarrow{a_2} s_2 \xrightarrow{a_3} s_5 \xrightarrow{a_1}\ldots$ </p>
<p><strong>initial visit</strong>：an episode is only used to estimate the action value of the initial state-action pair that the episode starts from. The initial-visit strategy merely estimates the action value of $(s_1, a_2)$. The MC Basic algorithm utilizes the initial-visit strategy. However, this strategy is not sample-efficient.because the episode also visits many other state-action pairs such as $(s_2, a_4), (s_2, a_3), (s_5, a_1)$, but they are not be used to estimate the corresponding action values.</p>
<p><strong>subepisode visit</strong>：The trajectory generated after the visit of a state-action pair can be viewed as a new episode.  In this way, the samples in the episode can be utilized more efficiently.</p>
<p>Moreover, a state-action pair may be visited multiple times in an episode. For example, $(s_1, a_2)$ is visited twice in the below episode. If we only count the first-time visit, this is called a <strong>first-visit</strong> strategy. If we count every visit of a state-action pair, such a strategy is called <strong>every-visit</strong> .<sup><a href="#fn_2" id="reffn_2">2</a></sup></p>
<blockquote id="fn_2">
<sup>2</sup>. C. Szepesvari, Algorithms for reinforcement learning. Springer, 2010.<a href="#reffn_2" title="Jump back to footnote [2] in the text."> ↩</a>
</blockquote>
<h4 id="5-3-2-Updating-policies-more-efficiently"><a href="#5-3-2-Updating-policies-more-efficiently" class="headerlink" title="5.3.2 Updating policies more efficiently"></a>5.3.2 Updating policies more efficiently</h4><p>The first strategy is, in the policy evaluation step, to collect all the episodes starting from the same state-action pair and then approximate the action value using the average return of these episodes. This strategy is adopted in the MC Basic algorithm. <strong>The drawback of this strategy is that the agent must wait until all the episodes have been collected before the estimate can be updated.</strong></p>
<p>The second strategy, which can overcome this drawback, is to use the return of a single episode to approximate the corresponding action value. In this way, we can immediately obtain a rough estimate when we receive an episode. Then, <strong>the policy can be improved in an episode-by-episode fashion.(note:this is not step-by-step ,so this is not Temporal-Difference idea)</strong></p>
<p>Since the return of a single episode cannot accurately approximate the corresponding action value, one may wonder whether the second strategy is good. In fact, this strategy falls into the scope of <strong>generalized policy iteration</strong> introduced in the last chapter. That is, <u>we can still update the policy even if the value estimate is not sufficiently accurate.</u></p>
<h4 id="5-3-3-Algorithm-description"><a href="#5-3-3-Algorithm-description" class="headerlink" title="5.3.3 Algorithm description"></a>5.3.3 Algorithm description</h4><p>The details of MC Exploring Starts are given in Algorithm 5.2. This algorithm uses the <strong>subepisode visit</strong> and <strong>every-visit strategy</strong>. Interestingly, when calculating the discounted return obtained by starting from each state-action pair, the procedure starts from the ending states and travels back to the starting state. Such techniques can make the algorithm more efficient, but it also makes the algorithm more complex.<br><img src="/2024/08/08/Mathmatical-Foundation-of-Reinforcement-Learning-Algorithms-model-based-non-incremental/2024-04-23-15-13-25.png" alt=""></p>
<p>It is worth noting that if an action is not well explored, its action value may be inaccurately estimated, and this action may not be selected by the policy even though it is indeed the best action. Both MC Basic and MC Exploring Starts require this condition. <strong>MC Exploring Starts needs to traverse any one $(s, a)$, that is, it needs to start an episode from any one $(s, a)$.</strong></p>
<p>Exploring starts requires an infinite number of (or sufficiently many) episodes to be generated when starting from every state-action pair. In theory, the exploring starts condition is necessary to find optimal policies. That is, only if every action value is well explored, can we accurately evaluate all the actions and then correctly select the optimal ones.</p>
<p>However, <u>this condition is difficult to meet in many applications, especially those involving physical interactions with environments.</u> Can we remove the exploring starts requirement? The answer is yes, as shown in the next section.</p>
<h3 id="5-4-MC-mathbb-epsilon-Greedy-Learning-without-exploring-starts"><a href="#5-4-MC-mathbb-epsilon-Greedy-Learning-without-exploring-starts" class="headerlink" title="5.4 MC $\mathbb{\epsilon}$ -Greedy: Learning without exploring starts"></a>5.4 MC $\mathbb{\epsilon}$ -Greedy: Learning without exploring starts</h3><h4 id="5-4-1-mathbb-epsilon-greedy-policies"><a href="#5-4-1-mathbb-epsilon-greedy-policies" class="headerlink" title="5.4.1 $\mathbb{\epsilon}$-greedy policies"></a>5.4.1 $\mathbb{\epsilon}$-greedy policies</h4><p>The fundamental idea is to make policies soft. <strong>Soft policies</strong> are stochastic, enabling an episode to visit many state-action pairs. In this way, we do not need a large number of episodes starting from every state-action pair.</p>
<p>One type of common soft policies is $\mathbb{\epsilon}$-greedy policies. An-greedy policy is a stochastic policy that has a higher chance of choosing the greedy action with the greatest action value. and the same nonzero probability of taking any other action.  In particular, suppose that $\mathbb{\epsilon} \in [0,1]$. The corresponding-greedy policy has the following form:</p>
<script type="math/tex; mode=display">
\pi_{k+1}(a \mid s)= \begin{cases}1-\frac{\epsilon}{|\mathbb{A}(s)|}(|\mathbb{A}(s)|-1) & \text{for the greedy action} , \\ \frac{\epsilon}{|\mathbb{A}(s)|} & \text{for the other action} |\mathbb{A}(s)|-1 \text{action}.\end{cases} \nonumber</script><ul>
<li>where $|\mathbb{A}(s)|$ denotes the number of actions associated with $s$.</li>
<li>When $\epsilon= 0$ ,$\epsilon$-greedy becomes greedy. When $\epsilon$= 1, the probability of taking any action equals $\frac{1}{|\mathbb{A}(s)|}$.</li>
<li>The probability of taking the greedy action is always greater than that of taking any other action because</li>
</ul>
<script type="math/tex; mode=display">
1-\frac{\epsilon}{|\mathbb{A}(s)|}(|\mathbb{A}(s)|-1)=1-\epsilon+\frac{\epsilon}{|\mathbb{A}(s)|} \geq \frac{\epsilon}{|\mathbb{A}(s)|}</script><p>for any $\mathbb{\epsilon} \in [0,1]$.</p>
<h4 id="5-4-2-Algorithm-description"><a href="#5-4-2-Algorithm-description" class="headerlink" title="5.4.2 Algorithm description"></a>5.4.2 Algorithm description</h4><p>MC $\mathbb{\epsilon}$-Greedy is a variant of MC Exploring Starts that removes the exploring starts requirement.we only need to change the policy improvement step from greedy to $\mathbb{\epsilon}$-greedy.</p>
<p><img src="/2024/08/08/Mathmatical-Foundation-of-Reinforcement-Learning-Algorithms-model-based-non-incremental/2024-04-23-16-20-44.png" alt="Algorithm 5.3."></p>
<p>If greedy policies are replaced by $\epsilon$-greedy policies in the policy improvement step, can we still guarantee to obtain optimal policies?<br>The answer is both yes and no. By yes, we mean that, when given sufficient samples, the algorithm can converge to an $\mathbb{\epsilon}$-greedy policy that is optimal in the set $\mathbb{\Pi}_{\epsilon}$. By no, we mean that the policy is merely optimal among all-greedy policies $\mathbb{\Pi}_{\epsilon}$ but may not be optimal in $\mathbb{\Pi}$. However, if $\mathbb{\epsilon}$ is sufficiently small, the optimal policies in are close to those in $\mathbb{\Pi}$.</p>
<h4 id="5-4-3-Exploration-and-exploitation-of-mathbb-epsilon-greedy-policies"><a href="#5-4-3-Exploration-and-exploitation-of-mathbb-epsilon-greedy-policies" class="headerlink" title="5.4.3  Exploration and exploitation of $\mathbb{\epsilon}$-greedy policies"></a>5.4.3  Exploration and exploitation of $\mathbb{\epsilon}$-greedy policies</h4><p><em>Exploration and exploitation constitute a fundamental tradeoff in reinforcement learning.</em> Here, exploration means that the policy can possibly take as many actions as <strong>possible</strong>. In this way, all the actions can be visited and evaluated well. Exploitation means that the improved policy should <strong>take the greedy action that has the greatest action value.</strong> However, since the action values obtained at the current moment may not be accurate due to insufficient exploration, we should keep exploring while conducting exploitation to avoid missing optimal actions.<strong>(exploration-exploitation dilemma,EvE )</strong></p>
<p><strong>$\mathbb{\epsilon}$-greedy policies provide one way to balance exploration and exploitation.</strong>  If we would like to enhance exploitation and optimality, we need to reduce the value of $\mathbb{\epsilon}$. However, if we would like to enhance exploration, we need to increase the value of $\mathbb{\epsilon}$.</p>
<div align="center"><img src="/2024/08/08/Mathmatical-Foundation-of-Reinforcement-Learning-Algorithms-model-based-non-incremental/2024-04-23-16-38-23.png" width="60%" alt="Figure 5.71"></div>

<div align="center"><img src="/2024/08/08/Mathmatical-Foundation-of-Reinforcement-Learning-Algorithms-model-based-non-incremental/2024-04-23-16-38-43.png" width="60%" alt="Figure 5.72"></div>

<p>Show in Figure 5.7, as the value of increases, the state values of the-greedy policies decrease, indicating that the optimality of these-greedy policies becomes worse. if we want to obtain-greedy policies that are consistent with the optimal greedy ones, the value of should be sufficiently small.</p>
<div align="center"><img src="/2024/08/08/Mathmatical-Foundation-of-Reinforcement-Learning-Algorithms-model-based-non-incremental/2024-04-23-16-53-15.png" width="80%" alt="Figure 5.81"></div>

<div align="center"><img src="/2024/08/08/Mathmatical-Foundation-of-Reinforcement-Learning-Algorithms-model-based-non-incremental/2024-04-23-16-53-33.png" width="80%" alt="Figure 5.82"></div>

<p>Show in Figure 5.8, it is clear that the exploration ability of an $\mathbb{\epsilon}$-greedy policy is strong when $\mathbb{\epsilon}$ is large.One useful technique is to initially set to be large to enhance exploration and gradually reduce it to ensure the optimality of the final policy <sup><a href="#fn_3" id="reffn_3">3</a></sup>.</p>
<blockquote id="fn_3">
<sup>3</sup>. A. Maroti, RBED: Reward based epsilon decay, arXiv:1910.13701, 2019. <strong>AND</strong>  Human-level control through deep reinforcement learning, Nature, vol. 518, no. 7540, pp. 529533, 2015<a href="#reffn_3" title="Jump back to footnote [3] in the text."> ↩</a>
</blockquote>
<h3 id="5-5-Summary"><a href="#5-5-Summary" class="headerlink" title="5.5 Summary"></a>5.5 Summary</h3><ul>
<li><p><strong>MC Basic</strong>: This is the simplest MC-based reinforcement learning algorithm. This algorithm is obtained by replacing the model-based policy evaluation step in the policy iteration algorithm with a model-free MC-based estimation component. Given <em>sufficient samples</em>, it is guaranteed that this algorithm can converge to optimal policies and optimal state values.</p>
</li>
<li><p><strong>MC Exploring Starts</strong>: This algorithm is a variant of MC Basic. It is a variant of MC Basic that adjusts <em>the sample usage strategy</em>.</p>
</li>
<li><p><strong>MC-Greedy</strong>: This algorithm is a variant of MC Exploring Starts. Specifically, in the policy improvement step, it searches for the best $\mathbb{\epsilon}$-greedy policies instead of greedy policies. In this way, the exploration ability of the policy is enhanced and hence <em>the condition of exploring starts can be removed</em>.</p>
</li>
</ul>
<p>Finally, <strong>a tradeoff between exploration and exploitation</strong> was introduced by examining the properties of $\mathbb{\epsilon}$-greedy policies.</p>
]]></content>
  </entry>
  <entry>
    <title>Mathematical Foundation of Reinforcement Learning-Concept Notes</title>
    <url>/2024/08/06/Mathmatical-Foundation-of-Reinforcement-Learning-Concept-Notes/</url>
    <content><![CDATA[<h2 id="Reference-Sources"><a href="#Reference-Sources" class="headerlink" title="Reference Sources:"></a>Reference Sources:</h2><ol>
<li><a href="https://github.com/MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning"><em>Shiyu Zhao</em>. 《Mathematical Foundation of Reinforcement Learning》Chapter 1-3</a>.</li>
<li><a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#">OpenAI Spinning Up</a></li>
<li><a href="https://lyk-love.cn/tags/reinforcement-learning/">Lu yukuan’s notes</a></li>
</ol>
<h2 id="1-Basic-Concepts"><a href="#1-Basic-Concepts" class="headerlink" title="1. Basic Concepts"></a>1. Basic Concepts</h2><h3 id="1-1-A-grid-world-example-Consistent-example"><a href="#1-1-A-grid-world-example-Consistent-example" class="headerlink" title="1.1 A grid world example (Consistent example)"></a>1.1 A grid world example (Consistent example)</h3><div align="center"><img src="/2024/08/06/Mathmatical-Foundation-of-Reinforcement-Learning-Concept-Notes/grid%20world.jpg" width="50%" alt="Figure 1.2: The grid world example is used throughout the book."></div>

<p>Consider an example as shown in Figure, where a robot moves in a grid world. The robot, called agent, can move across adjacent cells in the grid. At each time step, it can only occupy a single cell.The white cells are <em>accessible</em> for entry, and the orange cells are <em> forbidden </em>. There is a <em>target</em> cell that the robot would like to reach. We will use such grid world examples throughout  RL since they are intuitive for illustrating new concepts and algorithms.</p>
<p>How can the goodness of a policy be defined? The idea is that the agent should reach the target <strong>without entering any forbidden cells, taking unnecessary detours, or colliding with the boundary of the grid</strong>.It would be trivial to plan a path to reach the target cell if the agent knew the map of the grid world.<em>The task becomes nontrivial if the agent does not know any information about the environment in advance.</em> Then,  <strong>the agent must interact with the environment to find a good policy by trial and error.</strong> To do that, the concepts presented in the rest of the chapter are necessary.</p>
<h3 id="1-2-State-and-action-s-n-a-m"><a href="#1-2-State-and-action-s-n-a-m" class="headerlink" title="1.2 State and action ( $ s_n, a_m $ )"></a>1.2 State and action ( $ s_n, a_m $ )</h3><p><strong><em>State</em> describes the agent’s status with respect to the environment.</strong> The set of all the states is called the state space, denoted as $\mathcal{S}=\left\{s_1,s_2, \ldots, s_n\right\}$.</p>
<p>For each state, the agent can take actions $\mathcal{A}$. Different states can have different action spaces ($\mathcal{A}(s)$ when at state $s$) . The set of all actions is called the action space, denoted as $\mathcal{A}=\left\{a_1, a_2,\ldots, a_m\right\}$.</p>
<div align="center"><img src="/2024/08/06/Mathmatical-Foundation-of-Reinforcement-Learning-Concept-Notes/state%20and%20action.jpg" width="70%" alt="Figure 1.3: Illustrations of the state and action concepts."></div>

<p>In this RL serie, we consider the most general case: $\mathcal{A}\left(s_i\right)=\mathcal{A}=\left\{a_1, \ldots, a_5\right\}$ for all $i$.</p>
<h3 id="1-3-States-and-Observations"><a href="#1-3-States-and-Observations" class="headerlink" title="1.3 States and Observations"></a>1.3 States and Observations</h3><p>You may see the terminology “observation”. It’s similar to “states”. Whatt’s the difference?</p>
<ul>
<li>A <strong>state</strong> $s$ is a complete description of the state of the world. There is no  information about the world which is hidden from the state. </li>
<li><p>An <strong>observation</strong> $o$ is a partial description of a state, which may omit information.</p>
</li>
<li><p>The environment is <strong>fully observed</strong> when the agent is able to observe the complete state of the environment.</p>
</li>
<li>The environment is <strong>partially observed</strong> when the agent can only see a partial observation.</li>
</ul>
<p>Reinforcement learning notation sometimes puts the symbol for state ($s$), in places where it would be technically more appropriate to write the symbol for observation($o$). </p>
<p>Specifically, this happens when talking about how the agent decides an action: we often signal in notation that the action is conditioned on the state, when in practice, <u>the action is conditioned on the observation because the agent does not have access to the state</u>.</p>
<h3 id="1-4-State-transition-p-s-n-mid-s-1-a-2-x"><a href="#1-4-State-transition-p-s-n-mid-s-1-a-2-x" class="headerlink" title="1.4 State transition ( $p(s_{n} \mid s_1, a_{2})=x$ );"></a>1.4 State transition ( $p(s_{n} \mid s_1, a_{2})=x$ );</h3><p>When taking an action, the agent may move from one state to another. Such a process is called <strong>state transition</strong>. For example, if the agent is at state $s_1$ and selects action $a_2$ , then the agent moves to state $s_2$. Such a process can be expressed as</p>
<script type="math/tex; mode=display">
s_1 \stackrel{a_2}{\rightarrow} s_2</script><ul>
<li>the agent will be bounced back because it is impossible for the agent to exit the state space. Hence, we have $s_1 \stackrel{a_1}{\rightarrow} s_1$.</li>
<li>What is the next state when the agent attempts to enter a forbidden cell, for example, taking action $a_2$ at state $s_5$ ? Two different scenarios may be encountered. </li>
<li><ol>
<li>In the first scenario, although $s_6$ is forbidden, it is still accessible. In this case, the next state is $s_6$; hence, the state transition process is $s_5 \stackrel{a_2}{\longrightarrow} s_6$. </li>
</ol>
</li>
<li><ol>
<li>In the second scenario, $s_6$ is not accessible because, for example, it is surrounded by walls. In this case, the agent is bounced back to $s_5$ if it attempts to move rightward; hence, the state transition process is $s_5 \stackrel{a_2}{\rightarrow} s_5$.</li>
</ol>
</li>
<li>Which scenario should we consider? The answer depends on the physical environment. In this series, <u>we consider the first scenario where the forbidden cells are accessible, although stepping into them may get punished</u>. This scenario is more general and interesting. Moreover, since we are considering a simulation task, <strong>we can define the state transition process however we prefer.</strong> In real-world applications, the state transition process is determined by real-world dynamics.</li>
</ul>
<p>Mathematically, the state transition process can be described by conditional probabilities.  $p(s_{n} \mid s_1, a_{2})=x , x\in(0,1)$; </p>
<h3 id="1-5-Policy-pi-a-mid-s"><a href="#1-5-Policy-pi-a-mid-s" class="headerlink" title="1.5 Policy ($\pi(a \mid s)$)"></a>1.5 Policy ($\pi(a \mid s)$)</h3><p><strong>A policy tells the agent which actions to take at every state.</strong> Intuitively, policies can be depicted as arrows (see Figure 1.4(a)). Following a policy, the agent can generate a trajectory starting from an initial state (see Figure 1.4(b)).</p>
<p></p><div align="center"><img src="/2024/08/06/Mathmatical-Foundation-of-Reinforcement-Learning-Concept-Notes/2024-04-11-12-33-12.png" width="80%" alt="Figure 1.4: A policy represented by arrows and some trajectories obtained by starting from different initial states."></div><br>A policy can be deterministic or stochastic.<p></p>
<ol>
<li><p>Deterministic policy<br> A <strong>deterministic</strong> policy is usually denoted by $\mu$ :</p>
<script type="math/tex; mode=display">
 a_t=\mu\left(s_t\right) .</script></li>
<li><p>Stochastic policy (conditional probabilities)<br> A <strong>stochastic</strong> policy is usually denoted by $\pi$ :</p>
<script type="math/tex; mode=display">
 a_t \sim \pi\left(\cdot \mid s_t\right) .</script><p> In this case, at state $s$, the probability of choosing action $a$ is $\pi(a \mid s)$. It holds that $\sum_{a \in \mathcal{A}(s)} \pi(a \mid s)=1$ for any $s \in \mathcal{S}$.</p>
</li>
</ol>
<p>In RL, the policies are often <strong>parameterized</strong>, the parameters are commonly denoted as $\theta$ or $\phi$ and are written as a subscript on the policy symbol:</p>
<script type="math/tex; mode=display">
\begin{aligned}
a_t & = \mu_{\theta}(s_t) \\ 
a_t & \sim \pi_{\theta}(\cdot | s_t).
\end{aligned}</script><h3 id="1-6-Rewards-r-s-a-p-r-mid-s-a"><a href="#1-6-Rewards-r-s-a-p-r-mid-s-a" class="headerlink" title="1.6 Rewards ($r(s, a)$, $p(r \mid s,a)$)"></a>1.6 Rewards ($r(s, a)$, $p(r \mid s,a)$)</h3><p>After executing an action at a state, the agent obtains a reward, denoted as $r$, as feedback from the environment. The reward is a function of the state $s$ and action $a$. Hence, it is also denoted as $r(s, a)$.  Its value can be a positive or negative real number or zero. </p>
<p>In the grid world example, the rewards are designed as follows:</p>
<ol>
<li>If the agent attempts to exit the boundary, let $r_{\text {boundary }}=-1$.</li>
<li>If the agent attempts to enter a forbidden cell, let $r_{\text {forbidden }}=-1$.</li>
<li>If the agent reaches the target state, let $r_{\text {target }}=+1$.</li>
<li>Otherwise, the agent obtains a reward of $r_{\text {other }}=0$.</li>
</ol>
<p><strong>A reward can be interpreted as a human-machine interface</strong> , with which we can guide the agent to behave as we expect. For example, with the rewards designed above, we can expect that the agent tends to avoid exiting the boundary or stepping into the forbidden cells. <u>Designing appropriate rewards is an important step in reinforcement learning. This step is, <strong>however, nontrivial for complex tasks since it may require the user to understand the given problem well.</strong> Nevertheless, it may still be much easier than solving the problem with other approaches that require a professional background or a deep understanding of the given problem.</u></p>
<p>One question that beginners may ask is as follows: if given the table of rewards, can we find good policies by simply selecting the actions with the greatest rewards? The answer is no. That is because these rewards are immediate rewards that can be obtained after taking an action. To determine a good policy, we must consider the total reward obtained in the long run (see Section 1.7 for more information). <strong>An action with the greatest immediate reward may not lead to the greatest total reward.</strong> It is possible that the immediate reward is negative while the future reward is positive. Thus, which actions to take should be determined by the return (i.e., the total reward) rather than the immediate reward to avoid short-sighted decisions.</p>
<h3 id="1-7-Trajectories-and-Return"><a href="#1-7-Trajectories-and-Return" class="headerlink" title="1.7 Trajectories and Return"></a>1.7 Trajectories and Return</h3><p>A <strong>trajectory</strong> $\tau$ is a state-action-reward chain $\tau = (s_1, a_1, s_2, a_2, …)$.</p>
<p></p><div align="center"><img src="/2024/08/06/Mathmatical-Foundation-of-Reinforcement-Learning-Concept-Notes/2024-04-11-13-19-07.png" width="100%" alt="Figure 1.6: Trajectories obtained by following two policies. The trajectories are indicated by red dashed lines."></div><br>For example, given the policy shown in Figure 1.6(a), if the agent can move along a trajectory as follows:<p></p>
<script type="math/tex; mode=display">
s_1 \underset{r=0}{\stackrel{a_2}{\longrightarrow}} s_2 \underset{r=0}{\stackrel{a_3}{\longrightarrow}} s_5 \underset{r=0}{\stackrel{a_3}{\longrightarrow}} s_8 \underset{r=1}{\stackrel{a_2}{\longrightarrow}} s_9 .</script><p>The undiscounted <strong>return</strong> of this trajectory is defined as the sum of all the rewards collected along the trajectory:</p>
<script type="math/tex; mode=display">
\text { return }=0+0+0+1=1 .</script><p>This return is <strong>undiscounted</strong>. But in RL we usually consider <strong>discounted</strong> return with a discount rate $\gamma \in (0,1)$ especially for the trajectory of infinite length.</p>
<script type="math/tex; mode=display">
\text { return }= 0+ \gamma 0+ \gamma^2 0+ \gamma^3 1 = \gamma^3 .</script><p><strong>The introduction of the discount rate is useful for the following reasons.</strong> </p>
<ol>
<li>it <em>removes the stop criterion</em> and <em>allows for infinitely long trajectories</em>. </li>
<li><em>the discount rate can be used to adjust the emphasis placed on near- or far-future rewards.</em> In particular, if is close to 0, then the agent places more emphasis on rewards obtained in the near future. The resulting policy would be short-sighted. If is close to 1, then the agent places more emphasis on the far future rewards. The resulting policy is far-sighted and dares to take risks of obtaining negative rewards in the near future. </li>
</ol>
<p><u>Returns are also called <em>total rewards</em> or <em>cumulative rewards</em>.</u></p>
<h3 id="1-8-Episodes"><a href="#1-8-Episodes" class="headerlink" title="1.8 Episodes"></a>1.8 Episodes</h3><p>When interacting with the environment by following a policy, the agent may <u>stop at some terminal states</u>. In this case, the resulting trajectory is <u>finite</u>, and is called an <strong>episode</strong> (or a <strong>trial</strong>, <strong>rollout</strong>).</p>
<p>However, some tasks may have no terminal states. In this case, the resulting trajectory is <u>infinite</u>.</p>
<p>Tasks with episodes are called <u>episodic tasks</u>.  some tasks have no terminal states are called <u>continuing tasks</u>.</p>
<p>In fact, we can treat episodic and continuing tasks in a unified mathematical manner by converting episodic tasks to continuing ones. We have two options:</p>
<ol>
<li>First, if we treat the terminal state as a special state, we can specifically design its action space or state transition so that the agent stays in this state forever. Such states are called absorbing states, meaning that the agent never leaves a state once reached.</li>
<li>Second, if we treat the terminal state as a normal state, we can simply set its action space to the same as the other states, and the agent may leave the state and come back again. <u>Since a positive reward of $r=1$ can be obtained every time $s_9$ is reached, the agent will eventually learn to stay at $s_9$ forever (self-circle)</u> to collect more rewards.</li>
</ol>
<p>In this RL series, we consider the second scenario where the target state is treated as a normal state whose action space is $\mathcal{A}\left(s_9\right)=\left\{a_1, \ldots, a_5\right\}$.</p>
<h3 id="1-9-Markov-decision-processes"><a href="#1-9-Markov-decision-processes" class="headerlink" title="1.9 Markov decision processes"></a>1.9 Markov decision processes</h3><p>This section presents the basic RL concepts in a more formal way under the framework of Markov decision processes (MDPs).</p>
<p>An MDP is a general framework for describing stochastic dynamical systems. The key ingredients of an MDP are listed below.</p>
<p><strong>Sets</strong>:</p>
<ul>
<li>— State space: the set of all states, denoted as $\mathcal{S}$.</li>
<li>— Action space: a set of actions, denoted as $\mathcal{A}(s)$, associated with each state $s \in \mathcal{S}$.</li>
<li>— <strong>Reward set</strong>: a set of rewards, denoted as $\mathcal{R}(s, a)$, associated with each state-action pair $(s, a)$.</li>
</ul>
<p>Note:</p>
<ol>
<li>The state, action, reward at time index $t$ are denoted as $s_t, a_t, r_t$ separately.</li>
<li>The reward depends on the state $s$ and action $a$, but not the next state $s^{\prime}$. because since $s^{\prime}$ also depends on $s$ and $a$ i.e. $p(s^{\prime}\mid s,a)$ ($s,a$后下一状态可能是随机的，$s’$代表的下一状态是固定的一种情况，因此与$s’$无关只与$s,a$有关).</li>
</ol>
<p><strong>Model</strong>:</p>
<ul>
<li><strong>State transition probability</strong>: At state $s$, when taking action $a$, the probability of transitioning to state $s^{\prime}$ is $p\left(s^{\prime} \mid s, a\right)$. It holds that $\sum_{s^{\prime} \in \mathcal{S}} p\left(s^{\prime} \mid s, a\right)=1$ for any $(s, a)$.</li>
<li><strong>Reward probability</strong>: At state $s$, when taking action $a$, the probability of obtaining reward $r$ is $p(r \mid s, a)$. It holds that $\sum_{r \in \mathcal{R}(s, a)} p(r \mid s, a)=1$ for any $(s, a)$.</li>
</ul>
<p>Note:</p>
<script type="math/tex; mode=display">
p(r \mid s,a )=\sum_{s^{\prime} \in \mathcal{S}}p(r \mid s,a,s^{\prime} )p(s^{\prime} \mid s,a)</script><p><strong>Policy</strong>: At state $s$, the probability of choosing action $a$ is $\pi(a \mid s)$. It holds that $\sum_{a \in \mathcal{A}(s)} \pi(a \mid s)=1$ for any $s \in \mathcal{S}$.</p>
<p><strong>Markov property</strong>:</p>
<p>The name Markov Decision Process refers to the fact that the system obeys the Markov property, <strong>the memoryless property of a stochastic process.</strong> </p>
<p>Mathematically, it means that</p>
<ol>
<li><p>The state is markovian:</p>
<script type="math/tex; mode=display">
p(s_{t+1} \mid s_t, s_{t-1}, \cdots, s_0)=p(s_{t+1} \mid s_t)</script></li>
<li><p>The state transition is markovian:</p>
<script type="math/tex; mode=display">
p\left(s_{t+1} \mid s_t, a_t, s_{t-1}, a_{t-1}, \ldots, s_0, a_0\right)=p\left(s_{t+1} \mid s_t, a_t\right)</script></li>
<li><p>The action itself doesn’t have markov property, but it’s defined to only rely on current state and policy $a_t \sim \pi\left(\cdot \mid s_t\right)$,</p>
<script type="math/tex; mode=display">
p(a_{t+1}|s_{t+1}, s_t, \cdots, s_0) =  p(a_{t+1}|s_{t+1}).</script><p>$a_t$ doesn’t depends on $s_{t-1}, s_{t-2}, \cdots$.</p>
</li>
<li><p>The reward $r_{t+1}$ itself doesn’t have markov property, but it’s defined to only rely on $s_t, a_t$:</p>
<script type="math/tex; mode=display">
p\left(r_{t+1} \mid s_t, a_t, s_{t-1}, a_{t-1}, \ldots, s_0, a_0\right)=p\left(r_{t+1} \mid s_t, a_t\right).</script></li>
</ol>
<p>$t$ represents the current time step and $t+1$ represents the next time step. </p>
<p>Here, $p\left(s^{\prime} \mid s, a\right)$ and $p(r \mid s, a)$ for all $(s, a)$ are called the <strong>model or dynamics</strong>. The model can be either stationary or nonstationary (or in other words, time-invariant or time-variant). A stationary model does not change over time; a nonstationary model may vary over time. For instance, in the grid world example, if a forbidden area may pop up or disappear sometimes, the model is nonstationary. Unless specifically stated, <u>we only consider stationary models</u>.</p>
<p><strong>note:</strong> model or dynamics are essentially state transitions and reward acquisition.</p>
<h3 id="1-10-Markov-processes"><a href="#1-10-Markov-processes" class="headerlink" title="1.10 Markov processes"></a>1.10 Markov processes</h3><p>One may have heard about the Markov processes (MPs). What is the difference between an MDP and an MP? The answer is that, <u>once the policy in an MDP is fixed, the MDP degenerates into an MP.</u> In this book, the terms “Markov process” and “Markov chain” are used interchangeably when the context is clear. Moreover, this book mainly considers finite MDPs where the numbers of  states and actions are finite. </p>
<h3 id="1-11-What-is-reinforcement-learning"><a href="#1-11-What-is-reinforcement-learning" class="headerlink" title="1.11 What is reinforcement learning"></a>1.11 What is reinforcement learning</h3><p>Reinforcement learning can be described as an <strong>agent-environment interaction process</strong>. The agent is a decision-maker that can <em>sense its state, maintain policies, and execute actions</em>. Everything outside of the agent is regarded as the environment.<br>In the grid world examples, the agent and environment correspond to the robot and grid world, respectively. After the agent decides to <strong>take an action</strong>, the actuator executes such a decision. Then, the state of the agent would be changed and <strong>a reward can be obtained</strong>. By using interpreters, the agent can interpret the new state and the reward. Thus, a closed loop can be formed.</p>
<hr>
<h2 id="2-State-Values-Bellman-Equation-and-Action-Values"><a href="#2-State-Values-Bellman-Equation-and-Action-Values" class="headerlink" title="2. State Values, Bellman Equation and Action Values"></a>2. State Values, Bellman Equation and Action Values</h2><p><strong>ABSTRACT</strong>: The <strong>state value</strong> is the average reward an agent can obtain if it follows a given policy, which is used as a metric to evaluate whether a policy is good or not. <strong>Bellman equation</strong> describes the relationships between the values of all states, which is an important tool for analyzing state values. By solving the Bellman equation, we can obtain the state values. This process is called policy evaluation, which is a fundamental concept in reinforcement learning. The <strong>action value</strong> is introduced to describe the value of taking one action at a state.  Action values play a more direct role than state values when we attempt to find optimal policies. </p>
<h3 id="2-1-State-Values"><a href="#2-1-State-Values" class="headerlink" title="2.1 State Values"></a>2.1 State Values</h3><p>The goal of RL is to find an excellent policy that achieves the <strong>maximal return (or rewards)</strong>. Although returns can be used to evaluate policies, it is more formal to use state values to evaluate policies(the policy or system model may be stochastic): policies that generate greater state values are better. Therefore, state values constitute a core concept in reinforcement learning. How to calculate it? This question is answered in the next section 2.2.<br>Consider a random state-action-reward trajectory for a sequence of time steps $t = 0, 1, 2 \dots$:</p>
<script type="math/tex; mode=display">
S_t \xrightarrow{A_t} R_{t+1}, S_{t+1} \stackrel{A_{t+1}}{\longrightarrow} R_{t+2}, S_{t+2} \stackrel{A_{t+2}}{\longrightarrow} R_{t+3}, \ldots</script><p>The discounted return of the trajectory is defined as</p>
<script type="math/tex; mode=display">
\begin{aligned}
G_t & =R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\ldots \\
& =R_{t+1}+\gamma\left(R_{t+2}+\gamma R_{t+3}+\ldots\right) \\
& =R_{t+1}+\gamma G_{t+1}
\end{aligned}</script><p>where $R_{t+1}$ is the immediate reward of arriving state $S_{t+1}$ or leaving $S_t$, $\gamma$ is the discount rate. Note that $S_t,A_t,R_{t+1} G_t$ are all random variables.</p>
<p><u>The expectation (or called expected value or mean) of $G_t$ is called the <strong>state-value function</strong> or simply <strong>state value</strong></u>:</p>
<script type="math/tex; mode=display">
\begin{equation} 
v_\pi(s)=\mathbb{E}\left[G_t \mid S_t=s\right]
\end{equation}</script><p>Remarks:</p>
<ul>
<li>It is a function of $s$. It is a conditional expectation with the condition that the agent starts from state $s$.</li>
<li>It is based on the policy $\pi$. For a different policy, the state value may be different.</li>
<li>It represents the “value” of a state. If the state value is greater, then the policy is better because greater cumulative rewards can be obtained.</li>
</ul>
<p><u>The relationship between state values and returns is further clarified as follows</u>. When both <strong>the policy and the system model are deterministic</strong>, starting from a state always leads to the same trajectory. In this case, the return obtained starting from a state is equal to the value of that state. By contrast, when either <strong>the policy or the system model is stochastic</strong>, starting from the same state may generate different trajectories. In this case, the returns of different trajectories are different, and the state value is the mean of these returns.</p>
<h3 id="2-2-Bellman-Equation"><a href="#2-2-Bellman-Equation" class="headerlink" title="2.2 Bellman Equation"></a>2.2 Bellman Equation</h3><p>the Bellman equation is a set of linear equations that describe the relationships between the values of all the states. Given a policy, finding out the corresponding state values from the Bellman equation is called <strong>policy evaluation</strong>. It is an important process in many reinforcement learning algorithms.<br>Recalling the definition of the state value , we substitute $G(t) = R_{t+1}+\gamma G_{t+1}$ into it:</p>
<script type="math/tex; mode=display">
\begin{aligned}
v_\pi(s) & =\mathbb{E}\left[G_t \mid S_t=s\right] \\
& =\mathbb{E}\left[R_{t+1}+\gamma G_{t+1} \mid S_t=s\right] \\
& =\mathbb{E}\left[R_{t+1} \mid S_t=s\right]+\gamma \mathbb{E}\left[G_{t+1} \mid S_t=s\right]
\end{aligned}</script><p>Next we calculate the two terms of the last line, respectively.</p>
<h4 id="2-2-1-First-term-the-mean-of-immediate-rewards"><a href="#2-2-1-First-term-the-mean-of-immediate-rewards" class="headerlink" title="2.2.1 First term: the mean of immediate rewards"></a>2.2.1 First term: the mean of immediate rewards</h4><p>First, calculate the first term $\mathbb{E}\left[R_{t+1} \mid S_t=s\right]$, is the mean of immediate rewards. </p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbb{E}\left[R_{t+1} \mid S_t=s\right] & =\sum_{a \in \mathcal A(s)} \pi(a \mid s) \mathbb{E}\left[R_{t+1} \mid S_t=s, A_t=a\right] \\
& =\sum_{a \in \mathcal A(s)}\pi(a \mid s)\sum_{r \in \mathcal R(s,a)} p(r \mid s, a) r .
\end{aligned}</script><p>Given events $R_{t+1} = r, S_t = s, A_t = a$, the deduction is quite simple.</p>
<p>background knowledge1(<strong>Total Probability Theorem</strong>): From the definition of <em>conditional expectation</em>: given event $A$ and  a discrete random variable $X$,  the conditional expectation is</p>
<script type="math/tex; mode=display">
   \mathrm{E}(X \mid A)=\sum x P(X=x \mid A) .</script><p>A more verbose version of deduction is:</p>
<script type="math/tex; mode=display">
\mathbb{E}\left[R_{t+1} \mid S_t=s\right] \triangleq \sum_r p(r | s) r.</script><script type="math/tex; mode=display">
p(r | s) = \sum_{a \in \mathcal A(s)} \pi(a|s) . p(r | s, a)</script><h4 id="2-2-2-Second-term-the-mean-of-future-rewards"><a href="#2-2-2-Second-term-the-mean-of-future-rewards" class="headerlink" title="2.2.2 Second term: the mean of future rewards"></a>2.2.2 Second term: the mean of future rewards</h4><p>Second, calculate the second term $\mathbb{E}\left[G_{t+1} \mid S_t=s\right]$ , is the mean of future rewards.</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbb{E}\left[G_{t+1} \mid S_t=s\right] & =\sum_{s^{\prime}\in \mathcal S} \mathbb{E}\left[G_{t+1} \mid S_t=s, S_{t+1}=s^{\prime}\right] p\left(s^{\prime} \mid s\right) \\
& =\sum_{s^{\prime}\in \mathcal S} \mathbb{E}\left[G_{t+1} \mid S_{t+1}=s^{\prime}\right] p\left(s^{\prime} \mid s\right) \text{(due to the memoryless Markov property.)}\\
& =\sum_{s^{\prime}\in \mathcal S} v_\pi\left(s^{\prime}\right) p\left(s^{\prime} \mid s\right) \\
& =\sum_{s^{\prime}\in \mathcal S} v_\pi\left(s^{\prime}\right) \sum_{a \in \mathcal A(s)} p\left(s^{\prime} \mid s, a\right) \pi(a \mid s)
\end{aligned}</script><h4 id="2-2-3-Formula-simplification"><a href="#2-2-3-Formula-simplification" class="headerlink" title="2.2.3 Formula simplification"></a>2.2.3 Formula simplification</h4><script type="math/tex; mode=display">
\begin{aligned}
v_\pi(s) & =\mathbb{E}\left[R_{t+1} \mid S_t=s\right]+\gamma \mathbb{E}\left[G_{t+1} \mid S_t=s\right], \\
& =\underbrace{\sum_{a \in \mathcal A(s)} \pi(a \mid s) \sum_{r \in \mathcal R(s,a)} p(r \mid s, a) r}_{\text {mean of immediate rewards }}+\underbrace{\gamma \sum_{s^{\prime}\in \mathcal S} \pi(a \mid s) \sum_{a \in \mathcal A(s)} p\left(s^{\prime} \mid s, a\right) v_\pi\left(s^{\prime}\right),}_{\text {mean of future rewards }}  \\
& =\sum_{a \in \mathcal A(s)} \pi(a \mid s)\left[\sum_{r \in \mathcal R(s,a)} p(r \mid s, a) r+\gamma \sum_{s^{\prime}\in \mathcal S} p\left(s^{\prime} \mid s, a\right) v_\pi\left(s^{\prime}\right)\right], \quad \forall s \in \mathcal{S} .
\end{aligned}</script><p>The above equation is called the <strong>Bellman equation</strong>, which characterizes the relationship among the state-values functions of different states.</p>
<ul>
<li>$\pi(a \mid s)$ is a given policy , $r$ is designed, $p(r \mid s, a)$, $p\left(s^{\prime} \mid s, a\right)$ represent the system model,which can be obtained. </li>
<li>Only $v_{\pi} (s)$ and $v_{\pi} (s^{\prime})$ are unknown state values to be calculated. It may be confusing to beginners how to calculate the unknown $v_{\pi} (s)$ given that it relies on another unknown $v_{\pi} (s^{\prime})$. It must be noted that the Bellman equation refers to a set of linear equations for all states rather than a single equation. (<strong>Bootstrapping</strong>)</li>
</ul>
<h3 id="2-3-Examples"><a href="#2-3-Examples" class="headerlink" title="2.3 Examples"></a>2.3 Examples</h3><h4 id="2-3-1-For-deterministic-policy"><a href="#2-3-1-For-deterministic-policy" class="headerlink" title="2.3.1 For deterministic policy"></a>2.3.1 For deterministic policy</h4><p></p><div align="center"><img src="/2024/08/06/Mathmatical-Foundation-of-Reinforcement-Learning-Concept-Notes/2024-04-12-21-11-39.png" width="50%" alt="Figure 2.4: An example for demonstrating the Bellman equation. The policy in this example is deterministic."></div><br>Consider the first example shown in Figure 2.4, where the policy is deterministic. We next write out the Bellman equation and then solve the state values from it.<p></p>
<script type="math/tex; mode=display">
\begin{aligned}
& v_\pi\left(s_1\right)=0+\gamma v_\pi\left(s_3\right), \\
& v_\pi\left(s_2\right)=1+\gamma v_\pi\left(s_4\right), \\
& v_\pi\left(s_3\right)=1+\gamma v_\pi\left(s_4\right), \\
& v_\pi\left(s_4\right)=1+\gamma v_\pi\left(s_4\right) .
\end{aligned}</script><p>We can solve the state values from these equations. Since the equations are simple, we can manually solve them. More complicated equations can be solved by the iterative algorithm.</p>
<script type="math/tex; mode=display">
\begin{aligned}
& v_\pi\left(s_4\right)=\frac{1}{1-\gamma}, \\
& v_\pi\left(s_3\right)=\frac{1}{1-\gamma}, \\
& v_\pi\left(s_2\right)=\frac{1}{1-\gamma}, \\
& v_\pi\left(s_1\right)=\frac{\gamma}{1-\gamma} .
\end{aligned}</script><p>Furthermore, if we set $\gamma=0.9$, then</p>
<script type="math/tex; mode=display">
\begin{aligned}
& v_\pi\left(s_4\right)=\frac{1}{1-0.9}=10, \\
& v_\pi\left(s_3\right)=\frac{1}{1-0.9}=10, \\
& v_\pi\left(s_2\right)=\frac{1}{1-0.9}=10, \\
& v_\pi\left(s_1\right)=\frac{0.9}{1-0.9}=9 .
\end{aligned}</script><h4 id="2-3-2-For-stochastic-policy"><a href="#2-3-2-For-stochastic-policy" class="headerlink" title="2.3.2 For stochastic policy"></a>2.3.2 For stochastic policy</h4><p></p><div align="center"><img src="/2024/08/06/Mathmatical-Foundation-of-Reinforcement-Learning-Concept-Notes/2024-04-12-21-14-38.png" width="50%" alt="Figure 2.5: An example for demonstrating the Bellman equation. The policy in this example is stochastic."></div><br>Consider the second example shown in Figure 2.5, where the policy is stochastic. The state transition probability is deterministic, and the reward probability is also deterministic. I,e, the system model is deterministic.<p></p>
<script type="math/tex; mode=display">
v_\pi\left(s_1\right)=0.5\left[0+\gamma v_\pi\left(s_3\right)\right]+0.5\left[-1+\gamma v_\pi\left(s_2\right)\right]</script><p>Similarly, it can be obtained that</p>
<script type="math/tex; mode=display">
\begin{aligned}
& v_\pi\left(s_2\right)=1+\gamma v_\pi\left(s_4\right), \\
& v_\pi\left(s_3\right)=1+\gamma v_\pi\left(s_4\right), \\
& v_\pi\left(s_4\right)=1+\gamma v_\pi\left(s_4\right) .
\end{aligned}</script><p>simple, we can solve the state values manually and obtain</p>
<script type="math/tex; mode=display">
\begin{aligned}
v_\pi\left(s_4\right) & =\frac{1}{1-\gamma}, \\
v_\pi\left(s_3\right) & =\frac{1}{1-\gamma}, \\
v_\pi\left(s_2\right) & =\frac{1}{1-\gamma}, \\
v_\pi\left(s_1\right) & =0.5\left[0+\gamma v_\pi\left(s_3\right)\right]+0.5\left[-1+\gamma v_\pi\left(s_2\right)\right], \\
& =-0.5+\frac{\gamma}{1-\gamma} .
\end{aligned}</script><p>Furthermore, if we set $\gamma=0.9$, then</p>
<script type="math/tex; mode=display">
\begin{aligned}
& v_\pi\left(s_4\right)=10, \\
& v_\pi\left(s_3\right)=10, \\
& v_\pi\left(s_2\right)=10, \\
& v_\pi\left(s_1\right)=-0.5+9=8.5 .
\end{aligned}</script><p>If we compare the state values of the two policies in the above examples, it can be seen that</p>
<script type="math/tex; mode=display">
v_{\pi_1}\left(s_i\right) \geq v_{\pi_2}\left(s_i\right), i= 1,2,3,4</script><p>which indicates that the policy in Figure 2.4 is better because it has greater state values.<br>This mathematical conclusion is consistent with the intuition that the rst policy is better because it can avoid entering the forbidden area when the agent starts from s1. As a result, the above two examples demonstrate that state values can be used to evaluate policies.</p>
<h3 id="2-4-Bellman-equation-the-matrix-vector-form"><a href="#2-4-Bellman-equation-the-matrix-vector-form" class="headerlink" title="2.4 Bellman equation: the matrix-vector form"></a>2.4 Bellman equation: the matrix-vector form</h3><p>Consider the Bellman equation:</p>
<script type="math/tex; mode=display">
\begin{equation} 
v_\pi(s)=\sum_a \pi(a \mid s)\left[\sum_r p(r \mid s, a) r+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right) v_\pi\left(s^{\prime}\right)\right]
\end{equation}</script><p>It’s an elementwise form. If we put all the equations together, we have a set of linear equations, which can be concisely written in a matrix-vector form.</p>
<p>Rewrite as</p>
<script type="math/tex; mode=display">
v_\pi(s)=r_\pi(s)+\gamma \sum_{s^{\prime}} p_\pi\left(s^{\prime} \mid s\right) v_\pi\left(s^{\prime}\right)</script><p>where</p>
<script type="math/tex; mode=display">
\begin{aligned}
& r_\pi(s) \triangleq \sum_a \pi(a \mid s) \sum_r p(r \mid s, a) r, \\
& p_\pi\left(s^{\prime} \mid s\right) \triangleq \sum_a \pi(a \mid s) p\left(s^{\prime} \mid s, a\right) .
\end{aligned}</script><p>Suppose the states could be indexed as $s_i(i=1, \ldots, n)$. For state $s_i$, the Bellman equation is</p>
<script type="math/tex; mode=display">
v_\pi\left(s_i\right)=r_\pi\left(s_i\right)+\gamma \sum_{s_j} p_\pi\left(s_j \mid s_i\right) v_\pi\left(s_j\right)</script><p>Put all these equations for all the states together and rewrite to a matrix-vector form</p>
<script type="math/tex; mode=display">
\begin{equation} 
v_\pi=r_\pi+\gamma P_\pi v_\pi
\end{equation}</script><p>where</p>
<p>$v_\pi=\left[v_\pi\left(s_1\right), \ldots, v_\pi\left(s_n\right)\right]^T \in \mathbb{R}^n$,</p>
<p>$r_\pi=\left[r_\pi\left(s_1\right), \ldots, r_\pi\left(s_n\right)\right]^T \in \mathbb{R}^n$,</p>
<p>$P_\pi \in \mathbb{R}^{n \times n}$, where $\left[P_\pi\right]_{i j}=p_\pi\left(s_j \mid s_i\right)$, is the state transition matrix.</p>
<p><strong>Examples: </strong>If there are four states, $v_\pi=r_\pi+\gamma P_\pi v_\pi$ can be written out as</p>
<script type="math/tex; mode=display">
\underbrace{\left[\begin{array}{l}
v_\pi\left(s_1\right) \\
v_\pi\left(s_2\right) \\
v_\pi\left(s_3\right) \\
v_\pi\left(s_4\right)
\end{array}\right]}_{v_\pi}=\underbrace{\left[\begin{array}{l}
r_\pi\left(s_1\right) \\
r_\pi\left(s_2\right) \\
r_\pi\left(s_3\right) \\
r_\pi\left(s_4\right)
\end{array}\right]}_{r_\pi}+\gamma \underbrace{\left[\begin{array}{llll}
p_\pi\left(s_1 \mid s_1\right) & p_\pi\left(s_2 \mid s_1\right) & p_\pi\left(s_3 \mid s_1\right) & p_\pi\left(s_4 \mid s_1\right) \\
p_\pi\left(s_1 \mid s_2\right) & p_\pi\left(s_2 \mid s_2\right) & p_\pi\left(s_3 \mid s_2\right) & p_\pi\left(s_4 \mid s_2\right) \\
p_\pi\left(s_1 \mid s_3\right) & p_\pi\left(s_2 \mid s_3\right) & p_\pi\left(s_3 \mid s_3\right) & p_\pi\left(s_4 \mid s_3\right) \\
p_\pi\left(s_1 \mid s_4\right) & p_\pi\left(s_2 \mid s_4\right) & p_\pi\left(s_3 \mid s_4\right) & p_\pi\left(s_4 \mid s_4\right)
\end{array}\right]}_{P_\pi} \underbrace{\left[\begin{array}{l}
v_\pi\left(s_1\right) \\
v_\pi\left(s_2\right) \\
v_\pi\left(s_3\right) \\
v_\pi\left(s_4\right)
\end{array}\right]}_{v_\pi} .</script><p>For the above deterministic policy</p>
<script type="math/tex; mode=display">
\left[\begin{array}{l}
v_\pi\left(s_1\right) \\
v_\pi\left(s_2\right) \\
v_\pi\left(s_3\right) \\
v_\pi\left(s_4\right)
\end{array}\right]=\left[\begin{array}{l}
0 \\
1 \\
1 \\
1
\end{array}\right]+\gamma\left[\begin{array}{llll}
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 0 & 1 \\
0 & 0 & 0 & 1
\end{array}\right]\left[\begin{array}{l}
v_\pi\left(s_1\right) \\
v_\pi\left(s_2\right) \\
v_\pi\left(s_3\right) \\
v_\pi\left(s_4\right)
\end{array}\right]</script><p>For the above stochastic policy:</p>
<script type="math/tex; mode=display">
\left[\begin{array}{c}
v_\pi\left(s_1\right) \\
v_\pi\left(s_2\right) \\
v_\pi\left(s_3\right) \\
v_\pi\left(s_4\right)
\end{array}\right]=\left[\begin{array}{c}
0.5(0)+0.5(-1) \\
1 \\
1 \\
1
\end{array}\right]+\gamma\left[\begin{array}{cccc}
0 & 0.5 & 0.5 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 0 & 1 \\
0 & 0 & 0 & 1
\end{array}\right]\left[\begin{array}{l}
v_\pi\left(s_1\right) \\
v_\pi\left(s_2\right) \\
v_\pi\left(s_3\right) \\
v_\pi\left(s_4\right)
\end{array}\right]</script><h3 id="2-5-Solving-state-values-from-the-Bellman-equation"><a href="#2-5-Solving-state-values-from-the-Bellman-equation" class="headerlink" title="2.5 Solving state values from the Bellman equation"></a>2.5 Solving state values from the Bellman equation</h3><p>Calculating the state values of a given policy is a fundamental problem in reinforcement learning. This problem is often referred to as policy evaluation. In this section, two methods are presented for calculating state values from the Bellman equation.</p>
<ul>
<li><p>The closed-form solution is:</p>
<script type="math/tex; mode=display">
v_\pi=\left(I-\gamma P_\pi\right)^{-1} r_\pi</script><p>The drawback of closed-form solution is that it involves a matrix inverse operation, which is computationally expensive. Thus, in practice, we’ll use an <strong>iterative algorithm</strong>.</p>
</li>
<li><p>An iterative solution is:</p>
<script type="math/tex; mode=display">
v_{k+1}=r_\pi+\gamma P_\pi v_k  \quad  k=0,1,2,\dots ,</script><p>where $I$ is the identity matrix. We can just <u>randomly select</u> a matrix $v_0$, then calculate $v_1, v_2, \cdots$. This leads to a sequence $\left\{v_0, v_1, v_2, \ldots\right\}$. We can show that</p>
<script type="math/tex; mode=display">
v_k \rightarrow v_\pi=\left(I-\gamma P_\pi\right)^{-1} r_\pi, \quad k \rightarrow \infty</script></li>
</ul>
<blockquote>
<p><strong>Proof: the iterative solution</strong>（Contraction mapping theorem）<br>Define the error as $\delta_k=v_k-v_\pi$. We only need to show $\delta_k \rightarrow 0$. Substituting: $v_{k+1}=\delta_{k+1}+v_\pi$,and $v_k=\delta_k+v_\pi$ into $v_{k+1}=r_\pi+\gamma P_\pi v_k$ gives</p>
<script type="math/tex; mode=display">
\delta_{k+1}+v_\pi=r_\pi+\gamma P_\pi\left(\delta_k+v_\pi\right)</script><p>which can be rewritten as</p>
<script type="math/tex; mode=display">
\delta_{k+1}=-v_\pi+r_\pi+\gamma P_\pi \delta_k+\gamma P_\pi v_\pi=\gamma P_\pi \delta_k</script><p>As a result,</p>
<script type="math/tex; mode=display">
\delta_{k+1}=\gamma P_\pi \delta_k=\gamma^2 P_\pi^2 \delta_{k-1}=\cdots=\gamma^{k+1} P_\pi^{k+1} \delta_0</script><p>Note that $0 \leq P_\pi^k \leq 1$, which means every entry of $P_\pi^k$ is no greater than 1 for any $k=0,1,2, \ldots$. That is because $P_\pi^k =\mathbf{1}$, where $\mathbf{1}=[1, \ldots, 1]^T$. On the other hand, since $\gamma&lt;1$, we know $\gamma^k \rightarrow 0$ and hence $\delta_{k+1} \rightarrow 0$ as $k \rightarrow \infty$.</p>
</blockquote>
<h3 id="2-6-Action-Value"><a href="#2-6-Action-Value" class="headerlink" title="2.6 Action Value"></a>2.6 Action Value</h3><p>From state value to action value:</p>
<ul>
<li>State value: the average return the agent can <u>get starting from a state</u>.</li>
<li>Action value: the average return the agent can <u>get starting from a state and taking an action</u>.</li>
</ul>
<p>Definition of action value (or action value function):</p>
<script type="math/tex; mode=display">
\begin{equation} 
q_\pi(s, a) \triangleq \mathbb{E}\left[G_t \mid S_t=s, A_t=a\right]
\end{equation}</script><p>Note:</p>
<ol>
<li>The $q_\pi(s, a)$ is a function of the state-action pair $(s, a)$.</li>
<li>The $q_\pi(s, a)$ depends on $\pi$.</li>
</ol>
<p><strong>Relation to the state value function</strong></p>
<p>It follows from the properties of <em>conditional expectation</em> that</p>
<script type="math/tex; mode=display">
\underbrace{\mathbb{E}\left[G_t \mid S_t=s\right]}_{v_\pi(s)}=\sum_a \underbrace{\mathbb{E}\left[G_t \mid S_t=s, A_t=a\right]}_{q_\pi(s, a)} \pi(a \mid s)</script><p>Hence,</p>
<script type="math/tex; mode=display">
\begin{equation} 
v_\pi(s)=\sum_a \pi(a \mid s) q_\pi(s, a)
\end{equation}</script><p>Recall the <a href="">Bellman equation</a>, that the state value is given by</p>
<script type="math/tex; mode=display">
v_\pi(s)=\sum_a \pi(a \mid s)[\underbrace{\sum_r p(r \mid s, a) r+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right) v_\pi\left(s^{\prime}\right)}_{q_\pi(s, a)}]</script><p>By comparing  and , we have the action-value function as</p>
<script type="math/tex; mode=display">
\begin{equation} 
q_\pi(s, a)=\sum_r p(r \mid s, a) r+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right) 
v_\pi\left(s^{\prime}\right)
\end{equation}</script><p>equation (5) and equation (6) are the two sides of the same coin:</p>
<ul>
<li>equation (5) shows how to obtain state values from action values.</li>
<li>equation (6) shows how to obtain action values from state values.</li>
</ul>
<p><strong>Example</strong></p>
<p></p><div align="center"><img src="/2024/08/06/Mathmatical-Foundation-of-Reinforcement-Learning-Concept-Notes/2024-04-12-22-00-41.png" width="50%" alt="Figure 2.8: An example for demonstrating the process of calculating action values."></div><br>Consider the stochastic policy shown in Figure 2.8. <p></p>
<script type="math/tex; mode=display">
q_\pi\left(s_1, a_2\right)=-1+\gamma v_\pi\left(s_2\right)</script><p>Note that even if an action would not be selected by a policy, it still has an action value. Therefor, for the other actions:</p>
<script type="math/tex; mode=display">
\begin{aligned}
& q_\pi\left(s_1, a_1\right)=-1+\gamma v_\pi\left(s_1\right), \\
& q_\pi\left(s_1, a_3\right)=0+\gamma v_\pi\left(s_3\right), \\
& q_\pi\left(s_1, a_4\right)=-1+\gamma v_\pi\left(s_1\right), \\
& q_\pi\left(s_1, a_5\right)=0+\gamma v_\pi\left(s_1\right) .
\end{aligned}</script><p><strong>NOTE:</strong><br>Although some actions cannot be possibly selected by a given policy, this does not mean that these actions are not good. It is possible that the given policy is not good, so it cannot select the best action. The purpose of reinforcement learning is to find optimal policies. To that end, we must keep exploring all actions to determine better actions for each state.</p>
<h3 id="2-7-Summary"><a href="#2-7-Summary" class="headerlink" title="2.7 Summary"></a>2.7 Summary</h3><p>The most important concept introduced in this chapter is the state value. Mathematically, a state value is the expected return that the agent can obtain by starting from a state. The values of different states are related to each other. That is, <strong>the value of state $s$ relies on the values of some other states</strong>, which may further rely on the value of state $s$ itself. This phenomenon might be the most confusing part of this chapter for beginners. It is related to an important concept called <strong>bootstrapping, which involves calculating something from itself.</strong> Although bootstrapping may be intuitively confusing, it is clear if we examine the matrix-vector form of the Bellman equation. In particular, the Bellman equation is a set of linear equations that describe the relationships between the values of all states.</p>
<p>Since state values can be used to evaluate whether a policy is good or not, <strong>the process of solving the state values of a policy from the Bellman equation is called policy evaluation.</strong> As we will see later in this book, policy evaluation is an important step in many reinforcement learning algorithms.</p>
<p>Another important concept, action value, was introduced to describe the value of taking one action at a state. As we will see later in this book, <strong>action values play a more direct role than state values when we attempt to find optimal policies.</strong> Finally, the Bellman equation is not restricted to the reinforcement learning field. Instead, it widely exists in many elds such as control theories and operation research. In different elds, the Bellman equation may have different expressions. In this book, the Bellman equation is studied under discrete Markov decision processes.</p>
<p><strong> How to calculate action value</strong></p>
<ul>
<li><p>We can first calculate all the state values and then calculate the action values.</p>
</li>
<li><p>We can also directly calculate the action values with or without models (discussed later).</p>
</li>
</ul>
<hr>
<h2 id="3-Bellman-Optimality-Equation-BOE"><a href="#3-Bellman-Optimality-Equation-BOE" class="headerlink" title="3. Bellman Optimality Equation(BOE)"></a>3. Bellman Optimality Equation(BOE)</h2><h3 id="3-1-Optimal-State-values-and-Policy"><a href="#3-1-Optimal-State-values-and-Policy" class="headerlink" title="3.1 Optimal State values and Policy"></a>3.1 Optimal State values and Policy</h3><p>The state value could be used to evaluate policy : if</p>
<script type="math/tex; mode=display">
v_{\pi_1}(s) \geq v_{\pi_2}(s) \quad \text { for all } s \in \mathcal{S}</script><p>then $\pi_1$ is said to be <strong>better</strong> than $\pi_2$.</p>
<p><strong>Definition 1</strong>(Optimal policy and optimal state value). A policy $\pi^\ast$<br>is optimal if $v_{\pi^\ast}(s) \geq v_\pi(s)$ for all $s\in S$ and for any other policy $\pi$. The state values $\pi^\ast$ of are the optimal state values.</p>
<p>The definition leads to many questions:</p>
<ul>
<li>Does the optimal policy <strong>exist</strong>?</li>
<li>Is the optimal policy <strong>unique</strong>?</li>
<li>Is the optimal policy <strong>stochastic or deterministic</strong>?</li>
<li><strong>How to obtain</strong> the optimal policy?</li>
</ul>
<p>These fundamental questions must be clearly answered to thoroughly understand optimal policies. For example, regarding the existence of optimal policies, if optimal policies do not exist, then we do not need to bother to design algorithms to find them. To answer these questions, we study the <em>Bellman optimality equation</em>.</p>
<h3 id="3-2-Bellman-optimality-equation-elementwise-form"><a href="#3-2-Bellman-optimality-equation-elementwise-form" class="headerlink" title="3.2 Bellman optimality equation  (elementwise form)"></a>3.2 Bellman optimality equation  (elementwise form)</h3><p><strong>Bellman optimality equation (BOE) is a tool for analyzing optimal policies and optimal state values.</strong> By solving this equation, we can obtain optimal policies and optimal state values.</p>
<p>For every $s \in \mathcal{S}$, the elementwise expression of the BOE is</p>
<script type="math/tex; mode=display">
\begin{aligned}
v(s) & =\max _{\pi(s) \in \Pi(s)} \sum_{a \in \mathcal{A}} \pi(a \mid s)\left(\sum_{r \in \mathcal{R}} p(r \mid s, a) r+\gamma \sum_{s^{\prime} \in \mathcal{S}} p\left(s^{\prime} \mid s, a\right) v\left(s^{\prime}\right)\right) \\
& =\max _{\pi(s) \in \Pi(s)} \sum_{a \in \mathcal{A}} \pi(a \mid s) q(s, a),
\end{aligned}</script><p>where $v(s), v\left(s^{\prime}\right)$ are unknown variables to be solved and</p>
<script type="math/tex; mode=display">
q(s, a) \doteq \sum_{r \in \mathcal{R}} p(r \mid s, a) r+\gamma \sum_{s^{\prime} \in \mathcal{S}} p\left(s^{\prime} \mid s, a\right) v\left(s^{\prime}\right)</script><p>Here, $\pi(s)$ denotes a policy for state $s$, and $\Pi(s)$ is the set of all possible policies for $s$.</p>
<p>Notes:</p>
<p>$p(r \mid s, a), p\left(s^{\prime} \mid s, a\right)$ are known and $r$,$\gamma$ are designed. this equation has two unknown variables $v(s)$ and $\pi(a \mid s)$. It may be confusing to beginners how to solve two unknown variables from one equation. The reason is this equation satisfy a special quality.(contraction mapping theorem)</p>
<h4 id="3-2-1-Maximize-on-the-right-hand-side-of-BOE"><a href="#3-2-1-Maximize-on-the-right-hand-side-of-BOE" class="headerlink" title="3.2.1 Maximize on the right-hand side of BOE"></a>3.2.1 Maximize on the right-hand side of BOE</h4><p>In practice we need to deal with the matrix-vector form since that is what we’re faced with. But since each row in the matrix is actually a vector of the elementwise form, we start with the element form.</p>
<p>In fact, we can turn the problem into “solve the optimal $\pi$ on the right-hand side, next to get the optimal state values “. Let’s look at one example first:</p>
<hr>
<p><strong>Example</strong> 1. Consider two unknown variables $x, y \in \mathbb{R}$ that satisfy</p>
<script type="math/tex; mode=display">
x=\max _{y \in \mathbb{R}}\left(2 x-1-y^2\right) .</script><p>The first step is to solve $y$ on the right-hand side of the equation. Regardless of the value of $x$, we always have $\max _y\left(2 x-1-y^2\right)=2 x-1$, where the maximum is achieved when $y=0$. The second step is to solve $x$. When $y=0$, the equation becomes $x=2 x-1$, which leads to $x=1$. Therefore, $y=0$ and $x=1$ are the solutions of the equation.</p>
<p><strong>Example</strong> 2. Given $q_1, q_2, q_3 \in \mathbb{R}$, we would like to find the optimal values of $c_1, c_2, c_3$ to maximize</p>
<script type="math/tex; mode=display">
\sum_{i=1}^3 c_i q_i=c_1 q_1+c_2 q_2+c_3 q_3,</script><p>where $c_1+c_2+c_3=1$ and $c_1, c_2, c_3 \geq 0$.<br>Without loss of generality, suppose that $q_3 \geq q_1, q_2$. Then, the optimal solution is $c_3^\ast=1$ and $c_1^\ast=c_2^\ast=0$. This is because</p>
<script type="math/tex; mode=display">
q_3=\left(c_1+c_2+c_3\right) q_3=c_1 q_3+c_2 q_3+c_3 q_3 \geq c_1 q_1+c_2 q_2+c_3 q_3</script><p>for any $c_1, c_2, c_3$.</p>
<hr>
<p>Inspired by the above example, since $\sum_a(\pi(a \mid s))= 1$, the (elementwise) BOE can be written as</p>
<script type="math/tex; mode=display">
v(s)=\max _{\pi(s) \in \Pi(s)} \sum_{a \in \mathcal{A}} \pi(a \mid s) q(s, a)\quad =\max _{a \in \mathcal{A}(s)} q(s, a) , s \in \mathcal{S}.</script><p>where the optimality is achieved when</p>
<script type="math/tex; mode=display">
\pi(a \mid s)= \begin{cases}1 & a=a^* \\ 0 & a \neq a^*\end{cases}</script><p>where $a^*=\arg \max _a q(s, a)$(arg is get the variant value).</p>
<p><strong>In summary, the optimal policy $\pi(s)$ is the one that selects the action that has the greatest value of $q(s,a)$.</strong></p>
<p>Now that we know the solution of BOE is to maximize the right-hand side, - just select the greatest action value $q(s,a)$. But <strong>we don’t know action value or state value at this time</strong>, next , we need study how to obtain state value from the <em>contraction mapping theorem</em>  on the matrix-vector form. </p>
<h4 id="3-2-2-Matrix-vector-form-of-the-BOE"><a href="#3-2-2-Matrix-vector-form-of-the-BOE" class="headerlink" title="3.2.2 Matrix-vector form of the BOE"></a>3.2.2 Matrix-vector form of the BOE</h4><p>To leverage the <em>contraction mapping theorem</em>, we’ll express the matrix-vector form as </p>
<script type="math/tex; mode=display">
\begin{equation} 
v =\max _{\pi \in \mathcal{\pi}}\left(r_\pi+\gamma P_\pi v\right) .
\end{equation}</script><p>where $v \in \mathbb{R}^{|\mathcal{S}|}$ and $\max _\pi$ is <strong>performed in an elementwise manner</strong>. The structures of $r_\pi$ and $P_\pi$ are the same as those in the matrix-vector form of the normal Bellman equation:</p>
<script type="math/tex; mode=display">
\left[r_\pi\right]_s \doteq \sum_{a \in \mathcal{A}} \pi(a \mid s) \sum_{r \in \mathcal{R}} p(r \mid s, a) r, \quad\left[P_\pi\right]_{s, s^{\prime}}=p\left(s^{\prime} \mid s\right) \doteq \sum_{a \in \mathcal{A}} \pi(a \mid s) p\left(s^{\prime} \mid s, a\right) .</script><p>Since the optimal value of $\pi$ is determined by $v$, the right-hand side of BOE (matrix-vector form) is a function of $v$, denoted as</p>
<script type="math/tex; mode=display">
\begin{equation} 
f(v) \triangleq \max _{\pi}\left(r_\pi+\gamma P_\pi v\right) .
\end{equation}</script><p>Then, the BOE can be expressed in a concise form as</p>
<script type="math/tex; mode=display">
v=f(v)</script><p>Every row $[f(v)]_s$ is the elementwise form of $s$.</p>
<h4 id="3-2-3-Contraction-mapping-theorem"><a href="#3-2-3-Contraction-mapping-theorem" class="headerlink" title="3.2.3 Contraction mapping theorem"></a>3.2.3 Contraction mapping theorem</h4><p>Now that the matrix-vector form is expressed as a nonlinear equation $v = f(v)$, we next introduce the <em>contraction mapping theorem</em> to analyze it. The contraction mapping theorem is a powerful tool for analyzing general nonlinear equations. </p>
<p> Concepts: Fixed point and Contraction mapping</p>
<p><strong>Fixed point</strong>: $x^\ast \in X$ is a fixed point of $f: X \rightarrow X$ if</p>
<script type="math/tex; mode=display">
f(x^*)=x^*</script><p><strong>Contraction mapping</strong> (or contractive function): $f$ is a contraction mapping if</p>
<script type="math/tex; mode=display">
\left\|f\left(x_1\right)-f\left(x_2\right)\right\| \leq \gamma\left\|x_1-x_2\right\|</script><p>where $\gamma \in(0,1)$. $|\cdot|$ denotes a vector or matrix norm.</p>
<p><strong>Example1</strong><br>Given $x=f(x)=A x$, where $x \in \mathbb{R}^n, A \in \mathbb{R}^{n \times n}$ and $|A| \leq \gamma&lt;1$.<br>It is easy to verify that $x=0$ is a fixed point since $0=A 0$. To see the contraction property,</p>
<script type="math/tex; mode=display">
\left\|A x_1-A x_2\right\|=\left\|A\left(x_1-x_2\right)\right\| \leq\|A\|\left\|x_1-x_2\right\| \leq \gamma\left\|x_1-x_2\right\| .</script><p>Therefore, $f(x)=A x$ is a contraction mapping.</p>
<p><strong>Theorem: Contraction Mapping Theorem</strong><br>For any equation that has the form of $x=f(x)$,where $x$ and $f(x)$ are real vectors if $f$ is a contraction mapping, then the following peoperties hold.</p>
<ul>
<li>Existence: there exists a fixed point $x^\ast$ satisfying $f\left(x^\ast\right)=x^\ast$.</li>
<li>Uniqueness: The fixed point $x^*$ is unique.</li>
<li>Algorithm: Consider a sequence $\left\{x_k\right\}$ where $x_{k+1}=f\left(x_k\right)$, then $x_k \rightarrow x^*$( fixed point) as $k \rightarrow \infty$ for any initial guess $x_0$. Moreover, the convergence rate is exponentially fast.</li>
</ul>
<p><a href="">-&gt; See the proof</a></p>
<h3 id="3-3-Contraction-property-of-the-BOE"><a href="#3-3-Contraction-property-of-the-BOE" class="headerlink" title="3.3 Contraction property of the BOE"></a>3.3 Contraction property of the BOE</h3><p><strong>Theorem (Contraction Property)</strong>:</p>
<p>$f(v)$  is a contraction mapping satisfying</p>
<script type="math/tex; mode=display">
\left\|f\left(v_1\right)-f\left(v_2\right)\right\|_{\infty} \leq \gamma\left\|v_1-v_2\right\|_{\infty}</script><p>where $\gamma \in(0,1)$ is the discount rate, and $|\cdot|_{\infty}$ is the maximum norm, which is the maximum absolute value of the elements of a vector.</p>
<p><a href="证明链接放置">-&gt; See the proof</a></p>
<h3 id="3-4-Solution-of-the-BOE"><a href="#3-4-Solution-of-the-BOE" class="headerlink" title="3.4 Solution of the BOE"></a>3.4 Solution of the BOE</h3><p>Due to the contraction property of BOE, the matrix-vector form can be solved by computing following equation iteratively</p>
<script type="math/tex; mode=display">
\begin{equation} 
v_{k+1}=f\left(v_k\right)=\max _\pi\left(r_\pi+\gamma P_\pi v_k\right) .
\end{equation}</script><p>At every iteration, for each state, <strong>what we face is actually the elementwise form</strong>:</p>
<script type="math/tex; mode=display">
\begin{aligned}
v_{k+1}(s) & =\max _\pi \sum_a \pi(a \mid s)\left(\sum_r p(r \mid s, a) r+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right) v_k\left(s^{\prime}\right)\right) \\
& =\max _\pi \sum_a \pi(a \mid s) q_k(s, a) \\
& =\max _a q_k(s, a) .
\end{aligned}</script><blockquote>
<p><strong>Procedure summary</strong> (value iteration algorithm):<br>For every $s$, estimate(randomly select) current state value as $v_k(s)$<br>For any $a \in \mathcal{A}(s)$, calculate</p>
<script type="math/tex; mode=display">
q_k(s, a)=\sum_r p(r \mid s, a) r+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right) v_k\left(s^{\prime}\right)</script><p>Calculate the greedy policy $\pi_{k+1}$ for every $s$ as</p>
<script type="math/tex; mode=display">
\pi_{k+1}(a \mid s)=\left\{\begin{array}{cc}
1 & a=a_k^*(s) \\
0 & a \neq a_k^*(s)
\end{array}\right.</script><p>where $a_k^*(s)=\arg \max _a q_k(s, a)$.<br>Calculate $v_{k+1}(s)=\max _a q_k(s, a)$</p>
</blockquote>
<h3 id="3-5-BOE-Optimality"><a href="#3-5-BOE-Optimality" class="headerlink" title="3.5 BOE: Optimality"></a>3.5 BOE: Optimality</h3><p>Suppose $v^*$ is the solution to the Bellman optimality equation. It satisfies</p>
<script type="math/tex; mode=display">
v^*=\max _\pi\left(r_\pi+\gamma P_\pi v^*\right)</script><p>Suppose</p>
<script type="math/tex; mode=display">
\pi^*=\arg \max _\pi\left(r_\pi+\gamma P_\pi v^*\right)</script><p>Then</p>
<script type="math/tex; mode=display">
v^*=r_{\pi^*}+\gamma P_{\pi^*} v^*</script><p><strong>Therefore, $ \pi^\ast $ is a policy and $ v^\ast=v_{\pi^\ast} $,and the BOE is a special Bellman equation whose corresponding policy is $ \pi^\ast $.</strong></p>
<h3 id="3-6-What-does-pi-ast-look-like"><a href="#3-6-What-does-pi-ast-look-like" class="headerlink" title="3.6 What does $\pi^\ast$ look like?"></a>3.6 What does $\pi^\ast$ look like?</h3><p>For any $s \in \mathcal{S}$, the deterministic <strong>greedy</strong> policy</p>
<script type="math/tex; mode=display">
\pi^*(a \mid s)= \begin{cases}1 & a=a^*(s) \\ 0 & a \neq a^*(s)\end{cases}</script><p>is an <strong>optimal policy</strong> solving the BOE. Here,</p>
<script type="math/tex; mode=display">
a^*(s)=\arg \max _a q^*(a, s)</script><p>where $ q^\ast(s, a):=\sum_r p(r \mid s, a) r+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right) v^\ast\left(s^{\prime}\right) $.</p>
<p>Proof: Due to</p>
<script type="math/tex; mode=display">
\pi^*(s)=\arg \max _{\pi \in \Pi} \sum_{a \in \mathcal{A}} \pi(a \mid s) \underbrace{\left(\sum_{r \in \mathcal{R}} p(r \mid s, a) r+\gamma \sum_{s^{\prime} \in \mathcal{S}} p\left(s^{\prime} \mid s, a\right) v^*\left(s^{\prime}\right)\right)}_{q^*(s, a)}, \quad s \in \mathcal{S} .</script><p>It is clear that $ \sum_{a \in \mathcal{A}} \pi(a \mid s) q^\ast(s, a) $ is maximized if $ \pi(s) $ selects the action with the greatest $ q^\ast(s, a) $.</p>
<p>Uniqueness of optimal policies: Although the value of $ v^\ast $ is unique, the optimal policy that corresponds to $ v^\ast $ may not be unique. This can be easily verified by counterexamples. For example, the two policies shown in below Figure are both optimal.<br>Stochasticity of optimal policies: An optimal policy can be either stochastic or deterministic, as demonstrated in below Figure. However, it is certain that there always exists a deterministic optimal policy.</p>
<p><img src="/2024/08/06/Mathmatical-Foundation-of-Reinforcement-Learning-Concept-Notes/2024-04-18-22-25-49.png" alt=""></p>
<h3 id="3-7-Factors-that-influence-optimal-policies"><a href="#3-7-Factors-that-influence-optimal-policies" class="headerlink" title="3.7 Factors that influence optimal policies"></a>3.7 Factors that influence optimal policies</h3><script type="math/tex; mode=display">
\pi^*(s)=\arg \max _{\pi \in \Pi(s)} \sum_{a \in \mathcal{A}} \pi(a \mid s) \left(\sum_{r \in \mathcal{R}} p(r \mid s, a) r+\gamma \sum_{s^{\prime} \in \mathcal{S}} p\left(s^{\prime} \mid s, a\right) v^*\left(s^{\prime}\right)\right), \quad s \in \mathcal{S} .</script><p>The optimal state value and optimal policy are determined by the following parameters: 1) the immediate reward $r$, 2) the discount rate $\gamma$, and 3) the system model<br> $p(s^{\prime} \mid s,a), p(r \mid s,a)$. While the system model is fixed, we next discuss how the optimal policy varies when we change the values of $r$ and $\gamma$.  </p>
<p><strong>Impact of the discount rate and the reward values</strong></p>
<ul>
<li><p>$\gamma$ from 0 to 1 , each state from extremely short-sighted to   far-sighted </p>
</li>
<li><p>if a state must travel along a longer trajectory to reach the target, its state value is smaller due to the discount rate.Therefore, although the immediate reward of every step does not encourage the agent to approach the target as quickly as possible, the discount rate does encourage it to do so.</p>
</li>
<li><p>If we want to strictly prohibit the agent from entering any forbidden area, we can increase the punishment received for doing so.</p>
</li>
</ul>
<p>Theorem (Optimal policy invariance)(Linear variation):</p>
<p>Consider a Markov decision process with $v^\ast \in \mathbb{R}^{|\mathcal{S}|}$ as the optimal state value satisfying $v^\ast=\max _{\pi \in \Pi}\left(r_\pi+\gamma P_\pi v^\ast\right)$. If every reward $r \in \mathcal{R}$ is changed by an affine transformation to $\alpha r+\beta$, where $\alpha, \beta \in \mathbb{R}$ and $\alpha&gt;0$, then the corresponding optimal state value $v^{\prime}$ is also an affine transformation of $v^\ast$ :</p>
<script type="math/tex; mode=display">
v^{\prime}=\alpha v^*+\frac{\beta}{1-\gamma} \mathbf{1}</script><p>where $\gamma \in(0,1)$ is the discount rate and $\mathbf{1}=[1, \ldots, 1]^T$.</p>
<p>Consequently, the optimal policy derived from $v^{\prime}$ is <strong>invariant</strong> to the affine transformation of the reward values.</p>
<h3 id="3-8-Summary"><a href="#3-8-Summary" class="headerlink" title="3.8  Summary"></a>3.8  Summary</h3><p>The core concepts in this chapter include <strong>optimal policies and optimal state values</strong>. In particular, a policy is optimal if its state values are greater than or equal to those of any other policy. The state values of an optimal policy are the optimal state values. The BOE is the core tool for analyzing optimal policies and optimal state values. This equation is a nonlinear equation <strong>with a nice contraction property</strong>. We can apply the contraction mapping theorem to analyze this equation. It was shown that the solutions of the BOE correspond to the optimal state value and optimal policy. This is the reason why we need to study the BOE.</p>
<p>Q: What is the definition of optimal policies?<br>A: A policy is optimal if its corresponding state values are greater than or equal to any other policy. It should be noted that this specific definition of optimality is valid only for tabular reinforcement learning algorithms. <strong>When the values or policies are approximated by functions, different metrics must be used to define optimal policies.</strong> This will become clearer in Chapters 8 and 9.</p>
<p>Q: Are optimal policies stochastic or deterministic?<br>A: An optimal policy can be either deterministic or stochastic. A nice fact is that there always exist deterministic greedy optimal policies.</p>
<p>Q: If we increase all the rewards by the same amount, will the optimal state value change? Will the optimal policy change?<br>A: Increasing all the rewards by the same amount is an affine transformation of the rewards, which would not affect the optimal policies. However, the optimal state value would increase, as shown in (Optimal policy invariance).</p>
]]></content>
  </entry>
  <entry>
    <title>Mathematical Foundation of Reinforcement Learning-DQN algorithm</title>
    <url>/2024/08/10/Mathmatical-Foundation-of-Reinforcement-Learning-DQN-algorithm/</url>
    <content><![CDATA[<p>[toc] </p>
<p>Sources:</p>
<ol>
<li><a href="https://github.com/MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning"><em>Shiyu Zhao</em>. 《Mathematical Foundation of Reinforcement Learning》Chapter 8</a>.</li>
<li><a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#">OpenAI Spinning Up</a></li>
</ol>
<h2 id="8-Value-Function-Approximation"><a href="#8-Value-Function-Approximation" class="headerlink" title="8  Value Function Approximation"></a>8  Value Function Approximation</h2><p>we continue to study temporal-difference learning algorithms. However, a different method is used to represent state/action values. The tabular method(Q-learning) is straight forward to understand, but it is <strong>inefficient for handling large state or action spaces.</strong> To solve this problem, this chapter introduces the function approximation method, which has become <strong>the standard way to represent values.</strong> It is also where <u>artificial neural networks are incorporated into reinforcement learning as function approximates</u>. The idea of function approximation can also be extended from representing values to representing policies, as introduced in Chapter 9.</p>
<h3 id="8-1-Value-representation-From-table-to-function"><a href="#8-1-Value-representation-From-table-to-function" class="headerlink" title="8.1 Value representation: From table to function"></a>8.1 Value representation: From table to function</h3><p><strong>The tabular method</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">state</th>
<th style="text-align:center">$s_1$</th>
<th style="text-align:center">$s_2$</th>
<th style="text-align:center">…</th>
<th style="text-align:center">$s_n$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Estimated value</td>
<td style="text-align:center">$\hat{v}(s_1)$</td>
<td style="text-align:center">$\hat{v}(s_2)$</td>
<td style="text-align:center">…</td>
<td style="text-align:center">$\hat{v}(s_1)$</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Q-learning</th>
<th style="text-align:center">$a_1$</th>
<th style="text-align:center">$a_2$</th>
<th style="text-align:center">…</th>
<th style="text-align:center">$a_m$</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">$s_1$</td>
<td style="text-align:center">$q_(s_1,a_1)$</td>
<td style="text-align:center">$q_(s_1,a_2)$</td>
<td style="text-align:center">…</td>
<td style="text-align:center">$q_(s_1,a_m)$</td>
</tr>
<tr>
<td style="text-align:center">$s_2$</td>
<td style="text-align:center">$q_(s_2,a_1)$</td>
<td style="text-align:center">$q_(s_2,a_2)$</td>
<td style="text-align:center">…</td>
<td style="text-align:center">$q_(s_2,a_m)$</td>
</tr>
<tr>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
<td style="text-align:center">…</td>
</tr>
<tr>
<td style="text-align:center">$s_n$</td>
<td style="text-align:center">$q_(s_n,a_1)$</td>
<td style="text-align:center">$q_(s_n,a_2)$</td>
<td style="text-align:center">…</td>
<td style="text-align:center">$q_(s_n,a_m)$</td>
</tr>
</tbody>
</table>
</div>
<p><strong>The function approximation method</strong> (for example approximated by a straight line)</p>
<script type="math/tex; mode=display">
\hat{v}(s,w) = as+b=\underbrace{[s,1]}_{\phi^T(s)}\underbrace{\begin{bmatrix} a  \\ b  \\ \end{bmatrix}}_{w}= \phi^T(s)w.
\tag{8.1}</script><p>Here, $\hat{v}(s,w)$ is a function for approximating $v_{\pi}(s)$. It is determined jointly by the state $s$ and the <strong>parameter vector</strong> $w \in \mathbb{R}^2$. $\hat{v}(s,w)$ is sometimes written as $\hat{v}_w(s)$. Here, $\phi(s)\in \mathbb{R}^2$ is called the <strong>feature vector</strong> of $s$.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left">tabular method</th>
<th style="text-align:left">function approximation</th>
<th style="text-align:left">compared anlysis</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>retrieve a value</strong>: directly read the corresponding entry in the table.</td>
<td style="text-align:left"><strong>retrieve a value</strong>: input the state index s into the function and calculate the function value</td>
<td style="text-align:left">The function approximation method enhances <strong>storage efficiency</strong> by sacrificing accuracy.</td>
</tr>
<tr>
<td style="text-align:left"><strong>update a value</strong>: directly rewrite the corresponding entry in the table.</td>
<td style="text-align:left"><strong>update a value</strong> : update $w$ to change the values indirectly.</td>
<td style="text-align:left">the function approximation method has another merit: its <strong>generalization ability</strong> is stronger than that of the tabular method.</td>
</tr>
</tbody>
</table>
</div>
<p><strong>efficient:</strong> The function approximation method is only need to store a lower dimensional parameter vector $w$. however, not free. It comes with a cost: the state values may not be accurately represented by the function.</p>
<p><strong>generalization ability:</strong>  When using the tabular method, we can update<br> a value if the corresponding state is visited in an episode. The values of the states that have not been visited cannot be updated. However, when using the function approximation method, we need to update $w$ to update the value of a state. The update of $w$ also affects the values of some other states even though these states have not been visited. Therefore, the experience sample for one state can generalize to help estimate the values of some other states. The above analysis is illustrated in blow Figure.<br><img src="/2024/08/10/Mathmatical-Foundation-of-Reinforcement-Learning-DQN-algorithm/2024-05-29-21-57-50.png" alt=""></p>
<p>We can use more complex functions that have stronger approximation abilities than straight lines. For example, consider a second-order polynomial:</p>
<script type="math/tex; mode=display">
\hat{v}(s,w) = as^2+bs+c=\underbrace{[s^2,s,1]}_{\phi^T(s)}\underbrace{\begin{bmatrix} a  \\ b  \\ c  \\ \end{bmatrix}}_{w}= \phi^T(s)w.
\tag{8.2}</script><p>Note that $\hat{v}(s,w)$ in either (1) or (2) is linear in $w$ (though it may be nonlinear in $s$). This type of method is called <strong>linear function approximation</strong>, which is the simplest function approximation method. To realize linear function approximation, <strong>The basis is to select an appropriate feature vector $\phi(s)$</strong>.  It requires prior knowledge of the given task: the better we understand the task, the better the feature vectors we can select. However, such prior knowledge is usually unknown in practice. <strong>If we do not have any prior knowledge,a popular solution is to use artificial neural networks as nonlinear function approximations.</strong></p>
<p> <strong>The chapter’s key is how to find the optimal parameter vector $w$.</strong>. If we know $v_{\pi}(s_i)$(the real value), this is a <strong>least-squares problem</strong>. The optimal parameter can be obtained by optimizing the following objective function:</p>
<script type="math/tex; mode=display">
J_1 =\sum_{i=1}^n(\hat{v}(s_i,w)-v_{\pi}(s_i))^2=\sum_{i=1}^n(\phi^T(s_i)w-v_{\pi}(s_i))^2</script><h3 id="8-2-TD-learning-of-state-values-based-on-function-approximation"><a href="#8-2-TD-learning-of-state-values-based-on-function-approximation" class="headerlink" title="8.2 TD learning of state values based on function approximation"></a>8.2 TD learning of state values based on function approximation</h3><p>how to integrate the function approximation method into TD learning to estimate the state values(the based TD algorithm) of a given policy.This algorithm will be extended to learn action values (sarsa) and optimal policies (Q-learning) in Section 8.3. Therefore,this section is the foundation of section 8.3.</p>
<p><em>Key core of this chapter：</em><u>**How to find the optimal parameter vector $w$?</u> == <u>How to update the state/action value?**</u><br>The function approximation method is formulated as an optimization problem.<br>(1) The objective function of this problem is introduced in Section 8.2.1 <strong>(the loss function)</strong>.<br>(2) The TD learning algorithm for optimizing this objective function is introduced in Section 8.2.2. <strong>(the optimal algorithm:SGD and TD learning algorithm)</strong><br>(3) There are two ways to formulate value function $\hat{v}(s,w)$/$\hat{q}(s,a,w)$ <strong>(formulate affine model)</strong>. </p>
<ul>
<li>3.1) TD-Linear: use a linear function,we need to select appropriate feature vectors. (Section 8.2.3: the based TD algorithm; section 8.3: sarsa\Q-learning); </li>
<li>3.2) Artificial neural network as a nonlinear function approximate (Section 8.4: DQN).</li>
</ul>
<h4 id="8-2-1-Objective-function"><a href="#8-2-1-Objective-function" class="headerlink" title="8.2.1 Objective function"></a>8.2.1 Objective function</h4><p>The problem to be solved is to find an <em>optimal</em> $w$ so that $\hat{v}(s,w)$ can best approximate $v_{\pi}(s)$ for every s. In particular, the objective function is </p>
<script type="math/tex; mode=display">
J(w) =\mathbf{E}[(v_{\pi}(S)-\hat{v}(S,w))^2]
\tag{8.3}</script><p>While $S$ is a random variable, what is its probability distribution? There are several ways to define the probability distribution of $S$:</p>
<p>(1) The first way is to use a <strong>uniform distribution</strong>：(setting the probability of each state to $1/n$.)</p>
<script type="math/tex; mode=display">
J(w) =\mathbf{E}[(v_{\pi}(S)-\hat{v}(S,w))^2]= \frac{1}{n} \sum_{s \in S}(v_{\pi}(s)-\hat{v}(s,w))^2</script><p>which is the average value of the approximation errors of all the states. However, this way does not consider the real dynamics of the Markov process under the given policy. <u>Since some states may be rarely visited by a policy, it may be <em>unreasonable</em> to treat all the states as equally important.</u></p>
<p>(2) The second way is to use the <strong>stationary distribution</strong>：(setting the probability of each state to the frequency of agent access to the state.)</p>
<p>The stationary distribution describes <strong>the long-term behavior of a Markov decision process.</strong> More specifically, after the agent executes a given policy for a sufficiently long period, the probability of the agent being located at any state can be described by this stationary distribution.</p>
<script type="math/tex; mode=display">
J(w) =\mathbf{E}[(v_{\pi}(S)-\hat{v}(S,w))^2]= \sum_{s \in S}d_{\pi}(s)(v_{\pi}(s)-\hat{v}(s,w))^2</script><p>which $d_{\pi}(s)$ denote the stationary distribution of the Markov process under policy $\pi$. $\sum_{s \in S}d_{\pi}(s)=1$.</p>
<p>it was assumed that the number of states was finite in the above equation. When the state space is continuous, we can replace the summations with integrals.</p>
<p>It is notable that the value of $d_{\pi}(s)$ is nontrivial to obtain because it requires knowing the state transition probability matrix $P_{\pi}$. Fortunately, we do not need to calculate the specific value of $d_{\pi}(s)$ to minimize this objective function as shown in the next subsection. <u><strong>I.e. When optimizing based on gradient descent, the experience samples are collected by the behavior strategy, which inherently have the meaning of $d_{\pi}(s)$.</strong></u></p>
<blockquote>
<p><strong>Conditions for the uniqueness of stationary distributions.</strong><br>A general class of Markov processes that have unique stationary(or limiting)distributions is <em>irreducible</em>(or <em>regular</em>) Markov processes.<br>— A Markov process is called <em>irreducible</em> if all of its states communicate with each other.<br>— A Markov process is called <em>regular</em> if there exists $k &gt;=1$ such that $[P_{\pi}]_{ij}^k&gt;0$ for all $i,j$. States $j$ is said to be accessible from state $s_i$ if there exists a finite integer $k$ so that $[P_{\pi}]_{ij}^k&gt;0$</p>
<p> <strong>Policies that may lead to unique stationary distributions.</strong><br>Once the policy is given, a Markov decision process becomes a Markov process, whose long-term behavior is jointly determined by the given policy and the system model. Then,an important question is what kind of policies can lead to regular Markov processes? In general,the answer is <strong>exploratory policies such as $\epsilon$-greedy policies</strong>. I.e.,since all the states communicate,there sulting Markov process is irreducible. Second, since every state can transition to itself, there resulting Markov process is regular.<br>— <strong>application</strong> —<br><em>There  are two methods to calculate the stationary distributions.</em><br>— (1) The theoretical value of $d_{\pi}$ can be calculated by the state transition probability matrix $P_{\pi}$.<br>— (2) The second method is to estimated $d_{\pi}$ numerically. we start from an arbitrary initial state and generate a sufficiently long episode by following the given policy. we select $s_1$ as the starting state and run 1000 steps by following the policy. The proportion of the visits of each state during the process is shown in the below Figure(<em> represent the theoretical value of $d_{\pi}(s_i)$). <em>*It can be seen that the proportions converge to the theoretical value of $d_{\pi}$ after hundreds of steps.</em></em> It is clear clarify that the experience samples are collected by the behavior strategy, which inherently have the meaning of $d_{\pi}(s)$.<br><img src="/2024/08/10/Mathmatical-Foundation-of-Reinforcement-Learning-DQN-algorithm/2024-05-30-22-35-21.png" alt=""></p>
</blockquote>
<h4 id="8-2-2-Optimization-algorithms"><a href="#8-2-2-Optimization-algorithms" class="headerlink" title="8.2.2 Optimization algorithms"></a>8.2.2 Optimization algorithms</h4><p>To minimize the objective function $J(w)$, we can use the gradient descent algorithm:</p>
<script type="math/tex; mode=display">
w_{k+1}=w_k - \alpha_{k} \nabla_{w} J(w_k),</script><p>where</p>
<script type="math/tex; mode=display">
\begin{aligned}
\nabla_{w} J(w_k) &=\nabla_{w} \mathbb{E}[(v_{\pi}(S)-\hat{v}(S,w_k))^2] \\
& =\mathbb{E}[\nabla_{w}(v_{\pi}(S)-\hat{v}(S,w_k))^2]\\
& =2\mathbb{E}[\nabla_{w}(v_{\pi}(S)-\hat{v}(S,w_k))(-\nabla_{w}\hat{v}(S,w_k))]\\
& =-2\mathbb{E}[\nabla_{w}(v_{\pi}(S)-\hat{v}(S,w_k))\nabla_{w}\hat{v}(S,w_k)]
\end{aligned}</script><p>Therefore, the gradient descent algorithm is</p>
<script type="math/tex; mode=display">
w_{k+1}=w_k +2\alpha_{k}\mathbb{E}[(v_{\pi}(S)-\hat{v}(S,w_k))\nabla_{w}\hat{v}(S,w_k)],
\tag{8.4}</script><p>where the coefficient 2 before $\alpha_{k}$ can be merged into $\alpha_{k}$ without loss of generality. In the spirit of stochastic gradient descent, we can replace the true gradient with a stochastic gradient. Then, becomes</p>
<script type="math/tex; mode=display">
w_{t+1}=w_t +\alpha_{t}(v_{\pi}(s_t)-\hat{v}(s_t,w_t))\nabla_{w}\hat{v}(s_t,w_t),
\tag{8.5}</script><p>where $s_t$ is a sample of $S$ at time $t$. Notably, this equation is not implementable because it requires the true state value $v_{\pi}$ , which is unknown and must be estimated.</p>
<p>The following two methods can replace $v_{\pi}(s_t)$ with an approximation to make the algorithm implementable.<br>—— Monte Carlo method: Suppose that we have an episode $(s_0,r_1,s_1, r_2<br>\dots )$. Let $g_t$ be the discounted return starting from $s_t$. Then, $g_t$ can be used as an approximation of $v_{\pi} (s_t)$. The algorithm of <em>Monte Carlo learning with function approximation</em> becomes</p>
<script type="math/tex; mode=display">
w_{t+1}=w_t +\alpha_{t}(\textcolor{red}{g_t}-\hat{v}(s_t,w_t))\nabla_{w}\hat{v}(s_t,w_t),
\tag{8.6}</script><p>—— Temporal-difference method: In the spirit of TD learning, $r_{t+1} + \gamma \hat{v}(s_{t+1},w_t)$ can be used as an approximation of $v_{\pi} (s_t)$. The algorithm of <em>TD learning with function approximation</em> becomes</p>
<script type="math/tex; mode=display">
w_{t+1}=w_t +\alpha_{t}(\overbrace{\textcolor{red}{\underbrace{r_{t+1} + \gamma \hat{v}(s_{t+1},w_t)}_{TD \ \ target}}-\hat{v}(s_t,w_t)}^{TD \ \ error})\nabla_{w}\hat{v}(s_t,w_t),
\tag{8.7}</script><p>Understanding the TD learning with function approximation is important for studying the section 8.3. Notably, this equation can only learn the state values of a given policy. It will be extended to algorithms that can learn action values in Sections 8.3.</p>
<h4 id="8-2-3-TD-Linear-approximates（select-appropriate-feature-vectors）"><a href="#8-2-3-TD-Linear-approximates（select-appropriate-feature-vectors）" class="headerlink" title="8.2.3 TD-Linear approximates（select appropriate feature vectors）"></a>8.2.3 TD-Linear approximates（select appropriate feature vectors）</h4><p>The linear function:</p>
<script type="math/tex; mode=display">
\hat{v}(s,w) = \phi^T(s)w.</script><p><img src="/2024/08/10/Mathmatical-Foundation-of-Reinforcement-Learning-DQN-algorithm/2024-05-31-13-51-34.png" alt=""><br>where $\phi(s) \in \mathbb{R}^m$ is the feature vector of $s$. The lengths of $\phi(s)$ and $w$ are equal to $m$, which is usually much smaller than the number of states. In the linear case, the gradient is $\nabla_{w}\hat{v}(s,w_k)=\phi(s)$.</p>
<p>This is the algorithm of TD learning with linear function approximation. We call it <strong>TD-Linear</strong> for short.</p>
<p> The linear case is much better understood in theory than the nonlinear case. However, <u><strong>its approximation ability is limited. It is also nontrivial to select appropriate feature vectors for complex tasks.</strong></u> By contrast, artificial neural networks can approximate values as black-box universal nonlinear approximates, which are more friendly to use.</p>
<p>The reason is to learn TD-linear that a better understanding of the linear case can help readers better grasp the idea of the function approximation method. More importantly, the linear case is still powerful in the sense that <strong>the tabular method can be viewed as a special linear case</strong>. </p>
<blockquote>
<p><strong>Tabular TD learning is a special case of TD-Linear.</strong><br>Consider the following special feature vector for any $s\in S$:</p>
<script type="math/tex; mode=display">
\phi(s)=e_s \in \mathbb{R}^n</script><p>where $e_s$ is the vector with the entry corresponding to s equal to 1,and the others as 0. In this case,</p>
<script type="math/tex; mode=display">
\hat{v}(s,w) = e_s^Tw = w(s).</script><p>where $w(s)$ is the $s$th entry of $w$.</p>
<script type="math/tex; mode=display">
\begin{aligned}
w_{t+1} &=w_t +\alpha_{t}(r_{t+1} + \gamma \hat{v}(s_{t+1},w_t)-\hat{v}(s_t,w_t))\phi(s_t)\\
&=w_t +\alpha_{t}(r_{t+1} + \gamma w_t(s_{t+1})-w_t(s_t))e_{s_t}\\
&=w_t +\alpha_{t}(r_{t+1} + \gamma w_t(s_{t+1})-w_t(s_t)),
\end{aligned}</script><p>which is exactly the tabular TD algorithm.</p>
</blockquote>
<p>We next present some examples for demonstrating <em>how to use the TD-Linear algorithm to estimate the state values of a given policy?</em> . In the meantime, we demonstrate <em>how to select feature vectors?</em></p>
<p>The grid world example is shown in the below Figure 8.6. The given policy takes any action at a state with a probability of 0.2(exploratory). Our goal is to estimate the state values under this policy. There are 25 state values in total. The true state values are shown in Figure 8.6(b). The true state values are visualized as a three-dimensional surface in Figure 8.6(c)<br><img src="/2024/08/10/Mathmatical-Foundation-of-Reinforcement-Learning-DQN-algorithm/2024-05-31-14-20-06.png" alt=""></p>
<p>We next show that we can use <strong>fewer than</strong> 25 parameters to approximate these state values. The simulation setup is as follows. Five hundred episodes are generated by the given policy. Each episode has 500 steps and starts from a randomly selected state-action pair following a uniform distribution. In addition, in each simulation trial, the parameter vector $w$ is <strong>randomly initialized</strong> such that each element is drawn from a standard normal distribution with a zero mean and a standard deviation of 1. We set $r_{forbidden} = r_{boundary} = -1, r_{target} = 1$, and $\gamma = 0.9$.</p>
<p>To implement the TD-Linear algorithm, we need to select the feature vector $\phi(s)$ first. There are different ways to do that as shown below.</p>
<p><strong>— (1) The first type of feature vector is based on polynomials</strong>. In the grid world example, a state $s$ corresponds to a 2D location. Let $x$ and $y$ denote the column and row indexes of $s$, respectively. To avoid numerical issues, we normalize x and y so that their values are within the interval of $[-1,+1]$. </p>
<script type="math/tex; mode=display">
\begin{aligned}
&\phi(s)=\begin{bmatrix} a  \\ b  \\ \end{bmatrix} \in \mathbb{R}^2,\\
&\hat{v}(s,w)=\phi^T(s)w=[x,y]\begin{bmatrix} w_1  \\ w_2  \\ \end{bmatrix}=w_1x+w_2y.
\end{aligned}</script><p>When $w$ is given, $\hat{v}(s,w) = w_1x+w_2y$ represents a 2D plane that passes through the origin. Since the surface of the state values may not pass through the origin, we need to introduce a bias to the 2D plane to better approximate the state values. In this case, the approximated state value is</p>
<script type="math/tex; mode=display">
\begin{aligned}
\hat{v}(s,w)=\phi^T(s)w=[1,x,y]\begin{bmatrix} w_1\\ w_2  \\ w_3  \\ \end{bmatrix}=w_1+w_2x+w_2y.
\end{aligned}</script><p>The estimation result when we use the feature vector in $[1,x,y]$ is shown in Figure 8.7(a). Although the estimation error converges as more episodes are used, <em>the error cannot decrease to zero due to the limited approximation ability of a 2D plane.</em><br>To enhance the approximation ability, we can increase the dimension of the feature vector.<br>a quadratic 3D surface: (in Figures 8.7(b))</p>
<script type="math/tex; mode=display">
\phi(s)=[1,x,y,x^2,y^2,xy]^T \in \mathbb{R}^6</script><p>a cubic 3D surface: (in Figures 8.7(c))</p>
<script type="math/tex; mode=display">
\phi(s)=[1,x,y,x^2,y^2,xy,x^3,y^3,x^2y,xy^2]^T \in \mathbb{R}^{10}</script><p><img src="/2024/08/10/Mathmatical-Foundation-of-Reinforcement-Learning-DQN-algorithm/2024-05-31-18-47-55.png" alt=""></p>
<p>As can be seen, the longer the feature vector is, the more accurately the state values can be approximated. <u>However, in all three cases, the estimation error cannot converge to zero because these linear approximates still have limited approximation abilities.</u></p>
<p><strong>— (2) The others type of feature vector such as Fourier basis and tile coding</strong>. First, the values of $x$ and $y$ of each state are normalized to the interval of $[0,1]$. The resulting feature vector is</p>
<script type="math/tex; mode=display">
\phi(s)=\begin{bmatrix} \vdots\\ cos(\pi(c_1x+c_2y))  \\ \vdots  \\ \end{bmatrix} \in \mathbb{R}^{(q+1)^2},</script><p>where $\pi$ denotes the circumference ratio,which is $3.1415\dots$ ,instead of a policy. $c_1$ or $c_2$ can be set as any integers in ${0,1,\dots,q}$,where $q$ is a user-specified integer. As a result, there are $(q+1)^2$ possible values for the pair $(c_1 c_2)$ to take. For example, in the case of $q=1$, the feature vector is</p>
<script type="math/tex; mode=display">
\phi(s)=\begin{bmatrix} cos(\pi(0x+0y))\\ cos(\pi(0x+1y))  \\ cos(\pi(1x+0y))  \\cos(\pi(1x+1y))\\ \end{bmatrix}= \begin{bmatrix} 1\\ cos(\pi y)  \\ cos(\pi x)  \\cos(\pi(x+y))\\ \end{bmatrix}\in \mathbb{R}^{4},</script><p>The estimation results obtained when we use the Fourier features with $q=1,2,3$ are shown in Figure 8.8. As can be seen, the higher the dimension of the feature vector is, the more accurately the state values can be approximated.</p>
<p><img src="/2024/08/10/Mathmatical-Foundation-of-Reinforcement-Learning-DQN-algorithm/2024-05-31-19-05-15.png" alt=""></p>
<h4 id="8-2-4-Theoretical-analysis-of-TD-Linear-approximates"><a href="#8-2-4-Theoretical-analysis-of-TD-Linear-approximates" class="headerlink" title="8.2.4 Theoretical analysis of TD-Linear approximates"></a>8.2.4 Theoretical analysis of TD-Linear approximates</h4><p>The algorithm</p>
<script type="math/tex; mode=display">
w_{t+1}=w_t +\alpha_{t}(\textcolor{red}{r_{t+1} + \gamma \hat{v}(s_{t+1},w_t)}-\hat{v}(s_t,w_t))\nabla_{w}\hat{v}(s_t,w_t),</script><p>does not minimize the following objective function:</p>
<script type="math/tex; mode=display">
J(w) =\mathbb{E}[(v_{\pi}(S)-\hat{v}(S,w))^2]</script><p>Different objective functions:<br><strong>Objective function 1: True value error</strong></p>
<script type="math/tex; mode=display">
J_E(w) =\mathbb{E}[(v_{\pi}(S)-\hat{v}(S,w))^2]=||\hat{v}(w)-v_{\pi}||^2_D</script><p><strong>Objective function 2: Bellman error</strong></p>
<script type="math/tex; mode=display">
J_E(w) =||\hat{v}(w)-(r_{\pi}+\gamma P_{\pi}\hat{v}(w))||^2_D=||\hat{v}(w)-T_{\pi}(\hat{v}(w))||^2_D</script><p>where $T_{\pi}(x) = r_{\pi} + \gamma P_{\pi}x $</p>
<p><strong>Objective function 3: Projected Bellman error</strong></p>
<script type="math/tex; mode=display">
J_PBE(w) =||\hat{v}(w)-MT_{\pi}(\hat{v}(w))||^2_D</script><p>where $M$ is a projection matrix.<br>The TD-Linear algorithm minimizes the projected Bellman error. we can always find a value of $w$ that can minimize $J_{PBE}(w)$ to zero. Like the TD-Linear algorithm, least-squares TD algorithm (LSTD) aims to minimize the projected Bellman error. But TD algorithm and LSTD  obtain  the estimated value $\hat{v}(w)$ also close the true state value $v_{\pi}$ .   Details can be found in the book.</p>
<h3 id="8-3-TD-learning-of-action-values-based-on-function-approximation"><a href="#8-3-TD-learning-of-action-values-based-on-function-approximation" class="headerlink" title="8.3 TD learning of action values based on function approximation"></a>8.3 TD learning of action values based on function approximation</h3><p>the present section introduces how to estimate action values. The tabular Sarsa and tabular Q-learning algorithms are extended to the case of value function approximation. Readers will see that the extension is straightforward.</p>
<h4 id="8-3-1-Sarsa-with-function-approximation"><a href="#8-3-1-Sarsa-with-function-approximation" class="headerlink" title="8.3.1 Sarsa with function approximation"></a>8.3.1 Sarsa with function approximation</h4><p>The Sarsa algorithm with function approximation can be readily by replacing the state values with action values. </p>
<script type="math/tex; mode=display">
w_{t+1}=w_t +\alpha_{t}(r_{t+1} + \gamma \textcolor{red}{\hat{q}(s_{t+1},a_{t+1},w_t)}-\textcolor{red}{\hat{q}(s_t,a_t,w_t)})\nabla_{w}\textcolor{red}{\hat{q}(s_t,a_t,w_t)}.
\tag{8.8}</script><p>When linear functions are used, we have $\hat{q}(s_t,a_t,w_t)=\phi^T(s,a)w$ where $\phi(s,a)$ is a feature vector.<br>The value estimation step in the equation can be combined with a policy improvement step to learn optimal policies. The procedure is summarized in Algorithm 8.2. It is notable that this equation is executed only once before switching to the policy improvement step. This is similar to the tabular Sarsa algorithm.<br>Moreover, the implementation in Algorithm 8.2 aims to solve the task of finding a good path to the target state from a presanctified starting state. As a result, it cannot find the optimal policy for every state. However, if sufficient experience data are available, the implementation process can be easily adapted to find optimal policies for every state.</p>
<p><img src="/2024/08/10/Mathmatical-Foundation-of-Reinforcement-Learning-DQN-algorithm/2024-05-31-20-02-32.png" alt=""></p>
<p>An illustrative example is shown in Figure 8.9. In this example, the task is to find a good policy that can lead the agent to the target when starting from the top-left state. <strong>Both the total reward and the length of each episode gradually converge to steady values.</strong> In this example, the linear feature vector is selected as the Fourier function of order 5. The expression of a Fourier feature vector is given in the above equation.</p>
<p><img src="/2024/08/10/Mathmatical-Foundation-of-Reinforcement-Learning-DQN-algorithm/2024-05-31-20-05-59.png" alt=""></p>
<h4 id="8-3-2-Q-learning-with-function-approximation"><a href="#8-3-2-Q-learning-with-function-approximation" class="headerlink" title="8.3.2 Q-learning with function approximation"></a>8.3.2 Q-learning with function approximation</h4><p>Tabular Q-learning can also be extended to the case of function approximation. The update rule is</p>
<script type="math/tex; mode=display">
w_{t+1}=w_t +\alpha_{t}(\textcolor{red}{r_{t+1}+\gamma \max_{a \in \mathbb{A}(s_{t+1})}q_{t}(s_{t+1},a_{t+1},w_t)}-\hat{q}(s_t,a_t,w_t))\nabla_{w}\hat{q}(s_t,a_t,w_t).
\tag{8.9}</script><p>An on-policy version is given in Algorithm 8.3. An example for demonstrating the on-policy version is shown in Figure 8.10. In this example, the task is to find a good policy that can lead the agent to the target state from the top-left state.</p>
<p><img src="/2024/08/10/Mathmatical-Foundation-of-Reinforcement-Learning-DQN-algorithm/2024-05-31-20-12-19.png" alt=""></p>
<p>As can be seen, Q-learning with linear function approximation can successfully learn an optimal policy. Here, linear Fourier basis functions of order five are used. The off-policy version will be demonstrated when we introduce deep Q-learning in Section 8.4.</p>
<p><img src="/2024/08/10/Mathmatical-Foundation-of-Reinforcement-Learning-DQN-algorithm/2024-05-31-20-13-16.png" alt=""></p>
<p>One may notice in Algorithm 8.2 and Algorithm 8.3 that, <strong>although the values are represented as functions, the policy $\pi(a|s)$ is still represented as a table</strong>. Thus, it still assumes nite numbers of states and actions. In Chapter 9, we will see that t<strong>he policies can be represented as functions</strong> so that continuous state and action spaces can be handled.</p>
<h3 id="8-4-Deep-Q-learning"><a href="#8-4-Deep-Q-learning" class="headerlink" title="8.4 Deep Q-learning"></a>8.4 Deep Q-learning</h3><p>We can integrate deep neural networks into Q-learning to obtain an approach called <strong>deep Q-learning or deep Q-network (DQN)</strong> <sup><a href="#fn_1" id="reffn_1">1</a></sup>. Deep Q-learning is one of the earliest and most successful deep reinforcement learning algorithms. Notably, the neural networks do not have to be deep. For simple tasks such as our grid world examples, shallow networks with one or two hidden layers may be sufficient.<br>Deep Q-learning can be viewed as an <strong>extension</strong> of the Q-learning algorithm in section 8.3. <u>However, its mathematical formulation and implementation techniques are substantially different and deserve special attention.</u></p>
<blockquote id="fn_1">
<sup>1</sup>. V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie, A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis, Human-level control through deep reinforcement learning, Nature, vol. 518,<a href="#reffn_1" title="Jump back to footnote [1] in the text."> ↩</a>
</blockquote>
<p> no. 7540, pp. 529533, 2015.</p>
<h4 id="8-4-1-Algorithm-description"><a href="#8-4-1-Algorithm-description" class="headerlink" title="8.4.1 Algorithm description"></a>8.4.1 Algorithm description</h4><p>Mathematically, deep Q-learning aims to minimize the following objective function:</p>
<script type="math/tex; mode=display">
J=\mathbb{E}\left[\left(R+\gamma \max_{a \in \mathbb{A}(s^{\prime})}q_{t}(S^{\prime},a,w)-\hat{q}(S,A,w)\right)^2\right].
\tag{8.10}</script><p>where $(S,A,R,S^{\prime})$ are random variables that denote a state, an action, the immediate reward, and the next state, respectively. This objective function can be viewed as the squared Bellman optimality error. </p>
<p>To minimize the objective function in below equation, we can use the gradient descent algorithm to calculate the gradient of $J$ with respect to $w$. <strong>It is noted that the parameter $w$ appears not only in $\hat{q}(S,A,w)$ but also in $y = R+\gamma \max_{a \in \mathbb{A}(s^{\prime})}q_{t}(S^{\prime},a,w)$.</strong> it is nontrivial to calculate the gradient. For the sake of simplicity, it is assumed that the value of $w$ in $y$ is fixed (for a short period of time) so that the calculation of the gradient becomes much easier. In particular, we introduce two networks: one is a <strong><em>main network</em></strong> representing $\hat{q}(s,a,w)$ and the other is a <strong><em>target network</em></strong> $\hat{q}(s,a,w_T)$. The objective function in this case become</p>
<script type="math/tex; mode=display">
J=\mathbb{E}\left[\left(R+\gamma \max_{a \in \mathbb{A}(s^{\prime})}q_{t}(S^{\prime},a,\textcolor{red}{w_T})-\hat{q}(S,A,w)\right)^2\right].</script><p>where $w_T$ is the target networks parameter. When $w_T$ is fixed, the gradient of $J$ is</p>
<script type="math/tex; mode=display">
\nabla_wJ=-\mathbb{E}\left[\left(R+\gamma \max_{a \in \mathbb{A}(s^{\prime})}q_{t}(S^{\prime},a,w_T)-\hat{q}(S,A,w)\right)\nabla_w\hat{q}(S,A,w)\right].
\tag{8.11}</script><p>where some constant coefficients are omitted without loss of generality. <u>The use of this gradient be hidden in the optimizer of  Pytorch environment(the software tools for training neural networks).</u></p>
<p><strong>The two most important characteristics of DQN:</strong><br><strong>(1) Two networks: a main network and a target network.</strong><br> Let $w$ and $w_T$ denote the parameters of the main and target networks, respectively. They are initially set to the same value.The main network is updated in every iteration. By contrast, the target network is set to be the same as the main network every certain number of iterations to satisfy the assumption that $w_T$ is fixed when calculating the gradient in (11).<br><strong>TD target changes after each update, thereby the entire training process is like chasing a moving target.</strong></p>
<p>Deep Q-Learning算法解决了tabular<br>Q-Learning算法状态空间过大时造成的维度灾难/存储空间爆炸的问题。DQN算法中引入了深度神经网络来近似Q值函数，通过经验回放和‘目标网络-主网络’的双网络训练方式来提高算法的稳定性和收敛性。</p>
<ul>
<li>经验回放：通过从经验池中均匀随机采样构造训练样本批次，来满足常用的MSE损失函数要求的均匀分布假设。（否则采样过程服从‘稳态分布’（stationary  distribution）。）</li>
<li>目标网络-主网络：通过固定目标网络的参数，减少目标值的波动，提高算法的稳定性。</li>
</ul>
<blockquote>
<p>注：书中讲解此处时，是通过‘max’函数难以求导的问题引出的，我个人认为这是一个较容易理解的方式，但是认为这并不是‘双网络’最大的优势（回想在卷积神经网络中的最大池化操作是如何通过bp求导的呢）。因此，我更倾向于从缓解TD<br>Target中的max操作带来的‘高估’现象来解释引入‘双网络’的必要性。可具体参考以下两博客：<a href="https://blog.csdn.net/weixin_51602120/article/details/128883464">(1)</a><br>和<a href="https://blog.csdn.net/qq_40206371/article/details/124991007">(2)</a></p>
</blockquote>
<p>_Question：目标网络参数与主网络参数‘对齐’的频率对于训练结果有无影响？应该如何设置？过高的对齐频率和过低的对齐频率各有什么不足？_</p>
<p><strong>(2) experience replay and mini-batch train</strong><br>After we have collected some experience samples, we store them in a dataset called the <strong><em>replay buffer</em></strong>. In particular, let $(s,a,r,s^{\prime})$ be an experience sample and $\mathbf{B} =\lbrace(s,a,r,s^{\prime})\rbrace$ be the replay buffer. Every time we update the main network, we can draw a mini-batch of experience samples from the replay buffer. The draw of samples, or called experience replay, should follow a uniform distribution.<br>(Todo:每次随机选(有放回抽样），还是按照state-action来分类后分层随机抽样;还是说无所谓，收集样本时采用的行为策略只要足够的exploration，随机抽就行?)</p>
<p><strong>Why is experience replay necessary in deep Q-learning, and why must the replay follow a uniform distribution?</strong><br><u>The state-action samples may not be uniformly distributed in practice since they are generated as a sample sequence according to the behavior policy.If a batch of samples obtained based on the given behavior strategy is directly used for deep learning, the network will learn the preference knowledge(as opposed to generalization knowledge) of this specific batch of samples. Furthermore, it narrows the focus of learning to a specific direction in the entire space, making it difficult to extract generalized knowledge. It is necessary to break the correlation between the samples in the sequence to satisfy the assumption of uniform distribution.</u> To do this, we can use the experience replay and mini-batch train technique by uniformly drawing samples from the replay buffer. This is the mathematical reason why experience replay is necessary and why experience replay must follow a uniform distribution. </p>
<p><strong>A mini-batch of samples to train a network instead of using a single sample to update the main network ).</strong> This is one notable difference between deep and non-deep reinforcement learning algorithms. When training it with a single sample, each sample and its corresponding gradient will have too much variance, making it difficult to converge the network weights.And the computational efficiency of single sample training is lower than that of mini-bach.<br>In short, a benefit of random sampling is that each experience sample  may be used multiple times, which can increase the data efficiency. This is especially important when we have a limited amount of data.</p>
<p>The implementation procedure of deep Q-learning is summarized in Algorithm 8.3. This implementation is off-policy. It can also be adapted to become on-policy if needed.<br><img src="/2024/08/10/Mathmatical-Foundation-of-Reinforcement-Learning-DQN-algorithm/2024-05-31-23-30-17.png" alt=""></p>
<h4 id="8-4-2-Illustrative-examples"><a href="#8-4-2-Illustrative-examples" class="headerlink" title="8.4.2 Illustrative examples"></a>8.4.2 Illustrative examples</h4><p>This example aims to learn the optimal action values for every state-action pair. Once the optimal action values are obtained, the optimal greedy policy can be obtained immediately.</p>
<p>A single episode is generated by the behavior policy shown in Figure 8.11(a). This behavior policy is exploratory in the sense that it has the same probability of taking any action at any state. The episode has only 1,000 steps as shown in Figure 8.11(b). Although there are only 1,000 steps, almost all the state action pairs are visited in this episode due to the <strong>strong exploration ability</strong> of the behavior policy. The replay buffer is a set of 1,000 experience samples. The mini-batch size is 100, meaning that we uniformly draw 100 samples from the replay buffer every time we acquire samples.</p>
<p>The main and target networks have the same structure: a neural network with one hidden layer of 100 neurons (the numbers of layers and neurons can be tuned). The neural network has three inputs and one output. The first two inputs are the normalized row and column indexes of a state. The third input is the normalized action index. Here, normalization means converting a value to the interval of $[0,1]$. The output of the network is the estimated action value. The reason why we design the inputs as the row and column of a state rather than a state index is that <strong><em>we know that a state corresponds to a two-dimensional location in the grid. The more information about the state we use when designing the network, the better the network can perform.</em></strong> Moreover, the neural network can also be designed in other ways. For example, it can have two inputs and five outputs, where the two inputs are the normalized row and column of a state and the outputs are the five estimated action values for the input state.<br><img src="/2024/08/10/Mathmatical-Foundation-of-Reinforcement-Learning-DQN-algorithm/2024-05-31-23-34-00.png" alt=""><br><img src="/2024/08/10/Mathmatical-Foundation-of-Reinforcement-Learning-DQN-algorithm/2024-05-31-23-34-20.png" alt=""></p>
<p>As shown in Figure 8.11(d), the loss function, defined as the average squared TD error of each mini-batch, converges to zero, meaning that the network can fit the training samples well. As shown in Figure 8.11(e), the state value estimation error also converges to zero, indicating that the estimates of the optimal action values become sufficiently accurate. Then, the corresponding greedy policy is optimal. </p>
<p>This example demonstrates the high efficiency of deep Q-learning. In particular, a short episode of 1,000 steps is sufficient for obtaining an optimal policy here. By contrast, an episode with 100,000 steps is required by tabular Q-learning, as shown in Figure 7.4. <strong>One reason for the high efficiency is that the function approximation method has a strong generalization ability. Another reason is that the experience samples can be repeatedly used.</strong></p>
<p>We next deliberately challenge the deep Q-learning algorithm by considering a scenario with fewer experience samples. Figure 8.12 shows an example of an episode with merely 100 steps. In this example, although the network can still be well-trained in the sense that the loss function converges to zero, the state estimation error cannot converge to zero. <strong>That means the network can properly fit the given experience samples, but the experience samples are too few to accurately estimate the optimal action values.</strong><br><img src="/2024/08/10/Mathmatical-Foundation-of-Reinforcement-Learning-DQN-algorithm/2024-05-31-23-37-51.png" alt=""><br><img src="/2024/08/10/Mathmatical-Foundation-of-Reinforcement-Learning-DQN-algorithm/2024-05-31-23-38-00.png" alt=""></p>
<h3 id="8-5-Summary"><a href="#8-5-Summary" class="headerlink" title="8.5 Summary"></a>8.5 Summary</h3><p>This chapter continued introducing TD learning algorithms. However, it switches from the tabular method to the function approximation method. <em>The key to understanding the function approximation method is to know that it is an optimization problem.</em> The simplest objective function is the squared error between the true state values and the estimated values. There are also other objective functions such as the Bellman error and the projected Bellman error. We have shown that the TD-Linear algorithm actually minimizes the projected Bellman error. Several optimization algorithms such as Sarsa and Q-learning with value approximation have been introduced. </p>
<p>One reason why the value function approximation method is important is that it allows artificial neural networks to be integrated with reinforcement learning. For example, deep Q-learning is one of the most successful deep reinforcement learning algorithms.</p>
<p>An important concept named stationary distribution is introduced in this chapter. The stationary distribution plays an important role in defining an appropriate objective function in the value function approximation method. It also plays a key role in Chapter 9 when we use functions to approximate policies. An excellent introduction to this topic can be found in <sup><a href="#fn_2" id="reffn_2">2</a></sup> . </p>
<blockquote id="fn_2">
<sup>2</sup>. M. Pinsky and S. Karlin, An introduction to stochastic modeling Academic Press, 2010.[Chapter IV]<a href="#reffn_2" title="Jump back to footnote [2] in the text."> ↩</a>
</blockquote>
<p>Q: What is the stationary distribution and why is it important?<br>A: The stationary distribution describes the long-term behavior of a Markov decision process. More specifically, after the agent executes a given policy for a sufficiently long period, the probability of the agent visiting a state can be described by this stationary distribution. More information can be found in Box 8.1.</p>
<p>The reason why this concept emerges in this chapter is that it is necessary for defining a valid objective function. In particular, the objective function involves the probability distribution of the states, which is usually selected as the stationary distribution. The stationary distribution is important not only for the value approximation method but also for the policy gradient method, which will be introduced in Chapter 9.</p>
<p>Q: What are the advantages and disadvantages of the linear function approximation method?<br>A: Linear function approximation is the simplest case whose theoretical properties can be thoroughly analyzed. However, the approximation ability of this method is limited. It is also nontrivial to select appropriate feature vectors for complex tasks. By contrast, artificial neural networks can be used to approximate values as black-box universal nonlinear approximates, which are more friendly to use. Nevertheless, it is still meaningful to study the linear case to better grasp the idea of the function approximation method. Moreover, the linear case is powerful in the sense that the tabular method can be viewed as a special linear case (Box 8.2).</p>
<p>Q: Can tabular Q-learning use experience replay?<br>A: Although tabular Q-learning does not require experience replay, it can also use experience relay without encountering problems. That is because Q-learning has no requirements about how the samples are obtained due to its off-policy attribute. One benefit of using experience replay is that the samples can be used repeatedly and hence more efficiently</p>
<h2 id="Approximation-DQN-Steps"><a href="#Approximation-DQN-Steps" class="headerlink" title="Approximation (DQN Steps)"></a>Approximation (DQN Steps)</h2><h3 id="Replay-buffer"><a href="#Replay-buffer" class="headerlink" title="Replay buffer"></a>Replay buffer</h3><p>Replay buffer (or experience replay in the original paper) is used in DQN to make the samples i.i.d.<br></p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ReplayBuffer</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, capacity</span>):</span><br><span class="line">        self.buffer = deque(maxlen=capacity)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">push</span>(<span class="params">self, state, action, reward, next_state, done</span>):</span><br><span class="line">        <span class="comment"># print("State shape:", state.shape)</span></span><br><span class="line">        <span class="comment"># print("Next state shape:", next_state.shape)</span></span><br><span class="line">        </span><br><span class="line">        state      = np.expand_dims(state, <span class="number">0</span>)</span><br><span class="line">        next_state = np.expand_dims(next_state, <span class="number">0</span>)</span><br><span class="line">            </span><br><span class="line">        self.buffer.append((state, action, reward, next_state, done))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sample</span>(<span class="params">self, batch_size</span>):</span><br><span class="line">        state, action, reward, next_state, done = <span class="built_in">zip</span>(*random.sample(self.buffer, batch_size))</span><br><span class="line">        <span class="keyword">return</span> np.concatenate(state), action, reward, np.concatenate(next_state), done</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.buffer)</span><br></pre></td></tr></tbody></table></figure><p></p>
<h3 id="Target-network"><a href="#Target-network" class="headerlink" title="Target network"></a>Target network</h3><p>Firstly, it is possible to build a DQN with a single Q Network and no Target Network. In that case, we do two passes through the Q Network, first to output the Predicted Q value, and then to output the Target Q value.</p>
<p>But that could create a potential problem. The Q Network’s weights get updated at each time step, which improves the prediction of the Predicted Q value. However, since the network and its weights are the same, it also changes the direction of our predicted Target Q values. They do not remain steady but can fluctuate after each update. This is like chasing a moving target.</p>
<p>By employing a second network that doesn’t get trained, we ensure that the Target Q values remain stable, at least for a short period. But those Target Q values are also predictions after all and we do want them to improve, so a compromise is made. After a pre-configured number of time-steps, the learned weights from the Q Network are copied over to the Target Network.</p>
<p>This is like EMA?</p>
<p><strong>DQN</strong><br></p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DQN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_inputs, num_actions, device=<span class="string">'cuda'</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(DQN, self).__init__()</span><br><span class="line">        </span><br><span class="line">        self.device = device</span><br><span class="line">        self.num_inputs = num_inputs</span><br><span class="line">        self.num_actions = num_actions</span><br><span class="line">        self.layers = nn.Sequential(</span><br><span class="line">            nn.Linear(num_inputs, <span class="number">128</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">128</span>, <span class="number">128</span>),</span><br><span class="line">            nn.ReLU(),</span><br><span class="line">            nn.Linear(<span class="number">128</span>, num_actions)</span><br><span class="line">        ).to(self.device)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.layers(x)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">act</span>(<span class="params">self, state, epsilon</span>):</span><br><span class="line">        <span class="keyword">if</span> random.random() &gt; epsilon:</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():  <span class="comment"># Ensures that no gradients are computed, which saves memory and computations</span></span><br><span class="line">                state = torch.FloatTensor(state).unsqueeze(<span class="number">0</span>).to(self.device)  <span class="comment"># Convert state to tensor and add batch dimension</span></span><br><span class="line">                q_value = self.forward(state)  <span class="comment"># Get Q-values for all actions</span></span><br><span class="line">                action = q_value.<span class="built_in">max</span>(<span class="number">1</span>)[<span class="number">1</span>].item()  <span class="comment"># Get the action with the maximum Q-value and convert to integer</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            action = random.randrange(self.num_actions)</span><br><span class="line">        <span class="keyword">return</span> action</span><br></pre></td></tr></tbody></table></figure><br><strong>LOSS</strong><p></p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_td_loss</span>(<span class="params">batch_size, replay_buffer, model, gamma, optimizer</span>):</span><br><span class="line">    state, action, reward, next_state, done = replay_buffer.sample(batch_size)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Convert numpy arrays to torch tensors</span></span><br><span class="line">    state = torch.FloatTensor(state).to(model.device)</span><br><span class="line">    next_state = torch.FloatTensor(next_state).to(model.device)</span><br><span class="line">    action = torch.LongTensor(action).to(model.device)</span><br><span class="line">    reward = torch.FloatTensor(reward).to(model.device)</span><br><span class="line">    done = torch.FloatTensor(done).to(model.device)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute Q-values for current states</span></span><br><span class="line">    q_values = model(state)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute Q-values for next states using no gradient computation to speed up and reduce memory usage</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        next_q_values = model(next_state)</span><br><span class="line">        next_q_value = next_q_values.<span class="built_in">max</span>(<span class="number">1</span>)[<span class="number">0</span>]  <span class="comment"># Get the max Q-value along the action dimension</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Calculate the expected Q-values</span></span><br><span class="line">    expected_q_value = reward + gamma * next_q_value * (<span class="number">1</span> - done)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the loss between actual Q values and the expected Q values</span></span><br><span class="line">    q_value = q_values.gather(<span class="number">1</span>, action.unsqueeze(<span class="number">1</span>)).squeeze(<span class="number">1</span>)</span><br><span class="line">    loss = (q_value - expected_q_value.detach()).<span class="built_in">pow</span>(<span class="number">2</span>).mean()  <span class="comment"># Detach expected_q_value to prevent gradients from flowing</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backpropagation</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<p><strong>Training process</strong><br></p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">num_frames = <span class="number">10000</span></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">gamma      = <span class="number">0.99</span></span><br><span class="line"></span><br><span class="line">losses = []</span><br><span class="line">all_rewards = []</span><br><span class="line">episode_reward = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">state, info = env.reset()</span><br><span class="line"><span class="keyword">for</span> frame_idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, num_frames + <span class="number">1</span>):</span><br><span class="line">    epsilon = epsilon_by_frame(frame_idx)</span><br><span class="line">    action = model.act(state, epsilon)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># print(f"Select action: {action}. type: {type(action)}")</span></span><br><span class="line">    next_state, reward, terminated, truncated, info = env.step(action)</span><br><span class="line"> </span><br><span class="line">    </span><br><span class="line">    replay_buffer.push(state, action, reward, next_state, terminated)</span><br><span class="line">    </span><br><span class="line">    state = next_state</span><br><span class="line">    episode_reward += reward</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> terminated:</span><br><span class="line">        state, info = env.reset()</span><br><span class="line">        all_rewards.append(episode_reward)</span><br><span class="line">        episode_reward = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(replay_buffer) &gt; batch_size:</span><br><span class="line">        loss = compute_td_loss(batch_size, replay_buffer, model, gamma, optimizer)</span><br><span class="line">        losses.append(loss.data.item())</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="keyword">if</span> frame_idx % <span class="number">200</span> == <span class="number">0</span>:</span><br><span class="line">        plot(frame_idx, all_rewards, losses)</span><br></pre></td></tr></tbody></table></figure><p></p>
]]></content>
  </entry>
  <entry>
    <title>Mathematical Foundation of Reinforcement Learning — REINFORCE and AC algorithms</title>
    <url>/2024/08/11/Mathmatical-Foundation-of-Reinforcement-Learning-REINFORCE-and-AC-algorithms/</url>
    <content><![CDATA[<p> [toc] </p>
<p>Sources:</p>
<ol>
<li><a href="https://github.com/MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning"><em>Shiyu Zhao</em>. 《Mathematical Foundation of Reinforcement Learning》Chapter 9 and 10</a>.</li>
<li><a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#">OpenAI Spinning Up</a></li>
</ol>
<h2 id="9-Policy-Gradient-Methods"><a href="#9-Policy-Gradient-Methods" class="headerlink" title="9 Policy Gradient Methods"></a>9 Policy Gradient Methods</h2><p>So far in this book, policies have been represented by tables: the action probabilities of all states are stored in a table (e.g., Table 9.1). In this chapter, we show that <strong><em><u>policies can be represented by parameterized functions denoted as $\pi(a|s,\theta)$</u></em></strong>, where $\theta \in \mathbb{R}^m$ is a parameter vector. It can also be written in other forms such as $\pi_{\theta}(a|s)$, $\pi_{\theta}(a,s)$, or $\pi(a,s,\theta)$.<br><img src="/2024/08/11/Mathmatical-Foundation-of-Reinforcement-Learning-REINFORCE-and-AC-algorithms/2024-06-05-20-53-42.png" alt=""><br>When policies are represented as functions, <strong>optimal policies can be obtained by optimizing certain scalar metrics.</strong> Such a method is called <em>policy gradient</em>.The policy gradient method is a big step forward in this book because it is policy-based. The advantages of the policy gradient method are numerous. For example, it is <strong>more efficient for handling large state/action spaces.</strong> It has <strong>stronger generalization abilities</strong> and hence is more efficient in terms of sample usage.</p>
<h3 id="9-1-Policy-representation-From-table-to-function"><a href="#9-1-Policy-representation-From-table-to-function" class="headerlink" title="9.1 Policy representation:From table to function"></a>9.1 Policy representation:From table to function</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">Difference</th>
<th style="text-align:center">Tabular case</th>
<th style="text-align:center">Function representation</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">how to define optimal policies?</td>
<td style="text-align:center">maximize every state value.</td>
<td style="text-align:center">maximize certain scalar metrics.</td>
</tr>
<tr>
<td style="text-align:center">how to update a policy?</td>
<td style="text-align:center">directly changing the entries in the table.</td>
<td style="text-align:center">changing the parameter $\theta$</td>
</tr>
<tr>
<td style="text-align:center">how to retrieve the probability of an action?</td>
<td style="text-align:center">directly obtained by looking up the table.</td>
<td style="text-align:center">input $(s,a)$ or $s$ into the function to calculate its probability(e.g.,Table 9.2)</td>
</tr>
</tbody>
</table>
</div>
<p>The basic idea of the policy gradient method is summarized below. Suppose that <strong>$J(\theta)$ is a scalar metric.</strong> Optimal policies can be obtained by optimizing this metric via the gradient-based algorithm:</p>
<script type="math/tex; mode=display">
\begin{equation}
\theta_{t+1} = \theta_{t}+\alpha \nabla_\theta J(\theta_t),
\end{equation}</script><p>where $\nabla_\theta J$ is the gradient of $J$ with respect to $\theta$ , $t$ is the time step, and $\alpha$ is the optimization rate.<br>With this basic idea, we will answer the following three questions in the remainder of this chapter.</p>
<ol>
<li>What <strong>metrics</strong> should be used? (Section 9.2).</li>
<li>How to <strong>calculate the gradients</strong> of the metrics? (Section 9.3)</li>
<li>How to <strong>use experience samples</strong> to calculate the gradients? (Section 9.4)</li>
</ol>
<h3 id="9-2-Metrics-for-defining-optimal-policies"><a href="#9-2-Metrics-for-defining-optimal-policies" class="headerlink" title="9.2 Metrics for defining optimal policies"></a>9.2 Metrics for defining optimal policies</h3><p>If a policy is represented by a function, there are two types of metrics for defining optimal policies. One is based on state values and the other is based on immediate rewards.</p>
<p><strong>Metric 1: Average state value(Average value)</strong></p>
<script type="math/tex; mode=display">
\begin{equation}
\bar{v}_\pi = \sum_{s \in S}d(s)v_{\pi}(s)=\mathbb{E}_{S \sim d}[v_\pi(S)],
\end{equation}</script><p>where $d(s)$ is the weight of state $s$ (probability distribution).It satisfies $d(s)\geq0$ for any $s \in S$ and $\sum_{s \in S}d(s) = 1$.<br>How to select the distribution $d$ ? There are two cases.</p>
<ol>
<li><p>The first and simplest case is that $d$ is <strong>independent</strong> of the policy $\pi$. In this case, we specifically denote $d$ as $d_0$ and $\bar{v}_{\pi}$ as $\bar{v}_{\pi}^0$ to indicate that the distribution is independent of the policy.<br>——- One case is to treat all the states equally important and select $d_0(s) = 1/|S|$. (<strong>random</strong>)<br>——- Another case is when we are <strong>only</strong> interested in a specific state $s_0$  (e.g., the agent always starts from $s_0$). In this case, we can design $d_0(s_0)=1, d_0(s \neq s_0)=0 $.</p>
</li>
<li><p>The second case is that $d$ is <strong>dependent</strong> on the policy $\pi$.In this case, it is common to select $d$ as $d_{\pi}$ , which is the <strong>stationary distribution</strong> under $\pi$.<br>The interpretation of selecting $d_{\pi}$ is as follows. The stationary distribution reflects the long-term behavior of a Markov decision process under a given policy. If one state is frequently visited in the long term, it is more important and deserves a higher weight; if a state is rarely visited, then its importance is low and deserves a lower weight.</p>
</li>
</ol>
<p>Suppose that an agent collects rewards $\{R_{t+1}\}_{t=0}^\infty$ by following a given policy $\pi(\theta)$. Readers may often see the following metric in the literature(red and blue):</p>
<script type="math/tex; mode=display">
\begin{aligned}
J(\theta) &= \textcolor{red}{\lim_{n \to \infty}\mathbb{E}\left[\sum_{t=0}^n \gamma^t R_{t+1}\right ]} = \textcolor{blue}{\mathbb{E}\left[\sum_{t=0}^\infty \gamma^t R_{t+1}\right]},\\
&= \sum_{s \in S}d(s)\mathbb{E}\left[ \sum_{t=0}^\infty \gamma^t R_{t+1}|S_0 =s \right]\\
& = \sum_{s \in S}d(s)v_{\pi}(s)\\
& = \textcolor{green}{\bar{v}_{\pi}}
\end{aligned}
\tag{3}</script><p><strong>Metric 2: Average reward (Average one-step reward)</strong></p>
<script type="math/tex; mode=display">
\begin{aligned}
\bar{r}_\pi = \sum_{s \in S}d_{\pi}(s)r_{\pi}(s)&=\mathbb{E}_{S \sim d_{\pi}}[r_\pi(S)],\\
&= \mathbb{E}_{S \sim d_{\pi}}\left[\sum_{a \in A}\pi(a|s,\theta)r(s,a)\right]\\
&= \mathbb{E}_{S \sim d_{\pi},A \sim \pi(s,\theta)}\left[r(s,A)|s\right]\\
\end{aligned}
\tag{4}</script><p>where $d_{\pi}$ is the stationary distribution, $r_{\pi}(s)$ is the expectation of the immediate rewards.</p>
<p>Suppose that an agent collects rewards $\{R_{t+1}\}_{t=0}^\infty$ by following a given policy $\pi(\theta)$. A common metric that readers may often see in the literature is</p>
<script type="math/tex; mode=display">
\begin{aligned}
J(\theta) = \textcolor{red}{\lim_{n \to \infty}\frac{1}{n}\mathbb{E}\left[\sum_{t=0}^{n-1} R_{t+1}\right ]} &= \lim_{n \to \infty}\frac{1}{n}\sum_{s \in S}d(s) \mathbb{E}\left[\sum_{t=0}^{n-1} R_{t+1}|S_0=s \right ]\\
&= \sum_{s \in S}d(s)\lim_{n \to \infty}\frac{1}{n}\sum_{t=0}^{n-1}\mathbb{E}\left[ R_{t+1}\right |S_0=s_0 ]\\
&= \sum_{s \in S}d(s)\lim_{n \to \infty}\mathbb{E}\left[ R_{t+1}\right |S_0=s_0 ]\\
&= \sum_{s \in S}d(s)\sum_{s \in S}d_{\pi}(s)r_{\pi}(s)\\
&= \sum_{s \in S}d_{\pi}(s)\bar{r}_{\pi}\\
& = \textcolor{green}{\bar{r}_{\pi}}
\end{aligned}
\tag{5}</script><p>Up to now, we have introduced two types of metrics: $\bar{v}_{\pi}$ and $\bar{r}_{\pi}$ . Each metric has several different but equivalent expressions. They are summarized in Table 9.2.<br><img src="/2024/08/11/Mathmatical-Foundation-of-Reinforcement-Learning-REINFORCE-and-AC-algorithms/2024-06-05-22-13-35.png" alt=""></p>
<p>All these metrics are functions of $\pi$. Since $\pi$ is parameterized by $\theta$, these metrics are functions of $\theta$. In other words, <strong>different values of $\theta$ can generate different metric values.</strong> Therefore, we can search for the optimal values of $\theta$ to maximize these metrics. This is the basic idea of policy gradient methods.<br>The two metrics $\bar{v}_{\pi}$ and $\bar{r}_{\pi}$ are equivalent in the discounted case where $\gamma&lt; 1$. In particular, it can be shown that</p>
<script type="math/tex; mode=display">
\begin{equation}\tag{6}
\bar{r}_\pi = (1-\gamma)\bar{v}_\pi
\end{equation}</script><blockquote>
<p>Note that $\bar{v}_{\pi}(\theta) = d_{\pi}^Tv_{\pi}$ and $\bar{r}_{\pi}(\theta) = d_{\pi}^Tr_{\pi}$ , where $v_{\pi}$ and $r_{\pi}$ satisfy the Bellman equation $v_{\pi} = r_{\pi} + \gamma P_{\pi}v_{\pi}$ . Multiplying $d_{\pi}^T$ on both sides of the Bellman equation yields</p>
<script type="math/tex; mode=display">
\bar{v}_\pi = \bar{r}_\pi + \gamma d_{\pi}^TP_{\pi}v_{\pi}=\bar{r}_\pi + \gamma d_{\pi}^Tv_{\pi}=\bar{r}_\pi +\gamma \bar{v}_\pi \text{     (QED)}</script></blockquote>
<p><strong>The above equation indicates that these two metrics can be simultaneously maximized.</strong></p>
<h3 id="9-3-Gradients-of-the-metrics"><a href="#9-3-Gradients-of-the-metrics" class="headerlink" title="9.3 Gradients of the metrics"></a>9.3 Gradients of the metrics</h3><p>Given the metrics introduced in the last section, we can use gradient-based methods to maximize them. <strong>we first provide the policy gradient theorem(gradent ascent),and then provide the inference process.</strong><br>Theorem 9.1 (Policy gradient theorem). The gradient of $J(\theta )$ is</p>
<script type="math/tex; mode=display">
\begin{aligned}
\nabla_{\theta} J(\theta) &=\textcolor{blue}{\sum_{s \in S}\eta(s)\sum_{a \in A}\nabla_{\theta}\pi(a|s,\theta)q_{\pi}(s,a)}\\
&=\mathbb{E}_{S \sim\eta}\left[ \sum_{a \in A}\nabla_{\theta}\pi(a|S,\theta)q_{\pi}(S,a)\right]\\
&=\mathbb{E}_{S \sim\eta}\left[ \sum_{a \in A}\pi(a|S,\theta)\nabla_{\theta}\ln\pi(A|S,\theta)q_{\pi}(S,A)\right]\\
&=\textcolor{red}{\mathbb{E}_{S \sim\eta,A\sim\pi(S,\theta)}\left[ \nabla_{\theta}\ln\pi(A|S,\theta)q_{\pi}(S,A)\right]}\\
&\approx\textcolor{green}{ \nabla_{\theta}\ln\pi(a|s,\theta)q_{\pi}(s,a)}\text{   (stochastic gradient descent)}
\end{aligned}
\tag{7}</script><p>where is $\eta$ a state distribution and $\nabla_{\theta}\pi$ is the gradient of $\pi$ with respect to $\theta$.  $\ln$ is the natural logarithm. <strong>A natural logarithm function is introduced to express the gradient as an expected value $ \nabla_{\theta}\ln A(\theta)=\frac{\nabla_{\theta}A(\theta)}{A(\theta)} $. In this way, we can approximate the true gradient with a stochastic one.</strong> It is notable that $\pi(a|s,\theta)$must be positive for all $(s,a)$ to ensure that $\ln\pi(a|s,\theta)$ is valid.This can be achieved by using <em>softmax functions(Relu functions)</em>:</p>
<script type="math/tex; mode=display">
\begin{equation}
\pi(a|s,\theta)=\frac{e^{h(s,a,\theta)}}{\sum_{a'\in A}e^{h(s,a',\theta)}},a \in A
\end{equation}
\tag{8}</script><p>Since $\pi(a|s,\theta) &gt; 0$ for all $a$, the parameterized policy is <strong><em>stochastic</em></strong> and hence <strong><em>exploratory</em></strong>.</p>
<p>Theorem 9.1 is a summary of the results in Theorem 9.2, 9.3,and 9.5. These three theorems address different scenarios involving different metrics and discounted/undiscounted cases, but the gradients in these scenarios all have similar expressions. In particular, $J(\theta)$ could be $\bar{v}_{\pi}^0$, $\bar{v}_{\pi}$, or $\bar{r}_{\pi}$. The equality in Theorem 9.1 may become a strict equality or an approximation. The distribution also varies in different scenarios.</p>
<ol>
<li>distinguish different metrics $\bar{v}_{\pi}^0$, $\bar{v}_{\pi}$, or $\bar{r}_{\pi}$</li>
<li>distinguish the discounted and undiscounted cases.<h4 id="9-3-1-Derivation-of-the-gradients-in-the-discounted-case"><a href="#9-3-1-Derivation-of-the-gradients-in-the-discounted-case" class="headerlink" title="9.3.1 Derivation of the gradients in the discounted case"></a>9.3.1 Derivation of the gradients in the discounted case</h4><strong>Theorem 9.2</strong> (Gradient of $\bar{v}_{\pi}^0$ in the discounted case). In the discounted case where $\gamma \in (0,1)$, the gradient of $\bar{v}_{\pi}^0= d_0^Tv_{\pi}$ is</li>
</ol>
<script type="math/tex; mode=display">
\begin{aligned}
\nabla_{\theta} \bar{v}_{\pi}^0 &=\textcolor{blue}{\sum_{s \in S}d_0(s)\sum_{a \in A}\nabla_{\theta}v_{\pi}(s)}\\
&=\sum_{s \in S}d_0(s)\sum_{s'\in S}Pr_{\pi}(s'|s)\sum_{a \in A}\nabla_{\theta}\pi(a|s',\theta)q_{\pi}(s',a)\\
&=\sum_{s' \in S}\left(\sum_{s\in S}d_0(s)Pr_{\pi}(s'|s)\right)\sum_{a \in A}\nabla_{\theta}\pi(a|s',\theta)q_{\pi}(s',a)\\
&=\sum_{s' \in S}\rho_{\pi}(s')\sum_{a \in A}\nabla_{\theta}\pi(a|s',\theta)q_{\pi}(s',a)\\
&=\sum_{s \in S}\rho_{\pi}(s)\sum_{a \in A}\nabla_{\theta}\pi(a|s,\theta)q_{\pi}(s,a) \text{ (change $s'$ to $s$)}\\
&=\sum_{s \in S}\rho_{\pi}(s)\sum_{a \in A} \pi(a|s,\theta)\nabla_{\theta}\ln\pi(a|s,\theta)q_{\pi}(s,a) \\
&=\textcolor{red}{\mathbb{E}_{S \sim\rho_{\pi},A\sim\pi(S,\theta)}\left[ \nabla_{\theta}\ln\pi(A|S,\theta)q_{\pi}(S,A)\right]}\\
&\approx\textcolor{green}{ \nabla_{\theta}\ln\pi(a|s,\theta)q_{\pi}(s,a)}
\end{aligned}
\tag{9}</script><p><strong>Theorem 9.3</strong> (Gradients of $\bar{r}_{\pi}$ and $\bar{v}_{\pi}$ in the discounted case). In the discounted case where $\gamma \in (0,1)$, the gradients of $\bar{r}_{\pi}$ and $\bar{v}_{\pi}$ are</p>
<script type="math/tex; mode=display">
\begin{aligned}
\nabla_{\theta} \bar{r}_{\pi} &=(1-\gamma)\nabla_{\theta} \bar{v}_{\pi}=(1-\gamma)\nabla_{\theta}\sum_{s \in S}d_{\pi}(s)v_{\pi}(s)\\
&=(1-\gamma)\left[\sum_{s \in S}\nabla_{\theta}d_{\pi}(s)v_{\pi}(s) +\sum_{s \in S}d_{\pi}(s)\nabla_{\theta}v_{\pi}(s)\right]\\
&=(1-\gamma)\left[\sum_{s \in S}\nabla_{\theta}d_{\pi}(s)v_{\pi}(s) + \frac{1}{1-\gamma}\sum_{s \in S}d_{\pi}(s)\sum_{a \in A}\nabla_{\theta} \pi(a|s,\theta)q_{\pi}(s,a)\right]\\
&\approx(1-\gamma)\left[ 0+\frac{1}{1-\gamma}\sum_{s \in S}d_{\pi}(s)\sum_{a \in A}\nabla_{\theta} \pi(a|s,\theta)q_{\pi}(s,a) \text{  (when $\gamma \to 1$) }\right]\\
&=\sum_{s \in S}d_{\pi}(s)\sum_{a \in A}\nabla_{\theta} \pi(a|s,\theta)q_{\pi}(s,a)\\
&=\sum_{s \in S}d_{\pi}(s)\sum_{a \in A} \pi(a|s,\theta)\nabla_{\theta}\ln\pi(a|s,\theta)q_{\pi}(s,a) \\
&=\textcolor{red}{\mathbb{E}_{S \sim d_{\pi},A\sim\pi(S,\theta)}\left[ \nabla_{\theta}\ln\pi(A|S,\theta)q_{\pi}(S,A)\right]}\\
&\approx\textcolor{green}{ \nabla_{\theta}\ln\pi(a|s,\theta)q_{\pi}(s,a)}
\end{aligned}
\tag{10}</script><p>Here, the approximation is more accurate when $\gamma$ is closer to 1.</p>
<h4 id="9-3-2-Derivation-of-the-gradients-in-the-undiscounted-case"><a href="#9-3-2-Derivation-of-the-gradients-in-the-undiscounted-case" class="headerlink" title="9.3.2 Derivation of the gradients in the undiscounted case"></a>9.3.2 Derivation of the gradients in the undiscounted case</h4><p>the undiscounted case means $ \gamma = 1$. Why we suddenly start considering the undiscounted case,The reasons are as follows. First, for <strong>continuing tasks</strong>, it may be inappropriate to introduce the discount rate and we need to consider the undiscounted case.  Second, While the gradient of $\bar{r}_{\pi}$ in the discounted case is an approximation, we will see that its gradient in <strong>the undiscounted case is more elegant</strong>.<br><strong>Theorem 9.3</strong> (Gradient of $\bar{r}_{\pi}$ in the undiscounted case). In the undiscounted case, the gradient of the average reward $\bar{r}_{\pi}$ is</p>
<script type="math/tex; mode=display">
\begin{aligned}
\nabla_{\theta} \bar{r}_{\pi} &= \sum_{s \in S}d_{\pi}(s)\sum_{a \in A}\nabla_{\theta} \pi(a|s,\theta)q_{\pi}(s,a) \\
&=\sum_{s \in S}d_{\pi}(s)\sum_{a \in A} \pi(a|s,\theta)\nabla_{\theta}\ln\pi(a|s,\theta)q_{\pi}(s,a) \\
&=\textcolor{red}{\mathbb{E}_{S \sim d_{\pi},A\sim\pi(S,\theta)}\left[ \nabla_{\theta}\ln\pi(A|S,\theta)q_{\pi}(S,A)\right]}\\
&\approx\textcolor{green}{ \nabla_{\theta}\ln\pi(a|s,\theta)q_{\pi}(s,a)}
\end{aligned}
\tag{11}</script><div class="table-container">
<table>
<thead>
<tr>
<th>metric</th>
<th>derivative of the metric</th>
</tr>
</thead>
<tbody>
<tr>
<td>general case$J(\theta)$</td>
<td>$ \nabla_{\theta}J(\theta)=\sum \eta(s)\sum\nabla_{\theta}\pi(a\</td>
<td>s,\theta)q_{\pi}(s,a)=E(\nabla ln \pi(a\</td>
<td>s,\theta)q_{\pi}(s,a)) $</td>
</tr>
<tr>
<td>$\bar v_{\pi}^0=d_0^Tv_{\pi}$</td>
<td>$ \nabla_{\theta} \bar v_{\pi}^0=\sum d_0(s^{‘})[(I-\gamma P_{\pi})^{-1}]_{s’s}\sum \pi(a\</td>
<td>s,\theta)\nabla_{\theta}ln{\pi}(a\</td>
<td>s,\theta)q_{\pi}(s,a) $</td>
</tr>
<tr>
<td>$\bar v_{\pi}=d_{\pi}^T \bar v_{\pi}$</td>
<td>$ \nabla_{\theta} \bar v_{\pi}=(1-\gamma)^{-1} \sum d_{\pi}(s)\sum \nabla_{\theta}\pi (a\</td>
<td>s,\theta)q_{\pi}(s,a) $</td>
</tr>
<tr>
<td>$\bar r_{\pi}=d_{\pi}^T \bar r_{\pi}$</td>
<td>$ \nabla_{\theta} \bar r_{\pi}= (1-\gamma) \nabla_{\theta} \bar v_{\pi} $</td>
</tr>
</tbody>
</table>
</div>
<h3 id="9-4-Monte-Carlo-policy-gradient-REINFORCE"><a href="#9-4-Monte-Carlo-policy-gradient-REINFORCE" class="headerlink" title="9.4 Monte Carlo policy gradient (REINFORCE)"></a>9.4 Monte Carlo policy gradient (REINFORCE)</h3><p>The gradient-ascent algorithm for maximizing $J(\theta)$ is</p>
<script type="math/tex; mode=display">
\begin{aligned}
\theta_{t+1} &=\theta_t +\alpha \nabla_{\theta}J(\theta_t)\\
&=\theta_t +\alpha \mathbb{E}\left[ \nabla_{\theta}\ln\pi(A|S,\theta_t)q_{\pi}(S,A)\right]\\
&\approx \theta_t +\alpha\textcolor{green}{ \nabla_{\theta}\ln\pi(a_t|s_t,\theta_t)q_{t}(s_t,a_t)} \text{ (stochastic gradient)}
\end{aligned}
\tag{12}</script><p>where $q_t(s_t, a_t)$ is an approximation of $q_{\pi}(s_t, a_t)$. If $qt(st at)$ is obtained by Monte Carlo estimation, the algorithm is called <strong>REINFORCE</strong> <sup><a href="#fn_1" id="reffn_1">1</a></sup> or Monte Carlo policy gradient, which is one of earliest and simplest policy gradient algorithms.<br>REINFORCE is important since many other policy gradient algorithms can be obtained by extending it.<br>We next examine the interpretation of (11) more closely. Since $\nabla_{\theta}\ln\pi(a_t|s_t,\theta_t)=\frac{\nabla_{\theta}\pi(a_t|s_t,\theta_t)}{\pi(a_t|s_t,\theta_t)}$ , we can rewrite (11) as</p>
<script type="math/tex; mode=display">
\begin{aligned}
\theta_{t+1} &=\theta_t +\alpha \left( \frac{q_t(s_t,a_t)}{\pi(a_t|s_t,\theta_t)} \right)\nabla_{\theta}\pi(a_t|s_t,\theta_t)\\
&=\theta_t +\alpha \beta_t \nabla_{\theta}\pi(a_t|s_t,\theta_t)
\end{aligned}
\tag{13}</script><p>Two important interpretations can be seen from this equation.</p>
<ol>
<li><p>If $\beta_t \geq 0$,the probability of choosing $(s_t,a_t)$ is enhanced. $\pi(a_t|s_t,\theta_{t+1})\geq \pi(a_t|s_t,\theta_{t})$, The greater $\beta_t$ is, the stronger the enhancement is.<br>If $\beta_t \leq 0$,the probability of choosing $(s_t,a_t)$ decreases. $\pi(a_t|s_t,\theta_{t+1})\leq \pi(a_t|s_t,\theta_{t})$.</p>
</li>
<li><p>The algorithm can strike a balance between exploration and exploitation.<br>$\beta_t$ is proportional to $q_t(s_t,a_t)$. If the action value of $(s_t,a_t)$ is large, then $\theta_{t+1}$ is change to make $\pi(a_t|s_t,\theta_t)$ enhance so that the probability of selecting at $a_t$ increases. Therefore, the algorithm attempts to <strong>exploit actions with greater values</strong>.<br>$\beta_t$ is inversely proportional to $\pi(a_t|s_t,\theta_t)$ when $q_t(s_t,a_t)&gt;0$. As a result, if the probability of selecting $a_t$ is small, then $\beta_t$ is enhanced so that $\theta_{t+1}$ make the probability of selecting $a_t$ increases ($\pi(a_t|s_t,\theta_t)$). Therefore, the algorithm attempts to <strong>explore actions with low probabilities.</strong></p>
</li>
</ol>
<p>Moreover, since (12) uses samples to approximate the true gradient, it is important to understand how the samples should be obtained.</p>
<ol>
<li><p>How to sample $S$? $S$ in the true gradient $\mathbb{E}[\nabla_\theta \ln\pi(A|S, \theta_t)q_{\pi}(S,A)]$ should obey the distribution $\eta$ which is either the stationary distribution $d_{\pi}$ or the discounted total probability distribution $\rho_{\pi}$ in (9). Either $d_{\pi}$ or $\rho_{\pi}$ represents the long-term behavior exhibited under $\pi$.</p>
</li>
<li><p>How to sample $A$? $A$ in $\mathbb{E}[\nabla_\theta \ln\pi(A|S, \theta_t)q_{\pi}(S,A)]$ should obey the distribution of $\pi(A|S,\theta)$. The ideal way to sample $A$ is to select at following $\pi(a|s_t, \theta_t)$. Therefore,<strong> the policy gradient algorithm is on-policy.</strong></p>
</li>
</ol>
<p>Unfortunately, the ideal ways for sampling $S$ and $A$ are not strictly followed in practice due to their low efficiency of sample usage.A more sample-efficient implementation of (12) is given in Algorithm 9.1. In this implementation, an episode is first generated by following $\pi(\theta)$. Then, $\theta$ is updated multiple times using every experience sample in the episode.<br><img src="/2024/08/11/Mathmatical-Foundation-of-Reinforcement-Learning-REINFORCE-and-AC-algorithms/2024-06-07-14-31-39.png" alt="">(图片有笔误，计算动作值时从后往前计算$t=T-1,T-2,…,1,0$)</p>
<p><sup><a href="#fn_1" id="reffn_1">1</a></sup>:R. J. Williams, Simple statistical gradient-following algorithms for connectionist reinforcement learning, Machine Learning, vol. 8, no. 3, pp. 229256, 1992.</p>
<h3 id="9-5-Summary"><a href="#9-5-Summary" class="headerlink" title="9.5 Summary"></a>9.5 Summary</h3><p>This chapter introduced the policy gradient method, which is the foundation of many modern reinforcement learning algorithms. Policy gradient methods are <strong><em>policy-based</em></strong>.The basic idea of the policy gradient method is simple. That is to select an appropriate scalar metric and then optimize it via a gradient-ascent algorithm.<br>The most complicated part of the policy gradient method is the derivation of the gradients of the metrics. That is because we have to distinguish various scenarios with different metrics and discounted/undiscounted cases. Fortunately, the expressions of the gradients in different scenarios are similar.Hence, we summarized the expressions in Theorem 9.1, which is the most important theoretical result in this chapter.<br>The policy gradient algorithm in (12) must be properly understood since it is the foundation of many advanced policy gradient algorithms. In the next chapter, this algorithm will be extended to another important policy gradient method called actor-critic.</p>
<h2 id="10-Actor-Critic-Methods"><a href="#10-Actor-Critic-Methods" class="headerlink" title="10 Actor-Critic Methods"></a>10 Actor-Critic Methods</h2><p>“actor-critic” refers to a structure that incorporates both policy-based and value-based methods. An <strong><em>actor</em></strong> refers to a policy update step. The reason that it is called an actor is that the actions are taken by following the policy. Here, an <strong><em>critic</em></strong> refers to a value update step. It is called a critic because it criticizes the actor by evaluating its corresponding values. In addition, actor-critic methods are still policy gradient algorithms.</p>
<h3 id="10-1-The-simplest-actor-critic-algorithm-QAC"><a href="#10-1-The-simplest-actor-critic-algorithm-QAC" class="headerlink" title="10.1 The simplest actor-critic algorithm (QAC)"></a>10.1 The simplest actor-critic algorithm (QAC)</h3><p>The optimal algorithm used by <strong>QAC</strong> is the same as Section 9.4 Monte Carlo policy gradient (<strong>REINFORCE</strong>)<br>The gradient-ascent algorithm for maximizing $J(\theta)$ is</p>
<script type="math/tex; mode=display">
\begin{aligned}
\theta_{t+1} &=\theta_t +\alpha \nabla_{\theta}J(\theta_t)\\
&=\theta_t +\alpha \mathbb{E}_{S \sim \eta,A \sim \pi}\left[ \nabla_{\theta}\ln\pi(A|S,\theta_t)q_{\pi}(S,A)\right]\\
&\approx \theta_t +\alpha\textcolor{green}{ \nabla_{\theta}\ln\pi(a_t|s_t,\theta_t)q_{t}(s_t,a_t)} \text{ (stochastic gradient)}
\end{aligned}
\tag{10.2}</script><p>On the one hand, it is a <strong>policy-based algorithm</strong> since it directly updates the policy parameter ($\theta$). On the other hand, this equation requires knowing $q_t(s_t, a_t)$, which is an estimate of the action value $q_t(s_t, a_t)$. As a result, another <strong>value-based algorithm</strong> is required to generate $q_t(s_t, a_t)$ (used Monte Carlo learning and temporal-difference (TD) learning).</p>
<ol>
<li>If $q_t(s_t, a_t)$ is estimated by Monte Carlo learning, the corresponding algorithm is called <em>REINFORCE</em> or <em>Monte Carlo policy gradient</em>.</li>
<li>If $q_t(s_t, a_t)$ is estimated by TD learning, the corresponding algorithms are usually called <em>actor-critic.</em> <u><strong>Therefore, actor-critic methods can be obtained by incorporating TD-based value estimation into policy gradient methods.</strong> </u></li>
</ol>
<p>The procedure of the simplest actor-critic algorithm is summarized in Algorithm 10.1.This actor-citric algorithm is sometimes called Q actor-critic (QAC). Although it is simple, QAC reveals the core idea of actor-critic methods.<br><img src="/2024/08/11/Mathmatical-Foundation-of-Reinforcement-Learning-REINFORCE-and-AC-algorithms/2024-06-07-22-12-16.png" alt=""></p>
<h3 id="10-2-Advantage-actor-critic-A2C"><a href="#10-2-Advantage-actor-critic-A2C" class="headerlink" title="10.2 Advantage actor-critic (A2C)"></a>10.2 Advantage actor-critic (A2C)</h3><p>The core idea of A2C is to introduce a baseline to reduce estimation variance.<br>A2C算法可以看作是传统PG算法的一种扩展，引入了value-based方法。当我们采用一个函数来近似动作值并利用TD算法（随机梯度下降）来拟合该函数时，该方法被称为<em>QAC算法</em>。</p>
<blockquote>
<p>在实际应用中，REINFORCE和QAC由于估计的方差较大，因此效果并不好。A2C算法通过在’动作值‘的估计中引入偏置量来降低<strong>SGD</strong>中对于梯度估计的方差，<strong>近似最优</strong>偏置量为状态值$v_{\pi}(s_t)$</p>
</blockquote>
<h4 id="10-2-1-Baseline-invariance"><a href="#10-2-1-Baseline-invariance" class="headerlink" title="10.2.1 Baseline invariance"></a>10.2.1 Baseline invariance</h4><p><u><strong>One interesting property of the policy gradient is that it is invariant to an additional baseline.</strong> </u></p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbb{E}_{S \sim \eta,A \sim \pi}\left[ \nabla_{\theta}\ln\pi(A|S,\theta_t)q_{\pi}(S,A)\right]=\mathbb{E}_{S \sim \eta,A \sim \pi}\left[ \nabla_{\theta}\ln\pi(A|S,\theta_t)(q_{\pi}(S,A)-b(S))\right]
\end{aligned}
\tag{10.3}</script><p>where the additional baseline $b(S)$ is a scalar function of $S$. Note:The bias of $b(S)$ should only be related to the environment, because if it is related to the interaction strategy $\pi$, $b(S)$ will change with the change of strategy.</p>
<blockquote>
<p>Equation (10.3) holds if and only if:</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbb{E}_{S \sim \eta,A \sim \pi}\left[ \nabla_{\theta}\ln\pi(A|S,\theta_t)b(S)\right]&=\sum_{s\in S}\eta(s)\sum_{a\in A}\pi(a|s,\theta_t)\nabla_{\theta}\ln\pi(a|s,\theta_t)b(s)\\
&=\sum_{s\in S}\eta(s)\sum_{a\in A}\nabla_{\theta}\pi(a|s,\theta_t)b(s)\\
&=\sum_{s\in S}\eta(s)b(s)\sum_{a\in A}\nabla_{\theta}\pi(a|s,\theta_t)\\
&=\sum_{s\in S}\eta(s)b(s)\nabla_{\theta}\sum_{a\in A}\pi(a|s,\theta_t)\\
&=\sum_{s\in S}\eta(s)b(s)\nabla_{\theta}1=0
\end{aligned}</script></blockquote>
<p><strong>why is the baseline useful?</strong><br><u>The baseline is useful because it can reduce the approximation variance when we use samples to approximate the true gradient.</u></p>
<script type="math/tex; mode=display">
\begin{aligned}
X(S,A)=\nabla_{\theta}\ln\pi(A|S,\theta_t)[q_{\pi}(S,A)-b(S))]
\end{aligned}
\tag{10.4}</script><p>We need to use a stochastic sample $x(s,a)$ to approximate $E[X(S,A)]$, According to equation 10.1, $E[X(S,A)]$ is invariant to the baseline, but the variance $var(X)$ is not. <strong>Our goal is to design a good baseline to minimize $var(X)$.</strong> In the algorithms of REINFORCE and QAC, we set $b=0$, which is not guaranteed to be a good baseline.<br>In fact, the optimal baseline that minimizes $var(X)$ is (proof by BOX10.1)</p>
<script type="math/tex; mode=display">
\begin{aligned}
b^{*}(s)=\frac{\mathbb{E}_{A \sim \pi}\left[ ||\nabla_{\theta}\ln\pi(A|s,\theta_t)||^2q_{\pi}(s, A)\right]}{\mathbb{E}_{A \sim \pi}\left[ ||\nabla_{\theta}\ln\pi(A|s,\theta_t)||^2\right]}, s \in S
\end{aligned}
\tag{10.5}</script><p>Although the baseline in(10.5) is optimal, it is too complex to be useful in practice. If the weight $||\nabla_{\theta}\ln\pi(A|s,\theta_t)||^2$ is removed from (10.5), we can obtain a suboptimal<br> baseline that has a concise expression:</p>
<script type="math/tex; mode=display">
\begin{aligned}
b^{**}(s)=\mathbb{E}_{A \sim \pi}\left[ q_{\pi}(s, A)\right]=v_{\pi}(s), s \in S
\end{aligned}
\tag{10.6}</script><p>Interestingly, this suboptimal baseline is the state value.</p>
<h4 id="10-2-2-Algorithm-description"><a href="#10-2-2-Algorithm-description" class="headerlink" title="10.2.2 Algorithm description"></a>10.2.2 Algorithm description</h4><p>When $b(s) = v_{\pi} (s)$, the gradient-ascent algorithm in (10.1) becomes</p>
<script type="math/tex; mode=display">
\begin{aligned}
\theta_{t+1} &=\theta_t +\alpha \mathbb{E}_{S \sim \eta,A \sim \pi}\left[ \nabla_{\theta}\ln\pi(A|S,\theta_t)\overbrace{[q_{\pi}(S,A)-v_{\pi}(S)]}^{Advantage  function}\right]\\
&=\theta_t +\alpha \mathbb{E}_{S \sim \eta,A \sim \pi}\left[ \nabla_{\theta}\ln\pi(A|S,\theta_t)\delta_{\pi}(S,A)\right]\\
&\approx \textcolor{green}{\theta_t +\alpha \nabla_{\theta}\ln\pi(a_t|s_t,\theta_t)[q_{t}(s_t,a_t)-v_t(s_t)]\text{  ( stochastic version)}}
\end{aligned}
\tag{10.7}</script><p>More specifically, note that $v_{\pi}(s) = \sum_{a \in A} \pi(a|s)q_{\pi}(s,a)$ is the mean of the action values. If $\delta_{\pi}(s,a) &gt; 0$, it means that the corresponding action has a greater value than the mean value.</p>
<p>Here, $q_t(s_t, a_t)$ and $v_t(s_t)$ are approximations of $q_{\pi(\theta_t)}(s_t,a_t)$ and $v_{\pi(\theta_t)}(s_t)$, respectively. The algorithm in (10.7) updates the policy based on <strong>the relative value</strong> of $q_t$ with respect to $v_t$ rather than the absolute value of $q_t$. This is intuitively reasonable because, when we attempt to select an action at a state, we only care about which action has the greatest value relative to the others.<br>If $q_t(s_t, a_t)$ and $v_t(s_t)$ are estimated by Monte Carlo learning, the algorithm in (10.7) is called <strong><em>REINFORCE with a baseline</em></strong>. If $q_t(s_t, a_t)$ and $v_t(s_t)$ are estimated by TD learning, the algorithm is usually called <strong><em>advantage actor-critic (A2C)</em></strong>. The implementation of A2C is summarized in Algorithm 10.2.<br>the advantage function in this implementation is approximated by the TD error:</p>
<script type="math/tex; mode=display">
\begin{aligned}
&q_{t}(s_t,a_t)-v_t(s_t)\approx r_{t+1} + \gamma v_t(s_{t+1})-v_t(s_t)\\
\text{Because : }  &q_{\pi}(s_t,a_t)-v_{\pi}(s_t)=\mathbb{E}\left[R_{t+1}+\gamma v_{\pi}(S_{t+1}-v_{\pi}(S_{t})| S_t = s_t, A_t = a_t) \right]
\end{aligned}</script><p>One merit of using the TD error is that we only need to use a single neural network to represent $v_{\pi} (s)$. Otherwise,  we need to maintain two networks to represent $q_{\pi}(s, a)$ and $v_{\pi}(s)$, respectively. The algorithm may also be called TD actor critic. In addition, it is notable that the policy $\pi(\theta_t)$ is stochastic and hence exploratory. Therefore, it can be directly used to generate experience samples without relying on techniques such as $\epsilon$-greedy. There are some variants of A2C such as asynchronous advantage actor-critic (A3C). Interested readers may check <sup><a href="#fn_2" id="reffn_2">2</a></sup><sup><a href="#fn_3" id="reffn_3">3</a></sup>.<br><img src="/2024/08/11/Mathmatical-Foundation-of-Reinforcement-Learning-REINFORCE-and-AC-algorithms/2024-06-07-23-24-30.png" alt=""></p>
<blockquote id="fn_2">
<sup>2</sup>. V. Mnih, A. P. Badia, M. Mirza, A. Graves, T. Lillicrap, T. Harley, D. Silver, and K. Kavukcuoglu, Asynchronous methods for deep reinforcement learning, in International Conference on Machine Learning, pp. 19281937, 2016.<a href="#reffn_2" title="Jump back to footnote [2] in the text."> ↩</a>
</blockquote>
<blockquote id="fn_3">
<sup>3</sup>. M. Babaeizadeh, I. Frosio, S. Tyree, J. Clemons, and J. Kautz, Reinforcement learning through asynchronous advantage actor-critic on a GPU, arXiv:1611.06256, 2016.<a href="#reffn_3" title="Jump back to footnote [3] in the text."> ↩</a>
</blockquote>
<h3 id="10-3-Off-policy-actor-critic"><a href="#10-3-Off-policy-actor-critic" class="headerlink" title="10.3 Off-policy actor-critic"></a>10.3 Off-policy actor-critic</h3><p>we can employ a technique called importance sampling. It is worth mentioning that the importance sampling technique is not restricted to the field of reinforcement learning. It is a general technique for estimating expected values defined over one probability distribution using some samples drawn from another distribution.</p>
<p>The advantage of off-policy compared to on-policy is that it can ensure that the converged policy in a stata will not diverge again, and the convergence is guaranteed. However, there will be a loss in exploration, which is inevitable.</p>
<p>_Question:off-policy actor-critic算法的优点是什么？为什么要提出off-policy算法？<strong>（Hint：强化学习要解决的本质问题是exploration-exploitation）</strong></p>
<blockquote>
<p>注：在实践中，由于Off-Policy Actor-Critic算法中actor和critic参数更新的步长中含有</p>
<script type="math/tex; mode=display">
\frac{\pi(a_t\|s_t,\theta_t)}{\beta(
a_t\|s_t)}</script><p>因此当采样策略与优化策略差距过大时（尤其当因此当采样策略与优化策略差距过大时（尤其当$\pi(\theta)与\beta$差异较大时，参数更新公式中的重要性因子会导致梯度爆炸），算法效果不佳（可以考虑PPO对优化目标添加正则项裁切梯度，或考虑DPG避免对动作值进行直接采样）</p>
</blockquote>
<h4 id="10-3-1-Importance-sampling"><a href="#10-3-1-Importance-sampling" class="headerlink" title="10.3.1 Importance sampling"></a>10.3.1 Importance sampling</h4><script type="math/tex; mode=display">
\mathbb{E}_{x \sim p_0}[X]=\sum_{x \in X}p_0(x)x=\sum_{x \in X}p_1(x)\underbrace{\frac{p_0(x)}{p_1(x)} x}_{f(x)}=\mathbb{E}_{x \sim p_1}[f(X)]</script><script type="math/tex; mode=display">
\mathbb{E}_{x \sim p_0}[X]=\mathbb{E}_{x \sim p_1}[f(X)]\approx \bar{f}=\frac{1}{n}\sum_{i=1}^n f(x_i)p_0(x)x=\frac{1}{n}\sum_{i=1}^n \underbrace{\frac{p_0(x)}{p_1(x)}}_{\text{importance weight}}x_i</script><p>$\frac{p_0(x)}{p_1(x)}$ is called the importance weight(it need be set). When $p_1 = p_0$, the importance weight is 1 and $\bar{f}$ becomes $\bar{x}$. When $p_0(x_i) ≥ p_1(x_i)$, $x_i$ can be sampled more frequently by $p_0$ but less frequently by $p_1$. In this case, the importance weight, which is greater than one, emphasizes the importance of this sample. $p_0$ is target policy, $p_1$ is behaviors policy</p>
<p><strong>The importance weight can be understood as follows: the samples corresponding to $p_0$ being large but $p_1$being small are more important, while the samples corresponding to $p_0$ being small but $p_1$ being large become less important.</strong></p>
<h4 id="10-3-2-The-off-policy-policy-gradient-theorem"><a href="#10-3-2-The-off-policy-policy-gradient-theorem" class="headerlink" title="10.3.2 The off-policy policy gradient theorem"></a>10.3.2 The off-policy policy gradient theorem</h4><p>With the importance sampling technique, we are ready to present the off-policy policy gradient theorem. Suppose that $\beta$ is a behavior policy. Our goal is to use the samples generated by $\beta$ to learn a target policy $\pi$ that can maximize the following metric:</p>
<script type="math/tex; mode=display">
J(\theta) = \sum_{s \in S}d_{\beta}(s)v_{\pi}(s)=\mathbb{E}_{S \sim d_{\beta}}\left[ v_{\pi}(S)\right]</script><script type="math/tex; mode=display">
\nabla_{\theta}J(\theta) = \mathbb{E}_{S \sim \rho,A \sim \beta}\left[\underbrace{\frac{\pi(A | S,\theta)}{\beta(A | S)}}_{\text{importance weight}} \nabla_{\theta}\ln\pi(A|S,\theta_t)q_{\pi}(S,A)\right]</script><p><img src="/2024/08/11/Mathmatical-Foundation-of-Reinforcement-Learning-REINFORCE-and-AC-algorithms/2024-08-29-10-30-15.png" alt=""><br>The gradient  is similar to that in the on-policy case in Theorem 9.1, but there are two differences. The first difference is the importance weight. The second difference is that $A ∼ β$ instead of $A ∼ π$.</p>
<h4 id="10-3-3-Algorithm-description"><a href="#10-3-3-Algorithm-description" class="headerlink" title="10.3.3 Algorithm description"></a>10.3.3 Algorithm description</h4><script type="math/tex; mode=display">
\nabla_{\theta}J(\theta) = \mathbb{E}_{S \sim \rho,A \sim \beta}\left[\frac{\pi(A | S,\theta)}{\beta(A | S)} \nabla_{\theta}\ln\pi(A|S,\theta_t)(q_{\pi}(S,A)-b(S))\right]</script><p>Because $\mathbb{E}\left[\frac{\pi(A | S,\theta)}{\beta(A | S)} \nabla_{\theta}\ln\pi(A|S,\theta_t)b(S)\right] = 0$ To reduce the estimation variance, we can select the baseline as $b(S) = v_{\pi}(S)$ and obtain</p>
<script type="math/tex; mode=display">
\begin{aligned}
\theta_{t+1} &=\theta_t +\alpha \mathbb{E}_{S \sim \eta,A \sim \pi}\left[\frac{\pi(A | S,\theta)}{\beta(A | S)}  \nabla_{\theta}\ln\pi(A|S,\theta_t)\overbrace{[q_{\pi}(S,A)-v_{\pi}(S)]}^{Advantage  function}\right]\\
&=\theta_t +\alpha \mathbb{E}_{S \sim \eta,A \sim \pi}\left[ \frac{\pi(A | S,\theta)}{\beta(A | S)} \nabla_{\theta}\ln\pi(A|S,\theta_t)\delta_{\pi}(S,A)\right]\\
&\approx \textcolor{green}{\theta_t +\alpha_{\theta} \frac{\pi(a_t|s_t,\theta_{t})}{\beta(a_t|s_t)} \nabla_{\theta}\ln\pi(a_t|s_t,\theta_t)[q_{t}(s_t,a_t)-v_t(s_t)]\text{ ( stochastic version)}}
\\
&\approx \textcolor{green}{\theta_t +\alpha_{\theta} \frac{\pi(a_t|s_t,\theta_{t})}{\beta(a_t|s_t)} \nabla_{\theta}\ln\pi(a_t|s_t,\theta_t)[r_{t+1}+\gamma v_t(s_{t+1})-v_t(s_t)]\text{ ( replaced by the TD error)}}
\end{aligned}
\tag{10.8}</script><p>Importance weight is included in both the critic and the actor.<br>It must be noted that, in addition to the actor, the critic is also converted from on-policy to off-policy by the importance sampling technique. In fact, importance sampling is a general technique that can be applied to both policy-based and value-based algorithms. Finally, Algorithm 10.3 can be extended in various ways to incorporate more techniques such as eligibility traces <sup><a href="#fn_4" id="reffn_4">4</a></sup>.<br><img src="/2024/08/11/Mathmatical-Foundation-of-Reinforcement-Learning-REINFORCE-and-AC-algorithms/2024-08-29-12-21-34.png" alt=""><br><sup><a href="#fn_4" id="reffn_4">4</a></sup>:T. Degris, M. White, and R. S. Sutton, “Off-policy actor-critic,” arXiv:1205.4839,2012.</p>
<h3 id="10-4-Deterministic-actor-critic"><a href="#10-4-Deterministic-actor-critic" class="headerlink" title="10.4 Deterministic actor-critic"></a>10.4 Deterministic actor-critic</h3><p>Up to now, the policies used in the policy gradient methods are all stochastic since it is required that $\pi(a|s, \theta) &gt; 0$ for every $(s, a)$. This section shows that deterministic policies can also be used in policy gradient methods. Here, “deterministic” indicates that, for any state, a single action is given a probability of one and all the other actions are given probabilities of zero. <strong>It is important to study the deterministic case since it is naturally off-policy and can effectively handle continuous action spaces and large scale action spaces.</strong></p>
<p>We have been using $\pi(a|s, \theta)$ to denote a general policy, which can be either stochastic or deterministic. In this section, we use <script type="math/tex">a = \mu(s, \theta)</script> to specifically denote a deterministic policy. Different from $\pi$ which gives the probability of an action, $\mu$ directly gives the action since it is a mapping from $\mathbf{S}$ to $\mathbf{A}$. This deterministic policy can be represented by, for example, a neural network with $s$ as its input, $a$ as its output, and $\theta$ as its parameter. For the sake of simplicity, we often write $\mu(s, \theta)$ as $\mu(s)$ for short.</p>
<h4 id="10-4-1-The-deterministic-policy-gradient-theorem"><a href="#10-4-1-The-deterministic-policy-gradient-theorem" class="headerlink" title="10.4.1 The deterministic policy gradient theorem"></a>10.4.1 The deterministic policy gradient theorem</h4><p>The policy gradient theorem introduced in the last chapter is only valid for stochastic policies. When we require the policy to be deterministic, a new policy gradient theorem must be derived.</p>
<p><strong>Theorem 10.2</strong> (Deterministic policy gradient theorem). The gradient of $J(\theta)$ is</p>
<script type="math/tex; mode=display">
\begin{aligned}
\nabla_{\theta}J(\theta) &=\sum_{s \in S}\eta(s)\nabla_{\theta}\mu(s)(\nabla_{a}q_{\mu}(s,a))|_{a=\mu(s)}\\
&=\mathbb{E}_{S \sim \eta}\left[ \nabla_{\theta}\mu(S)(\nabla_{a}q_{\mu}(S,a))|_{a=\mu(S)}\right]
\end{aligned}
\tag{10.9}</script><p>where $\eta$ is a distribution of the states.<br>Unlike the stochastic case, the gradient in the deterministic case shown in (10.9) does not involve the action random variable $A$ (because $a=\mu(S)$). As a result, when we use samples to approximate the true gradient, it is not required to sample actions. Therefore, the deterministic policy gradient method is <em>off-policy</em>.</p>
<p><strong>Theorem 10.3</strong> (Deterministic policy gradient theorem in the discounted case). in the discounted case where $γ ∈ (0, 1)$, the gradient of $J(θ)$  is </p>
<script type="math/tex; mode=display">
\begin{aligned}
\nabla_{\theta}J(\theta) &=\nabla_{\theta}\sum_{s \in S}d_0(s)v_{\mu}(s)\\
&=\sum_{s \in S}\rho_{\mu}(s)\nabla_{\theta}\mu(s)(\nabla_{a}q_{\mu}(s,a))|_{a=\mu(s)}\\
&=\mathbb{E}_{S \sim \rho_{\mu}}\left[ \nabla_{\theta}\mu(S)(\nabla_{a}q_{\mu}(S,a))|_{a=\mu(S)}\right]
\end{aligned}
\tag{10.10}</script><p><img src="/2024/08/11/Mathmatical-Foundation-of-Reinforcement-Learning-REINFORCE-and-AC-algorithms/2024-08-29-22-04-15.png" alt=""></p>
<p><strong>Theorem 10.4</strong> (Deterministic policy gradient theorem in the undiscounted case). In the undiscounted case, the gradient of $J(θ)$ is</p>
<script type="math/tex; mode=display">
\begin{aligned}
\nabla_{\theta}J(\theta) &=\nabla_{\theta}\sum_{s \in S}d_{\mu}(s)v_{\mu}(s)\\
&=\sum_{s \in S}d_{\mu}(s)\nabla_{\theta}\mu(s)(\nabla_{a}q_{\mu}(s,a))|_{a=\mu(s)}\\
&=\mathbb{E}_{S \sim d_{\mu}}\left[ \nabla_{\theta}\mu(S)(\nabla_{a}q_{\mu}(S,a))|_{a=\mu(S)}\right]\text{(stochastic gradient-ascent)}
\end{aligned}
\tag{10.11}</script><p>where $d_{\mu}$ is the stationary distribution of the states under policy $\mu$.</p>
<h4 id="10-4-2-Algorithm-description"><a href="#10-4-2-Algorithm-description" class="headerlink" title="10.4.2 Algorithm description"></a>10.4.2 Algorithm description</h4><script type="math/tex; mode=display">
\begin{aligned}
\theta_{t+1} &=\theta_{t}+ \alpha_{\theta}\mathbb{E}_{S \sim \eta}[\nabla_{\theta}\mu(S)(\nabla_{a}q_{\mu}(S,a))|_{a=\mu(S)}]\\
&\approx\theta_{t}+ \alpha_{\theta}\nabla_{\theta}\mu(s_t)(\nabla_{a}q_{\mu}(s_t,a))|_{a=\mu(s_t)}\\
&=\theta_{t}+ \alpha_{\theta}\nabla_{\theta}q_{\mu}(s_t,\mu(s_t))
\end{aligned}
\tag{10.12}</script><p>It should be noted that this algorithm is off-policy since the behavior policy $β$ may be different from $µ$. First, the actor is off-policy. We already explained the reason when presenting Theorem 10.2.<br>Second, the critic is also off-policy. Special attention must be paid to why the critic is off-policy but does not require the importance sampling technique. In particular, the experience sample required by the critic is $(s_t, a_t, r_{t+1}, s_{t+1}, \textcolor{red}{\tilde{a}_{t+1}})$, where $\tilde{a}_{t+1} = µ(s_{t+1})$. The generation of this experience sample involves two policies. The first is the policy for generating $a_t$ at $s_t$, and the second is the policy for generating $\tilde{a}_{t+1}$ at $s_{t+1}$. The first policy that generates $a_t$ is the behavior policy since $a_t$ is used to interact with the environment. The second policy must be $µ$ because it is the policy that the critic aims to evaluate. Hence, $µ$ is the target policy. It should be noted that $\tilde{a}_{t+1}$ is not used to interact with the environment in the next time step. Hence, $µ$ is not the behavior policy. Therefore, the critic is off-policy.</p>
<p>How to select the function $q(s, a, w)$? The original research work <sup><a href="#fn_5" id="reffn_5">5</a></sup> that proposed the deterministic policy gradient method adopted linear functions: $q(s, a, w) = φT(s, a)w$ where $φ(s, a)$ is the feature vector. It is currently popular to represent $q(s, a, w)$ using neural networks, as suggested in the deep deterministic policy gradient (DDPG) method<sup><a href="#fn_6" id="reffn_6">6</a></sup>.<br>How to select the behavior policy $β$? It can be any exploratory policy. It can also be a stochastic policy obtained by adding noise to $µ$ <sup><a href="#fn_6" id="reffn_6">6</a></sup>. In this case, $µ$ is also the behavior policy and hence this way is an on-policy implementation.</p>
<blockquote id="fn_5">
<sup>5</sup>. D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller, “Deterministic policy gradient algorithms,” in International Conference on Machine Learning, pp. 387–395, 2014.<a href="#reffn_5" title="Jump back to footnote [5] in the text."> ↩</a>
</blockquote>
<blockquote id="fn_6">
<sup>6</sup>. T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa, D. Silver, and D. Wierstra, “Continuous control with deep reinforcement learning,” arXiv:1509.02971, 2015.<a href="#reffn_6" title="Jump back to footnote [6] in the text."> ↩</a>
</blockquote>
<p><img src="/2024/08/11/Mathmatical-Foundation-of-Reinforcement-Learning-REINFORCE-and-AC-algorithms/2024-08-29-22-24-05.png" alt=""></p>
<p>DPG算法优点：</p>
<ul>
<li>能够处理具有无限元素的动作集合，提高算法的泛化性与训练效率</li>
<li><p>由于无需直接对动作进行采样，因此是off-policy算法，同时能够避免重要性因子过大导致的梯度爆炸（不恰当的采样策略会导致其与目标策略之间的分布间差异过大）</p>
<ul>
<li><p>原因：DPG算法利用梯度上升优化的目标函数表达式为：</p>
<script type="math/tex; mode=display">
E(\nabla_{\theta}\mu(S)(\nabla_a q_S,a=\mu(S)))</script><p>可以观察到<strong>真实梯度中并不包含对动作值的采样</strong>（取而代之的是由Actor Net直接输出的动作值估计），因此是off-policy算法</p>
</li>
</ul>
</li>
</ul>
<p><em>Question：DPG在数学意义上解决什么问题？与前述stochastic policy gradient算法的critic求解的数学问题有什么不同？（Hint：观察TD Error）</em></p>
<blockquote>
<p>注：考虑到DPG中Actor的输出为动作本身，因此选择gym提供的具有连续动作空间的Pendulum环境进行测试</p>
<p>思考：</p>
<ol>
<li>On-policy和Off-policy分别适用于解决什么类型的任务？为什么？<br><em>答：On-policy适合解决给定起始状态和终点状态之间最优‘路径’一类问题，Off-policy适合求解任意状态的最优策略。这是由于On-policy算法的behavior policy和target policy是同一个，因此无法保证充分地探索每一个‘状态-动作’对；而Off-policy算法的前述两类policy并非同一个，因此可以选择随机性较强的behavior policy（例如均匀随机策略）来充分探索。</em></li>
<li>Online和Offline算法分别适用于解决什么类型的任务？为什么？<em>答：Online算法适合样本量较少情况下的学习任务，诸多算法都利用了Generalized Policy Iteration的思想，效率较高；Offline算法适合已经拥有大量样本的情况，因为利用模拟仿真的方法（例如，Monte-Carlo）采样效率较低。</em></li>
</ol>
</blockquote>
<h3 id="10-5-Summary"><a href="#10-5-Summary" class="headerlink" title="10.5 Summary"></a>10.5 Summary</h3><p>In this chapter, we introduced actor-critic methods. The contents are summarized as follows.</p>
<p>Section 10.1 introduced the simplest actor-critic algorithm called QAC. This algorithm is similar to the policy gradient algorithm, REINFORCE, introduced in the last chapter. The only difference is that the q-value estimation in QAC relies on TD learning while REINFORCE relies on Monte Carlo estimation.</p>
<p>Section 10.2 extended QAC to advantage actor-critic. It was shown that the policy gradient is invariant to any additional baseline. It was then shown that an optimal baseline could help reduce the estimation variance.</p>
<p>Section 10.3 further extended the advantage actor-critic algorithm to the off-policy case. To do that, we introduced an important technique called importance sampling.</p>
<p>Finally, while all the previously presented policy gradient algorithms rely on stochastic policies, we showed in Section 10.4 that the policy can be forced to be deterministic. The corresponding gradient was derived, and the deterministic policy gradient algorithm was introduced.</p>
<p>Policy gradient and actor-critic methods are widely used in modern reinforcement learning. There exist a large number of advanced algorithms in the literature such as SAC <sup><a href="#fn_7" id="reffn_7">7</a></sup> <sup><a href="#fn_8" id="reffn_8">8</a></sup>, TRPO <sup><a href="#fn_9" id="reffn_9">9</a></sup>, PPO <sup><a href="#fn_10" id="reffn_10">10</a></sup>, and TD3 <sup><a href="#fn_11" id="reffn_11">11</a></sup>. In addition, the single-agent case can also be extended to the case of multi-agent reinforcement learning <sup><a href="#fn_12" id="reffn_12">12</a></sup> <sup><a href="#fn_13" id="reffn_13">13</a></sup> <sup><a href="#fn_14" id="reffn_14">14</a></sup> <sup><a href="#fn_15" id="reffn_15">15</a></sup> <sup><a href="#fn_16" id="reffn_16">16</a></sup>. Experience samples can also be used to fit system models to achieve model-based reinforcement learning <sup><a href="#fn_17" id="reffn_17">17</a></sup> <sup><a href="#fn_18" id="reffn_18">18</a></sup> <sup><a href="#fn_19" id="reffn_19">19</a></sup>. Distributional reinforcement learning provides a fundamentally different perspective from the conventional one <sup><a href="#fn_20" id="reffn_20">20</a></sup> <sup><a href="#fn_21" id="reffn_21">21</a></sup>. The relationships between reinforcement learning and control theory have been discussed in <sup><a href="#fn_22" id="reffn_22">22</a></sup> <sup><a href="#fn_23" id="reffn_23">23</a></sup> <sup><a href="#fn_24" id="reffn_24">24</a></sup> <sup><a href="#fn_25" id="reffn_25">25</a></sup> <sup><a href="#fn_26" id="reffn_26">26</a></sup> <sup><a href="#fn_27" id="reffn_27">27</a></sup>. This book is not able to cover all these topics. Hopefully, the foundations laid by this book can help readers better study them in the future.</p>
<blockquote id="fn_7">
<sup>7</sup>. T. Haarnoja, A. Zhou, P. Abbeel, and S. Levine, “Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor,” in International Conference on Machine Learning, pp. 1861–1870, 2018.<a href="#reffn_7" title="Jump back to footnote [7] in the text."> ↩</a>
</blockquote>
<blockquote id="fn_8">
<sup>8</sup>. T. Haarnoja, A. Zhou, K. Hartikainen, G. Tucker, S. Ha, J. Tan, V. Kumar, H. Zhu, A. Gupta, and P. Abbeel, “Soft actor-critic algorithms and applications,” arXiv:1812.05905, 2018.<a href="#reffn_8" title="Jump back to footnote [8] in the text."> ↩</a>
</blockquote>
<blockquote id="fn_9">
<sup>9</sup>. J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz, “Trust region policy optimization,” in International Conference on Machine Learning, pp. 1889–1897, 2015.<a href="#reffn_9" title="Jump back to footnote [9] in the text."> ↩</a>
</blockquote>
<blockquote id="fn_10">
<sup>10</sup>. J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, “Proximal policy optimization algorithms,” arXiv:1707.06347, 2017.<a href="#reffn_10" title="Jump back to footnote [10] in the text."> ↩</a>
</blockquote>
<blockquote id="fn_11">
<sup>11</sup>. S. Fujimoto, H. Hoof, and D. Meger, “Addressing function approximation error in actor-critic methods,” in International Conference on Machine Learning, pp. 1587–1596, 2018.<a href="#reffn_11" title="Jump back to footnote [11] in the text."> ↩</a>
</blockquote>
<blockquote id="fn_12">
<sup>12</sup>. J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson, “Counterfactual multi-agent policy gradients,” in AAAI Conference on Artificial Intelligence,vol. 32, 2018<a href="#reffn_12" title="Jump back to footnote [12] in the text."> ↩</a>
</blockquote>
<blockquote id="fn_13">
<sup>13</sup>. R. Lowe, Y. I. Wu, A. Tamar, J. Harb, O. Pieter Abbeel, and I. Mordatch, “Multiagent actor-critic for mixed cooperative-competitive environments,” Advances in Neural Information Processing Systems, vol. 30, 2017.<a href="#reffn_13" title="Jump back to footnote [13] in the text."> ↩</a>
</blockquote>
<blockquote id="fn_14">
<sup>14</sup>. Y. Yang, R. Luo, M. Li, M. Zhou, W. Zhang, and J. Wang, “Mean field multiagent reinforcement learning,” in International Conference on Machine Learning, pp. 5571–5580, 2018.<a href="#reffn_14" title="Jump back to footnote [14] in the text."> ↩</a>
</blockquote>
<blockquote id="fn_15">
<sup>15</sup>. O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev, et al., “Grandmaster level in StarCraft II using multi-agent reinforcement learning,” Nature, vol. 575, no. 7782, pp. 350–354, 2019.<a href="#reffn_15" title="Jump back to footnote [15] in the text."> ↩</a>
</blockquote>
<blockquote id="fn_16">
<sup>16</sup>. Y. Yang and J. Wang, “An overview of multi-agent reinforcement learning from game theoretical perspective,” arXiv:2011.00583, 2020.<a href="#reffn_16" title="Jump back to footnote [16] in the text."> ↩</a>
</blockquote>
<blockquote id="fn_17">
<sup>17</sup>. F.-M. Luo, T. Xu, H. Lai, X.-H. Chen, W. Zhang, and Y. Yu, “A survey on modelbased reinforcement learning,” arXiv:2206.09328, 2022.<a href="#reffn_17" title="Jump back to footnote [17] in the text."> ↩</a>
</blockquote>
<blockquote id="fn_18">
<sup>18</sup>. S. Levine and V. Koltun, “Guided policy search,” in International Conference on Machine Learning, pp. 1–9, 2013.<a href="#reffn_18" title="Jump back to footnote [18] in the text."> ↩</a>
</blockquote>
<blockquote id="fn_19">
<sup>19</sup>. M. Janner, J. Fu, M. Zhang, and S. Levine, “When to trust your model: Modelbased policy optimization,” Advances in Neural Information Processing Systems, vol. 32, 2019<a href="#reffn_19" title="Jump back to footnote [19] in the text."> ↩</a>
</blockquote>
<blockquote id="fn_20">
<sup>20</sup>. M. G. Bellemare, W. Dabney, and R. Munos, “A distributional perspective on reinforcement learning,” in International Conference on Machine Learning, pp. 449–458, 2017.<a href="#reffn_20" title="Jump back to footnote [20] in the text."> ↩</a>
</blockquote>
<blockquote id="fn_21">
<sup>21</sup>. M. G. Bellemare, W. Dabney, and M. Rowland, Distributional Reinforcement Learning. MIT Press, 2023<a href="#reffn_21" title="Jump back to footnote [21] in the text."> ↩</a>
</blockquote>
<blockquote id="fn_22">
<sup>22</sup>. H. Zhang, D. Liu, Y. Luo, and D. Wang, Adaptive dynamic programming for control:algorithms and stability. Springer Science &amp; Business Media, 2012.<a href="#reffn_22" title="Jump back to footnote [22] in the text."> ↩</a>
</blockquote>
<blockquote id="fn_23">
<sup>23</sup>. F. L. Lewis, D. Vrabie, and K. G. Vamvoudakis, “Reinforcement learning and feedback control: Using natural decision methods to design optimal adaptive controllers,” IEEE Control Systems Magazine, vol. 32, no. 6, pp. 76–105, 2012.<a href="#reffn_23" title="Jump back to footnote [23] in the text."> ↩</a>
</blockquote>
<blockquote id="fn_24">
<sup>24</sup>. F. L. Lewis and D. Liu, Reinforcement learning and approximate dynamic programming for feedback control. John Wiley&amp;Sons, 2013.<a href="#reffn_24" title="Jump back to footnote [24] in the text."> ↩</a>
</blockquote>
<blockquote id="fn_25">
<sup>25</sup>. Z.-P. Jiang, T. Bian, and W. Gao, “Learning-based control: A tutorial and some recent results,” Foundations and Trends in Systems and Control, vol. 8, no. 3, pp. 176–284, 2020.<a href="#reffn_25" title="Jump back to footnote [25] in the text."> ↩</a>
</blockquote>
<blockquote id="fn_26">
<sup>26</sup>. S. Meyn, Control systems and reinforcement learning. Cambridge University Press, 2022.<a href="#reffn_26" title="Jump back to footnote [26] in the text."> ↩</a>
</blockquote>
<blockquote id="fn_27">
<sup>27</sup>. S. E. Li, Reinforcement learning for sequential decision and optimal control. Springer, 2023.<a href="#reffn_27" title="Jump back to footnote [27] in the text."> ↩</a>
</blockquote>
]]></content>
  </entry>
  <entry>
    <title>Mathematical Foundation of Reinforcement Learning-TD algorithms</title>
    <url>/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/</url>
    <content><![CDATA[<p>[toc] </p>
<p>Sources:</p>
<ol>
<li><a href="https://github.com/MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning"><em>Shiyu Zhao</em>. 《Mathematical Foundation of Reinforcement Learning》Chapter 6-7</a>.</li>
<li><a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#">OpenAI Spinning Up</a></li>
</ol>
<h2 id="6-Stochastic-Approximation"><a href="#6-Stochastic-Approximation" class="headerlink" title="6 Stochastic Approximation"></a>6 Stochastic Approximation</h2><p>We use the present chapter to fill this knowledge gap by introducing the basics of stochastic approximation. Although this chapter does not introduce any specific reinforcement learning algorithms, it lays the necessary foundations for studying subsequent chapters. We will see in Chapter 7 that <strong>the temporal-difference algorithms can be viewed as special stochastic approximation algorithms.</strong> The well-known stochastic gradient descent <strong>(SGD)</strong> algorithms widely used in machine learning are also introduced in the present chapter.</p>
<h3 id="6-1-Mean-estimation"><a href="#6-1-Mean-estimation" class="headerlink" title="6.1 Mean estimation"></a>6.1 Mean estimation</h3><p>The basic idea of <strong>Monte Carlo estimation</strong> is:</p>
<script type="math/tex; mode=display">
\mathbb{E}[X] \approx \bar{x}=\frac{1}{n} \sum_{j=1}^n x_j .</script><p>as $n$ increases, the approximation becomes increasingly accurate. When $n \rightarrow \infty$, we have $\bar{x} \rightarrow \mathbb{E}[X]$.</p>
<p>We next show that two methods can be used to calculate $\bar{x}$ . The first <strong>non-incremental method</strong> collects all the samples first and then calculates the average. The drawback of such a method is that, if the number of samples is large, we may have to wait for a long time until all of the samples are collected. </p>
<p>The second method can avoid this drawback because it calculates the average in an <strong>incremental manner</strong> Specifically, suppose that:</p>
<script type="math/tex; mode=display">
w_{k+1} =\frac{1}{k} \sum_{i=1}^k x_i ,  k=1,2,\dots .</script><script type="math/tex; mode=display">
w_{k} =\frac{1}{k-1} \sum_{i=1}^{k-1} x_i ,  k=2,3,\dots .</script><p>Therefore, we obtain the following <strong>incremental algorithm</strong>:</p>
<script type="math/tex; mode=display">
\begin{equation}
  w_{k+1} =w_{k}-\frac{1}{k}(w_k-x_k) .
\end{equation}</script><p>The advantage of the incremental algorithm is that the average can be immediately calculated every time we receive a sample. This average can be used to approximate $\bar{x}$ and hence $\mathbb{E}[X]$. Notably, the approximation may not be accurate at the beginning due to insufficient samples. However, it is better than nothing.</p>
<p>Furthermore, consider an algorithm with a more general expression:</p>
<script type="math/tex; mode=display">
\begin{aligned}
  \textcolor{red}{w_{k+1} =w_{k}-a_k(w_k-x_k)} .
\end{aligned}</script><p>It is the same as the incremental algorithm except that the coefficient $1/k$ is replaced by $a_k &gt; 0$ However, we will show in the next section that, if $k$ satisfies some mild conditions, $w_k \rightarrow \mathbb{E}[X]$ as $k \rightarrow \infty$. In Chapter 7, we will see that temporal-difference algorithms have similar (but more complex) expressions.</p>
<h3 id="6-2-Robbins-Monro-RM-algorithm"><a href="#6-2-Robbins-Monro-RM-algorithm" class="headerlink" title="6.2 Robbins-Monro (RM) algorithm"></a>6.2 Robbins-Monro (RM) algorithm</h3><p><strong>Robbins-Monro (RM) algorithm</strong>: solve $g(w)=0$ using $\left\{\tilde{g}\left(w_k, \eta_k\right)\right\}$ </p>
<script type="math/tex; mode=display">
 w_{k+1}=w_k-a_k \tilde{g}\left(w_k, \eta_k\right) ,k=1,2,3,\dots</script><p><strong>Stochastic approximation</strong> refers to a broad class of stochastic iterative algorithms solving root finding or optimization problems. Compared to many other root-finding algorithms such as gradient-based methods, Stochastic approximation is powerful in the sense that it does not require the expression of the objective function nor its derivative.</p>
<p><strong>Robbins-Monro (RM) algorithm</strong> is a pioneering work in the field of stochastic approximation. The famous stochastic gradient descent (SGD) algorithm is a special form of the RM algorithm. </p>
<h4 id="6-2-1-Problem-statement"><a href="#6-2-1-Problem-statement" class="headerlink" title="6.2.1 Problem statement"></a>6.2.1 Problem statement</h4><p>Problem statement: Suppose we would like to find the root of the equation</p>
<script type="math/tex; mode=display">
 g(w)=0</script><p>where $w \in \mathbb{R}$ is the variable to be solved and $g: \mathbb{R} \rightarrow \mathbb{R}$ is a function. Many problems can be formulated as root- finding problems. For example, if $J(w)$ is an objective function to be optimized, this optimization problem can be converted to solving $g(w) = \nabla_w J(w) = 0$.</p>
<p>If the expression of $g$ or its derivative is known, there are many numerical algorithms that can solve this problem.However, <strong>the problem we are facing is that the expression of the function g is unknown.</strong> For example, the function may be represented by an artificial neural network whose structure and parameters are unknown. Moreover, we can only obtain a noisy observation of $g(w)$:</p>
<script type="math/tex; mode=display">
\begin{equation}
 \textcolor{purple} {\tilde{g}(w, \eta)=g(w)+\eta}
\end{equation}</script><p>where $\eta \in \mathbb{R}$ is the <em>observation error</em>, which may or may not be Gaussian. In summary, it is a black-box system where only the input $w$ and the noisy output $\tilde{g}(w, \eta)$ are known. Our aim is to solve $g(w)=0$ using $w$ and $\tilde{g}$.<br><img src="/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-04-24-21-31-33.png" alt="Figure 6.2: An illustration of the problem of solving g(w) = 0 from w and g."></p>
<p>The Robbins-Monro (RM) algorithm can solve $g(w)=0$ is </p>
<script type="math/tex; mode=display">
\begin{equation}
 \textcolor{red} {w_{k+1}=w_k-a_k \tilde{g}\left(w_k, \eta_k\right) ,k=1,2,3,\dots}
\end{equation}</script><p>where $w_k$ is the $k$ th estimate of the root, $\tilde{g}\left(w_k, \eta_k\right)=g\left(w_k\right)+\eta_k$ is the $k$ th noisy observation $a_k$ is a positive coefficient. As can been seen, RM algorithm only requires the input ($\left\{w_k\right\}$) and output data ($\left\{\tilde{g}\left(w_k, \eta_k\right)\right\}$). It does not need to know the expression of $g(w)$. Next we introduce an example to illustrate RM algorithm.</p>
<h4 id="6-2-2-An-illustrative-example"><a href="#6-2-2-An-illustrative-example" class="headerlink" title="6.2.2 An illustrative example"></a>6.2.2 An illustrative example</h4><p><img src="/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-04-24-21-40-10.png" alt=""><br>Consider the example shown in Figure. In this example, $g(w)=\tanh (w-1)$, the true root of $g(w)=0$ is $w^\ast=1$. Parameters: $w_1=3, a_k=1 / k, \eta_k \equiv 0$ (no noise for the sake of simplicity) The RM algorithm in this case is $\textcolor{blue} {w_{k+1}=w_k-a_k g\left(w_k\right)} $ since $\tilde{g}\left(w_k, \eta_k\right)=g\left(w_k\right)$ when $\eta_k=0$. The resulting $\left\{w_k\right\}$ generated by the RM algorithm is shown in Figure . It can be seen that $w_k$ converges to the true root $w^\ast=1$.</p>
<p>This simple example can illustrate why the RM algorithm converges.</p>
<ul>
<li>When $w_k&gt;w^\ast$, we have $g\left(w_k\right)&gt;0$. Then, $w_{k+1}=w_k-a_k g\left(w_k\right)&lt;w_k$. If $a_k g\left(w_k\right)$ is sufficiently small, we have $w^\ast&lt;w_{k+1}&lt;w_k$. As a result, $w_{k+1}$ is closer to $w^\ast$ than $w_k$.</li>
<li>When $w_k<w^\ast$, we="" have="" $g\left(w_k\right)<0$.="" then,="" $w_{k+1}="w_k-a_k" g\left(w_k\right)="">w_k$. If $\left|a_k g\left(w_k\right)\right|$ is sufficiently small, we have $w^\ast&gt;w_{k+1}&gt;w_k$. As a result, $w_{k+1}$ is closer to $w^\ast$ than $w_k$.</w^\ast$,></li>
</ul>
<p>In either case,$w_{k+1}$ is closer to $w^\ast$ than $w_k$. Therefore, it is intuitive that $w_k$ converges to $w^\ast$.</p>
<h4 id="6-2-3-RM-algorithm’s-proof"><a href="#6-2-3-RM-algorithm’s-proof" class="headerlink" title="6.2.3 RM algorithm’s proof"></a>6.2.3 RM algorithm’s proof</h4><p>Robbins-Monro theorem: In the Robbins-Monro algorithm $\textcolor{red} {w_{k+1}=w_k-a_k \tilde{g}\left(w_k, \eta_k\right) ,k=1,2,3,\dots}$, if：<br><em>(a)</em> $0&lt;c_1 \leq \nabla_w g(w) \leq c_2$ for all $w$;<br><em>(b)</em> $\sum_{k=1}^{\infty} a_k=\infty$ and $\sum_{k=1}^{\infty} a_k^2&lt;\infty$;<br><em>(c)</em> $\mathbb{E}\left[\eta_k \mid \mathcal{H}_k\right]=0$ and $\mathbb{E}\left[\eta_k^2 \mid \mathcal{H}_k\right]&lt;\infty$;</p>
<p>where $\mathcal{H}_k=\left\{w_k, w_{k-1}, \ldots\right\}$, then $w_k$ almost surely converges to the root $w^\ast$ satisfying $g\left(w^\ast\right)=0$.</p>
<p>The three conditions in previous are explained as follows.</p>
<p><strong><em>Condition (a)</em></strong><br>$0&lt;c_1 \leq \nabla_w g(w)$ indicates that $g(w)$ is a monotonically increasing function. This condition ensures that the root of $g(w)=0$ exists and is unique. If $g(w)$ is monotonically decreasing, we can simply treat $-g(w)$ as a new function that is monotonically increasing.</p>
<p>As an application, we can formulate an optimization problem in which the objective function is $J(w)$ as a root- finding problem: $g(w) = \nabla_wJ(w) = 0$. In this case, the condition that $g(w)$ is monotonically increasing <strong>indicates that $J(w)$ is convex, which is a commonly adopted assumption in optimization problems.</strong></p>
<p>The inequality $\nabla_w g(w) \leq c_2$ indicates that the gradient of $g(w)$ is bounded from above. For example, $g(w)=\tanh (w-1)$ satisfies this condition, but $g(w)=w^3-5$ does not.</p>
<p><strong><em>Condition (b)</em></strong><br>The second condition about $\left\{a_k\right\}$ is interesting. We often see conditions like this in reinforcement learning algorithms.</p>
<p>In particular, the condition $\sum_{k=1}^{\infty} a_k^2&lt;\infty$ means that $\lim _{n \rightarrow \infty} \sum_{k=1}^n a_k^2$ is bounded from above. It requires that $a_k$ converges to zero as $k \rightarrow \infty$.</p>
<p>The condition $\sum_{k=1}^{\infty} a_k=\infty$ means that $\lim _{n \rightarrow \infty} \sum_{k=1}^n a_k$ is infinitely large. It requires that $a_k$ should not converge to zero too fast. These conditions have interesting properties, which will be analyzed in detail shortly.</p>
<p><strong><em>Condition (c)</em></strong><br>The condition is mild. It does not require the observation error $\eta_k$ to be Gaussian. An important special case is that $\left\{\eta_k\right\}$ is an i.i.d. stochastic sequence satisfying $\mathbb{E}\left[\eta_k\right]=0$ and $\mathbb{E}\left[\eta_k^2\right]&lt;\infty$. In this case, the third condition is valid because $\eta_k$ is independent of $\mathcal{H}_k$ and hence we have $\mathbb{E}\left[\eta_k \mid \mathcal{H}_k\right]=\mathbb{E}\left[\eta_k\right]=0$ and $\mathbb{E}\left[\eta_k^2 \mid \mathcal{H}_k\right]=\mathbb{E}\left[\eta_k^2\right]$.</p>
<p><strong><u>Why is the second condition important for the convergence of the RM algorithm?</u></strong></p>
<p>This question can naturally be answered when we present a rigorous proof of the above theorem later. Here, we would like to provide some insightful intuition.</p>
<p>First, $\sum_{k=1}^{\infty} a_k^2&lt;\infty$ indicates that $a_k \rightarrow 0$ as $k \rightarrow \infty$. Why is this condition important? Suppose that the observation $\tilde{g}\left(w_k, \eta_k\right)$ is always bounded. Since </p>
<script type="math/tex; mode=display">
w_{k+1}-w_k=-a_k \tilde{g}\left(w_k, \eta_k\right)</script><p>if $a_k \rightarrow 0$, then $a_k \tilde{g}\left(w_k, \eta_k\right) \rightarrow 0$ and hence $w_{k+1}-w_k \rightarrow 0$, indicating that $w_{k+1}$ and $w_k$ approach each other when $k \rightarrow \infty$.Otherwise, if $a_k$ does not converge, then $w_k$ may still fluctuate when $k \rightarrow \infty$.</p>
<p>Second, $\sum_{k=1}^{\infty} a_k=\infty$ indicates that $a_k$ should not converge to zero too fast. Why is this condition important?</p>
<p>Summarizing both sides of the equations of $ w_2-w_1 = -a_1 \tilde{g}\left(w_1, \eta_1\right), w_3-w_2=-a_2 \tilde{g}\left(w_2, \eta_2\right), w_4-w_3=-a_3 \tilde{g}\left(w_3, \eta_3\right), \ldots $ gives </p>
<script type="math/tex; mode=display">
w_1-w_{\infty}=\sum_{k=1}^{\infty} a_k \tilde{g}\left(w_k, \eta_k\right) .</script><p>If $\sum_{k=1}^{\infty} a_k&lt;\infty$, then $\left|\sum_{k=1}^{\infty} a_k \tilde{g}\left(w_k, \eta_k\right)\right|$ is also bounded. But we will show that this can not guarantee the convergence. Let $b$ denote the finite upper bound such that </p>
<script type="math/tex; mode=display">
\begin{equation}
\left|w_1-w_{\infty}\right|=\left|\sum_{k=1}^{\infty} a_k \tilde{g}\left(w_k, \eta_k\right)\right| \leq b . 
\end{equation}</script><p>If the initial guess $w_1$ is selected far away from $w^\ast$ so that $\left|w_1-w^\ast\right|&gt;b$, then it is impossible to have $w_{\infty}=w^\ast$. This suggests that the RM algorithm cannot find the true solution $w^\ast$ in this case. Therefore, the condition $\sum_{k=1}^{\infty} a_k=\infty$ is necessary to ensure convergency given an arbitrary initial guess.</p>
<p><strong><u>An example of $a_k$</u></strong><br>What kinds of sequences satisfy $\sum_{k=1}^{\infty} a_k=\infty$ and $\sum_{k=1}^{\infty} a_k^2&lt;\infty$ ?</p>
<p>One typical sequence is </p>
<script type="math/tex; mode=display">
a_k=\frac{1}{k} .</script><p>On the one hand, it holds that </p>
<script type="math/tex; mode=display">
\lim _{n \rightarrow \infty}\left(\sum_{k=1}^n \frac{1}{k}-\ln n\right)=\kappa,</script><p>where $\kappa \approx 0.577$ is called the Euler-Mascheroni constant (or Euler’s constant) . Since $\ln n \rightarrow \infty$ as $n \rightarrow \infty$, we have </p>
<script type="math/tex; mode=display">
  \sum_{k=1}^{\infty} \frac{1}{k}=\infty</script><p>In fact, $H_n=\sum_{k=1}^n \frac{1}{k}$ is called the harmonic number in number theory [29]. On the other hand, it holds that </p>
<script type="math/tex; mode=display">
\sum_{k=1}^{\infty} \frac{1}{k^2}=\frac{\pi^2}{6}<\infty .</script><p>Finding the value of $\sum_{k=1}^{\infty} \frac{1}{k^2}$ is known as the Basel problem [30].</p>
<p>In summary, the sequence $\left\{a_k=1 / k\right\}$ satisfies the second condition in Theorem 6.1. Notably, a slight modification, such as $a_k=1 /(k+1)$ or $a_k=c_k / k$ where $c_k$ is bounded, also preserves this condition.</p>
<p><strong>Note:</strong><br>In the RM algorithm, <strong>$a_k$ is often selected as a sufficiently small constant in many applications.</strong> Although the second condition is not satisfied anymore in this case because $\sum_{k=1}^{\infty} a_k^2=\infty$ rather than $\sum_{k=1}^{\infty} a_k^2&lt;\infty$, the algorithm can still converge in a certain sense.</p>
<h4 id="6-2-4-Mean-estimation-is-a-special-example-of-RM-algorithm"><a href="#6-2-4-Mean-estimation-is-a-special-example-of-RM-algorithm" class="headerlink" title="6.2.4 Mean estimation is a special example of RM algorithm"></a>6.2.4 Mean estimation is a special example of RM algorithm</h4><p>In particular, define a function as</p>
<script type="math/tex; mode=display">
g(w) = w- \mathbb{E}[X]</script><p>The original problem is to obtain the value of $\mathbb{E}[X]$. This problem is formulated as a root- finding problem to solve $g(w) = 0$. Given a value of $w$, the noisy observation that we can obtain is $\tilde{g} = w -x$, where $x$ is a sample of $X$. Note that $\tilde{g}$ can be written as</p>
<script type="math/tex; mode=display">
\begin{aligned} 
\tilde{g}(w, \eta) & = w-x \\ 
& =w-x+\mathbb{E}[X] -\mathbb{E}[X] \\
& =(w-\mathbb{E}[X]) + (\mathbb{E}[X]-x)\\
& =g(w)+\eta
\end{aligned}</script><p>The RM algorithm for solving this problem is</p>
<script type="math/tex; mode=display">
w_{k+1}=w_k- a_k \tilde{g}\left(w_k, \eta_k\right)=w_k-a_k(w_k-x_k).</script><p>So, solving $\mathbb{E}(X)$ by mean estimation can be convert to solving <strong>a RM algorithm with $g(w) = w-\mathbb{E}(x)$ and $a_k=\frac{1}{k}$.</strong></p>
<h3 id="6-3-Stochastic-gradient-descent"><a href="#6-3-Stochastic-gradient-descent" class="headerlink" title="6.3 Stochastic gradient descent"></a>6.3 Stochastic gradient descent</h3><p>Stochastic Gradient Descent (SGD) algorithm: minimize $J(w)=\mathbb{E}[f(w, X)]$ using $\left\{\nabla_w f\left(w_k, x_k\right)\right\}$ </p>
<script type="math/tex; mode=display">
w_{k+1}=w_k-\alpha_k \nabla_w f\left(w_k, x_k\right)</script><p>The iterative mean estimation algorithm is a special form of the Stochastic Gradient Descent (SGD) algorithm, and  the SGD algorithm is a special form of the RM algorithm. (<strong>IME → SGD → RM</strong>)</p>
<p>Consider the following optimization problem:</p>
<script type="math/tex; mode=display">
\min_w J(w)=\mathbb{E}[f(w, X)]</script><p>where $w$ is the parameter to be optimized. $X$ is a random variable. The expectation is with respect to $X$. $w$ and $X$ can be either scalars or vectors. The function $f(\cdot)$ is a scalar.</p>
<p><strong>The gradient descent algorithm (GD)</strong></p>
<script type="math/tex; mode=display">
\begin{aligned} 
w_{k+1} & = w_k-\alpha_k \nabla_w J(w) = w_k-\alpha_k  \nabla_w \mathbb{E}\left[f\left(w_k, X\right)\right]\\
& = w_k-\alpha_k \textcolor{blue} {\mathbb{E}\left[\nabla_w f\left(w_k, X\right)\right]}
\end{aligned}</script><p><em>we must know the probability distribution of $X$, if you want use this algorithm.</em></p>
<p><strong>The batch gradient descent algorithm (BGD)</strong><br>We can use Monte Carlo estimation to compute the estimate $\mathbb{E}\left[\nabla_w f\left(w_k, X\right)\right]$. </p>
<script type="math/tex; mode=display">
\begin{aligned} 
w_{k+1} & = w_k-\alpha_k \mathbb{E}\left[\nabla_w f\left(w_k, X\right)\right]\\
&\approx w_k-\alpha_k \textcolor{blue} {\frac{1}{n} \sum_{i=1}^n \nabla_w f(w_k , x_i)} \\
&(\text{ if no model need have data/samples})
\end{aligned}</script><p><em>One problem of the BGD algorithm is that it requires all the samples in each iterationfor each $w_k$.</em></p>
<p><strong>The stochastic gradient descent algorithm (SGD)</strong></p>
<script type="math/tex; mode=display">
\begin{equation}
w_{k+1} = w_k-\alpha_k \textcolor{blue} {\nabla_w f\left(w_k, x_k\right)}, \end{equation}</script><p>where $x_k$ is the sample collected at time step $k$. It relies on stochastic samples ${x_k}$</p>
<ol>
<li><p>Compared to the gradient descent algorithm: Replace the true gradient $\mathbb{E}\left[\nabla_w f\left(w_k, X\right)\right]$ by the stochastic gradient $\nabla_w f\left(w_k, x_k\right)$. </p>
</li>
<li><p>Compared to the batch gradient descent method: let $n=1$.</p>
</li>
<li><p><strong>From GD to SGD</strong>: The stochastic gradient $\nabla_w f\left(w_k, x_k\right)$ can be viewed as a noisy measurement of the true gradient $\mathbb{E}\left[\nabla_w f(w, X)\right]$: </p>
</li>
</ol>
<script type="math/tex; mode=display">
\nabla_w f\left(w_k, x_k\right)=\mathbb{E}\left[\nabla_w f(w, X)\right]+\underbrace{\nabla_w f\left(w_k, x_k\right)-\mathbb{E}\left[\nabla_w f(w, X)\right]}_\eta .</script><script type="math/tex; mode=display">
\begin{aligned} 
\mathbb{E}(\eta) & =\mathbb{E}(\nabla_w f\left(w_k, x_k\right)-\mathbb{E}\left[\nabla_w f(w, X)\right])\\
& =\mathbb{E}_{x_k}[\nabla_w f\left(w_k, x_k\right)]-\mathbb{E}_X\left[\nabla_w f(w, X)\right]=0
\end{aligned}</script><p>Therefore, the perturbation term $\eta_{k}$ has a zero mean, which intuitively suggests that it may not jeopardize the convergence property. </p>
<h4 id="6-3-1-Mean-estimation-is-a-special-example-of-SGD"><a href="#6-3-1-Mean-estimation-is-a-special-example-of-SGD" class="headerlink" title="6.3.1 Mean estimation is a special example of SGD"></a>6.3.1 Mean estimation is a special example of SGD</h4><p>we formulate the mean estimation problem as an optimization problem:</p>
<script type="math/tex; mode=display">
 \min _w J(w)=\mathbb{E}[f(w, X)]=\mathbb{E}\left[\frac{1}{2}\|w-X\|^2\right],</script><p>where $ f(w, X)=|w-X|^2 / 2 \quad \nabla_w f(w, X)=w-X $ . It can be verified that the optimal solution is by solving $\textcolor{green} {\nabla_w J(w)=0}$. Therefore, this optimization problem is equivalent to the iterative mean estimation problem.</p>
<script type="math/tex; mode=display">
\begin{aligned}
w_{k+1} & =w_k-\alpha_k \nabla_w J\left(w_k\right) \\ & =w_k-\alpha_k \mathbb{E}\left[\nabla_w f\left(w_k, X\right)\right] \\ & =w_k-\alpha_k \mathbb{E}\left[w_k-X\right] . 
\end{aligned}</script><p>The SGD algorithm for solving the above problem is </p>
<script type="math/tex; mode=display">
w_{k+1}=w_k-\alpha_k \nabla_w f\left(w_k, x_k\right)=w_k-\alpha_k\left(w_k-x_k\right)</script><p><strong>We can therefore see that the iterative mean estimation algorithm is a special form of the SGD algorithm for solving the mean estimation problem.</strong></p>
<h4 id="6-3-2-SGD-is-a-special-example-of-RM-algorithm-Convergence-analysis"><a href="#6-3-2-SGD-is-a-special-example-of-RM-algorithm-Convergence-analysis" class="headerlink" title="6.3.2 SGD is a special example of RM algorithm (Convergence analysis)"></a>6.3.2 SGD is a special example of RM algorithm (Convergence analysis)</h4><p>We next show that SGD is a special RM algorithm, Then, the convergence naturally follows.</p>
<p>The aim of SGD is to minimize </p>
<script type="math/tex; mode=display">
J(w)=\mathbb{E}[f(w, X)]</script><p>This problem can be converted to a root-finding problem: </p>
<script type="math/tex; mode=display">
\nabla_w J(w)=\mathbb{E}\left[\nabla_w f(w, X)\right]=0</script><p>Let </p>
<script type="math/tex; mode=display">
\textcolor{blue} {g(w)=\nabla_w J(w)=\mathbb{E}\left[\nabla_w f(w, X)\right]} .</script><p>Then, the aim of SGD is to find the root of $g(w)=0$.</p>
<p>As in RM algorithm, we don’t know $g(w)$. Knowing it needs GD (gradient descent), not SGD.</p>
<p>What we can measure is </p>
<script type="math/tex; mode=display">
\begin{aligned}
 \tilde{g}(w, \eta) & =\nabla_w f(w, x) \\ & =\underbrace{\mathbb{E}\left[\nabla_w f(w, X)\right]}_{g(w)}+\underbrace{\nabla_w f(w, x)-\mathbb{E}\left[\nabla_w f(w, X)\right]}_\eta .
\end{aligned}</script><p>Then, the RM algorithm for solving $g(w)=0$ is </p>
<script type="math/tex; mode=display">
w_{k+1}=w_k-a_k \tilde{g}\left(w_k, \eta_k\right)=w_k-a_k \nabla_w f\left(w_k, x_k\right) .</script><p><strong>It is exactly the SGD algorithm. Therefore, SGD is a special RM algorithm.Since SGD is a special RM algorithm, its convergence naturally follows.</strong></p>
<p><strong>Theorem</strong> (Convergence of SGD): In the SGD algorithm, if</p>
<p>$0&lt;c_1 \leq \nabla_w^2 f(w, X) \leq c_2$;<br>$\sum_{k=1}^{\infty} a_k=\infty$ and $\sum_{k=1}^{\infty} a_k^2&lt;\infty$;<br>$\left\{x_k\right\}_{k=1}^{\infty}$ is iid;<br>then $w_k$ converges to the root of $ \mathbb{E}[\nabla_wf(w, X)]=0$ with probability 1.</p>
<h4 id="6-3-3-Convergence-pattern"><a href="#6-3-3-Convergence-pattern" class="headerlink" title="6.3.3 Convergence pattern"></a>6.3.3 Convergence pattern</h4><p>Since the stochastic gradient is random and hence the approximation is inaccurate, whether the convergence of SGD is slow or random? To answer this question, we consider the relative error between the stochastic and batch gradients:</p>
<p>The relative error between the stochastic and true gradients is </p>
<script type="math/tex; mode=display">
\delta_k \triangleq \frac{\left|\nabla_w f\left(w_k, x_k\right)-\mathbb{E}\left[\nabla_w f\left(w_k, X\right)\right]\right|}{\left|\mathbb{E}\left[\nabla_w f\left(w_k, X\right)\right]\right|} .</script><p>For the sake of simplicity, we consider the case where $w$ and $\nabla_w f(w, x)$ are both scalars. Since $w^\ast$ is the optimal solution, it holds that $\mathbb{E}\left[\nabla_w f\left(w^\ast, X\right)\right]=0$. Then, the relative error can be rewritten as </p>
<script type="math/tex; mode=display">
\delta_k=\frac{\left|\nabla_w f\left(w_k, x_k\right)-\mathbb{E}\left[\nabla_w f\left(w_k, X\right)\right]\right|}{\left|\mathbb{E}\left[\nabla_w f\left(w_k, X\right)\right]-\textcolor{blue} {\mathbb{E} [\nabla_w f\left(w^\ast, X\right)] }\right|} = \frac{\left|\nabla_w f\left(w_k, x_k\right)-\mathbb{E}\left[\nabla_w f\left(w_k, X\right)\right]\right|}{| \textcolor{purple} {\mathbb{E} [\nabla_w^2 f\left(\tilde{w}_k, X\right)\left(w_k-w^\ast\right)] }|},</script><p>where the last equality is due to the <strong>mean value theorem</strong> and $\tilde{w}_k \in\left[w_k, w^\ast\right]$. Suppose that $f$ is strictly convex such that $\nabla_w^2 f \geq c&gt;0$ for all $w, X$. Then, the denominator becomes </p>
<script type="math/tex; mode=display">
 \begin{aligned} \left|\mathbb{E}\left[\nabla_w^2 f\left(\tilde{w}_k, X\right)\left(w_k-w^\ast\right)\right]\right| & =\left|\mathbb{E}\left[\nabla_w^2 f\left(\tilde{w}_k, X\right)\right]\right|\left|\left(w_k-w^\ast\right)\right| \\ & \geq c\left|w_k-w^\ast\right| . \end{aligned}</script><p>Substituting the above inequality into the above equation yields </p>
<script type="math/tex; mode=display">
\delta_k \leq \frac{|\overbrace{\nabla_w f\left(w_k, x_k\right)}^{\text {stochastic gradient }}-\overbrace{\mathbb{E}\left[\nabla_w f\left(w_k, X\right)\right]}^{\text {true gradient }}|}{\underbrace{c\left|w_k-w^\ast\right|}_{\text {distance to the optimal solution }}} .</script><p>The above inequality suggests an interesting convergence pattern of SGD: the relative error $\delta_k$ is inversely proportional to $\left|w_k-w^\ast\right|$. As a result, when $\left|w_k-w^\ast\right|$ is large, $\delta_k$ is small. In this case, the SGD algorithm behaves like the gradient descent algorithm and hence $w_k$ quickly converges to $w^\ast$. When $w_k$ is close to $w^\ast$, the relative error $\delta_k$ may be large, and the convergence exhibits more randomness.</p>
<p><img src="/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-04-25-21-20-47.png" alt=""></p>
<p>The simulation results are shown in Figure. Here, $X\in \mathbb{R^2}$ represents a random position in the plane. Its distribution is uniform in the square area centered at the origin and $\mathbb{E}[X] = 0$. The mean estimation is based on 100 i.i.d. samples. <strong>Although the initial guess of the mean is far away from the true value, it can be seen that the SGD estimate quickly approaches the neighborhood of the origin. When the estimate is close to the origin, the convergence process exhibits certain randomness.</strong></p>
<h4 id="6-3-4-BGD-SGD-and-mini-batch-GD"><a href="#6-3-4-BGD-SGD-and-mini-batch-GD" class="headerlink" title="6.3.4 BGD, SGD, and mini-batch GD"></a>6.3.4 BGD, SGD, and mini-batch GD</h4><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">algorithms</th>
<th style="text-align:center">sample used in every iteration</th>
<th style="text-align:center">randomness</th>
<th style="text-align:center">flexible and efficient</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">SGD</td>
<td style="text-align:center">a single sample ($1$)</td>
<td style="text-align:center">maximum</td>
<td style="text-align:center">maximum</td>
</tr>
<tr>
<td style="text-align:center">MBGD</td>
<td style="text-align:center">a few more samples ($1&lt;=m&lt;=n$)</td>
<td style="text-align:center">median</td>
<td style="text-align:center">median</td>
</tr>
<tr>
<td style="text-align:center">BGD</td>
<td style="text-align:center">all samples   ($n$)</td>
<td style="text-align:center">minimum</td>
<td style="text-align:center">minimum</td>
</tr>
</tbody>
</table>
</div>
<p><strong>notes</strong><br>If $m=n$, MBGD does NOT become BGD strictly speaking, because MBGD uses randomly fetched $n$ samples whereas BGD uses all $n$ numbers. In particular, MBGD may use a value in $\left\{x_i\right\}_{i=1}^n$ multiple times whereas BGD uses each number once.</p>
<p>Suppose we would like to minimize $J(w)=\mathbb{E}[f(w, X)]$, given a set of random samples $\left\{x_i\right\}_{i=1}^n$ of $X$. The BGD, SGD, MBGD algorithms solving this problem are, respectively, </p>
<script type="math/tex; mode=display">
\begin{aligned}
& w_{k+1}=w_k-\alpha_k \frac{1}{n} \sum_{i=1}^n \nabla_w f\left(w_k, x_i\right), \quad \text{(BGD)}\\ & w_{k+1}=w_k-\alpha_k \frac{1}{m} \sum_{j \in \mathcal{I}_k} \nabla_w f\left(w_k, x_j\right), \quad \text{(MBGD)}\\ & w_{k+1}=w_k-\alpha_k \nabla_w f\left(w_k, x_k\right) \quad \text{(SGD)} 
\end{aligned}</script><p>where $n$ is the size of the dataset, $m \leq n$.</p>
<ol>
<li><p>In the BGD algorithm, all the samples are used in every iteration. When $n$ is large, $(1 / n) \sum_{i=1}^n \nabla_w f\left(w_k, x_i\right)$ is close to the true gradient $\mathbb{E}\left[\nabla_w f\left(w_k, X\right)\right]$.</p>
</li>
<li><p>In the MBGD algorithm, $\mathcal{I}_k$ is a subset of $\{1, \ldots, n\}$ with the size as $\left|\mathcal{I}_k\right|=m$. The set $\mathcal{I}_k$ is obtained by $m$ times i.d.d. samplings.</p>
</li>
<li><p>In the SGD algorithm, $x_k$ is randomly sampled from $\left\{x_i\right\}_{i=1}^n$ at time $k$.</p>
</li>
</ol>
<p><strong>A good example</strong><br>Given some numbers $\left\{x_i\right\}_{i=1}^n$, our aim is to calculate the mean $\bar{x}=\sum_{i=1}^n x_i / n$. This problem can be equivalently stated as the following optimization problem: </p>
<script type="math/tex; mode=display">
\min _w J(w)=\frac{1}{2 n} \sum_{i=1}^n\left\|w-x_i\right\|^2</script><p>The three algorithms for solving this problem are, respectively, </p>
<script type="math/tex; mode=display">
\begin{aligned} & w_{k+1}=w_k-\alpha_k \frac{1}{n} \sum_{i=1}^n\left(w_k-x_i\right)=w_k-\alpha_k\left(w_k-\bar{x}\right), \quad \text{(BGD)} \\ & w_{k+1}=w_k-\alpha_k \frac{1}{m} \sum_{j \in \mathcal{I}_k}\left(w_k-x_j\right)=w_k-\alpha_k\left(w_k-\bar{x}_k^{(m)}\right), \quad \text{(MBGD)}\\ & w_{k+1}=w_k-\alpha_k\left(w_k-x_k\right), \quad \text { (SGD) } \end{aligned}</script><p>where $\bar{x}_k^{(m)}=\sum_{j \in \mathcal{I}_k} x_j / m$. Furthermore, if $\alpha_k=1 / k$, the above equation can be solved as </p>
<script type="math/tex; mode=display">
 \begin{aligned} & w_{k+1}=\frac{1}{k} \sum_{j=1}^k \bar{x}=\bar{x}, \quad \text{(BGD)} \\ & w_{k+1}=\frac{1}{k} \sum_{j=1}^k \bar{x}_j^{(m)}, \quad \text{(MBGD)}\\ & w_{k+1}=\frac{1}{k} \sum_{j=1}^k x_j . \quad \text{(SGD)} \end{aligned}</script><ol>
<li><p>The estimate of BGD at each step is exactly the optimal solution $w^\ast=\bar{x}$.</p>
</li>
<li><p>The estimate of MBGD converges the mean faster than SGD because $\bar{x}_k^{(m)}$ is already an average.</p>
</li>
<li><p>the convergence rate of SGD is still fast, especially when $w_k$ is far from $w^\ast$.</p>
</li>
</ol>
<h3 id="6-4-Summary"><a href="#6-4-Summary" class="headerlink" title="6.4 Summary"></a>6.4 Summary</h3><p>This chapter introduced the preliminaries of <strong>stochastic approximation</strong> such as the RM and SGD algorithms. Compared to many other root-finding algorithms, the RM algorithm does <strong>not require the expression of the objective function or its derivative.</strong> It has been shown that the SGD algorithm is a special RM algorithm. Moreover, an important problem frequently discussed throughout this chapter is mean estimation. The mean estimation algorithm is a special SGD algorithm. We will see in Chapter 7 that temporal-difference learning algorithms have similar expressions. Finally, the name <strong>“stochastic approximation”</strong> was first used by Robbins and Monro in 1951 <sup><a href="#fn_1" id="reffn_1">1</a></sup>. More information about stochastic approximation can be found in <sup><a href="#fn_2" id="reffn_2">2</a></sup>.</p>
<p><sup><a href="#fn_1" id="reffn_1">1</a></sup>:H. Robbins and S. Monro, A stochastic approximation method, The Annals of<br> Mathematical Statistics, pp. 400407, 1951.</p>
<blockquote id="fn_2">
<sup>2</sup>. H.-F. Chen, Stochastic approximation and its applications, vol. 64. Springer Science &amp; Business Media, 2006<a href="#reffn_2" title="Jump back to footnote [2] in the text."> ↩</a>
</blockquote>
<h2 id="7-Temporal-Difference-Methods"><a href="#7-Temporal-Difference-Methods" class="headerlink" title="7 Temporal-Difference Methods"></a>7 Temporal-Difference Methods</h2><p>Temporal-difference (TD) learning is one of the most well-known methods in reinforcement learning (RL).TD learning often refers to a broad class of reinforcement learning algorithms(Sarsa\ Expectded Sarsa\ n-step Sarsa\Q-learning eta).<br>Monte Carlo (MC) learning and TD learning are the model-free methods. but TD learning has some advantages due to its <strong>incremental form</strong>.<br>TD learning algorithms can be viewed as special stochastic algorithms for solving the Bellman or Bellman optimality equations.</p>
<h3 id="7-1-TD-learning-of-state-values"><a href="#7-1-TD-learning-of-state-values" class="headerlink" title="7.1 TD learning of state values"></a>7.1 TD learning of state values</h3><h4 id="7-1-1-Algorithm-description"><a href="#7-1-1-Algorithm-description" class="headerlink" title="7.1.1 Algorithm description"></a>7.1.1 Algorithm description</h4><p>TD algorithm can estimate the state values of a given policy using the following equations:<br>Suppose that we have some experience samples $(s_0, r_1, s_1,\dots, s_t, r_{t+1}, s_{t+1},\dots)$ generated following .</p>
<script type="math/tex; mode=display">
\begin{equation}
\textcolor{blue}{v_{t+1}(s_t) =v_{t}(s_t)-\alpha_t(s_t)[v_{t}(s_t)-(r_{t+1}+\gamma v_{t}(s_{t+1}))]}  , \end{equation}</script><script type="math/tex; mode=display">
\begin{equation}
\textcolor{blue}{v_{t+1}(s) =v_{t}(s) \quad \text{for  all} \quad s \neq s_t} \end{equation}</script><p>where $t = 0,1,2,\dots$ denotes the time step. Here, $v_t(s_t)$ is the estimate of $v_\pi(s_t)$ at time $t$; $\alpha_t(s_t)$ is the learning rate for $s_t$ at time $t$. Equation (7) is often omitted for simplicity, but it should be kept in mind because the algorithm would be <em>mathematically incomplete without this equation</em>.</p>
<p>TD learning algorithm can be viewed as a special stochastic approximation<br> algorithm(i.e. Robbins-Monro algorithm) for solving the Bellman equation.</p>
<script type="math/tex; mode=display">
v_{t+1}(s_t) =\textcolor{blue}{\mathbb{E}[R_{t+1}+\gamma G_{t+1}\mid S_{t}=s], s \in S} \longrightarrow \textcolor{green}{\mathbb{E}[R_{t+1}+\gamma v_{\pi}(S_{t+1})\mid S_{t}=s], s \in S}</script><p>It is another expression of the Bellman equation which sometimes called the <strong>Bellman expectation equation</strong>.</p>
<blockquote>
<p>Derivation of the TD algorithm</p>
<script type="math/tex; mode=display">
g(v_{\pi}(s_t))=v_{\pi}(s_t) - \mathbb{E}[R_{t+1}+\gamma v_{\pi}(S_{t+1})\mid S_{t}=s_t]</script><p>our goal is to solve the $g(v_{\pi}(s_t))=0$ to obtain $v_{\pi}(s_t)$ using the RM algorithm. $r_{t+1}$ and $s_{t+1}$ are the samples of $R_{t+1}$ and $S_{t+1}$,the noisy observation of $g(v_{\pi}(s_t))$ that we can obtain is</p>
<script type="math/tex; mode=display">
\tilde{g}(v_{\pi}(s_t))=v_{\pi}(s_t) - [r_{t+1}+\gamma v_{\pi}(s_{t+1})]=g(v_{\pi}(s_t))+\eta</script><p>Therefore, the Robbins-Monro algorithm (Section 6.2) for solving $g(v_{\pi}(s_t)) = 0$ is </p>
<script type="math/tex; mode=display">
\begin{aligned} 
v_{t+1}(s_t) &=v_{t}(s_t)-\alpha_t(s_t)\tilde{g}(v_{\pi}(s_t))\\
& =v_{t}(s_t)-\alpha_t(s_t)[v_{\textcolor{red}{\pi}}(s_t)-(r_{t+1}+\gamma v_{\textcolor{red}{\pi}}(s_{t+1}))]\\
\text{(convergence)}&\approx v_{t}(s_t)-\alpha_t(s_t)[v_{\textcolor{red}{t}}(s_t)-(r_{t+1}+\gamma v_{\textcolor{red}{t}}(s_{t+1}))]
\end{aligned}</script><p>$v_t(s)$ converges almost surely to $v_{\pi}(s)$ as $t \to \infty$ for all $s \in S$</p>
</blockquote>
<h4 id="7-1-2-Property-analysis-TD-target-and-TD-error"><a href="#7-1-2-Property-analysis-TD-target-and-TD-error" class="headerlink" title="7.1.2 Property analysis (TD target and TD error)"></a>7.1.2 Property analysis (TD target and TD error)</h4><p>TD algorithm can be described as</p>
<script type="math/tex; mode=display">
\begin{equation}
\textcolor{red}{\underbrace{v_{t+1}(s_t)}_{\text{new estimate}} =\underbrace{v_{t}(s_t)}_{\text{current estimate}}-\alpha_t(s_t)[\overbrace{v_{t}(s_t)-(\underbrace{r_{t+1}+\gamma v_{t}(s_{t+1})}_{\text{TD target} \quad{} \tilde{v}_t})}^{\text{TD error} \quad \delta_t}]} \end{equation}</script><p><strong>TD target</strong> : $\tilde{v}_t=r_{t+1}+\gamma v_{t}(s_{t+1})$<br>Key:The current estimated state value ($\tilde{v}_t$ or $\tilde{v}_t(s_t)$) is obtained  by using the next state value $v_t(s_{t+1})$  in time step $t+1$.</p>
<p><strong>TD error</strong> :$\delta_t=v_{t}(s_t)-\tilde{v}_t=v_{t}(s_t)-(r_{t+1}+\gamma v_{t}(s_{t+1}))$<br>Key:the error of the current state value $v_t(s_t)$ with the estimated state value ($\tilde{v}_t$ or $\tilde{v}_t(s_t)$) in time step $t$.</p>
<p>the TD error can be interpreted as the innovation, which indicates new information obtained from the experience sample $(s_t, r_{t+1}, s_{t+1})$. The fundamental idea of TD learning is to correct our current estimate of the state value based on the newly obtained information.<br><strong>Note</strong>: </p>
<ol>
<li>the new value $v_{t+1}(s_t)$ is closer to $\tilde{v}_t$ than the old value $v_t(s_t)$. Therefore, this algorithm mathematically drives $v_t(s_t)$ toward $\tilde{v}_t$. This is why $\tilde{v}_t$ is called the TD target.</li>
<li>TD error is called temporal-difference because $\delta_t$ reflects the discrepancy between two time steps $t$ and $t + 1$. In addition, the TD error is zero in the expectation sense when the state value estimate is accurate. So the TD error also reflects the discrepancy between the estimate $v_t$ and the true state value $v_{\pi}$.<h4 id="7-1-3-Compare-TD-Learning-with-MC-Learning"><a href="#7-1-3-Compare-TD-Learning-with-MC-Learning" class="headerlink" title="7.1.3 Compare TD Learning with MC Learning"></a>7.1.3 Compare TD Learning with MC Learning</h4>|                                                                                  TD learning                                                                                   |                                                                    MC learning                                                                    |<br>| :——————————————————————————————————————————————————————————————————————————————————————: | :———————————————————————————————————————————————————————————————————————-: |<br>|                                      <strong>Online</strong>: It can update the state/action values immediately after receiving an experience sample.                                       | <strong>Offline</strong>: It must wait until an episode has been completely collected. That is because it must calculate the discounted return of the episode. |<br>|                                                <strong>Continuing tasks</strong>: online, it can handle both episodic and continuing tasks.                                                 |           <strong>Episodic tasks</strong>:  offline, it can only handle episodic tasks where the episodes terminate after a finite number of steps.            |<br>| <strong>Bootstrapping</strong>: Because the update of a state/action value relies on the previous estimate of this value. As a result, TD learning requires an initial guess of the values. |                       <strong>Non-bootstrapping</strong>: Because it can directly estimate state/action values without initial guesses.                        |<br>|                                                     <strong>Low estimation variance</strong>: few Random variables but biased estimate                                                      |                                    <strong>High estimation variance</strong>: many Random variables but non-biased estimate                                    |</li>
</ol>
<h3 id="7-2-TD-learning-of-action-values-Sarsa"><a href="#7-2-TD-learning-of-action-values-Sarsa" class="headerlink" title="7.2 TD learning of action values : Sarsa"></a>7.2 TD learning of action values : <strong>Sarsa</strong></h3><p>The TD algorithm introduced in Section 7.1 can only estimate the state values of a given policy(merely used for policy evaluation). To find optimal policies, we still need to further calculate the action values and then conduct policy improvement.<strong> Sarsa</strong> can directly estimate action values. <em>Estimating action values is important because it can be combined with a policy improvement step to learn optimal policies.</em><br>Sarsa is an abbreviation for state-action-reward-state-action. because each iteration of the algorithm requires $(s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1})$.  The Sarsa algorithm was first proposed in <sup><a href="#fn_3" id="reffn_3">3</a></sup> and its name was coined by <sup><a href="#fn_4" id="reffn_4">4</a></sup>.</p>
<blockquote id="fn_3">
<sup>3</sup>. G. A. Rummery and M. Niranjan, On-line Q-learning using connectionist systems. Technical Report, Cambridge University, 1994.<a href="#reffn_3" title="Jump back to footnote [3] in the text."> ↩</a>
</blockquote>
<blockquote id="fn_4">
<sup>4</sup>. R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction (2nd Edition). MIT Press, 2018.<a href="#reffn_4" title="Jump back to footnote [4] in the text."> ↩</a>
</blockquote>
<h4 id="7-2-1-Algorithm-description"><a href="#7-2-1-Algorithm-description" class="headerlink" title="7.2.1 Algorithm description"></a>7.2.1 Algorithm description</h4><p>Sarsa can estimate the action values of a given policy using the following equations:<br>Suppose that we have some experience samples $(s_0,a_0, r_1, s_1,a_1,\dots, s_t,a_t, r_{t+1}, s_{t+1},a_{t+1},\dots)$ generated following .</p>
<script type="math/tex; mode=display">
\begin{equation}
\textcolor{blue}{q_{t+1}(s_t,a_t) =q_{t}(s_t,a_t)-\alpha_t(s_t,a_t)[q_{t}(s_t,a_t)-(r_{t+1}+\gamma q_{t}(s_{t+1},a_{t+1}))]}  , \end{equation}</script><script type="math/tex; mode=display">
\begin{equation}
\textcolor{blue}{q_{t+1}(s,a) =q_{t}(s,a) \quad \text{for  all} \quad (s,a) \neq (s_t,a_t)} \end{equation}</script><p>where $t = 0,1,2,\dots$ denotes the time step. Here, $q_t(s_t,a_t)$ is the estimate of q_\pi(s_t,a_t) at time $t$; $\alpha_t(s_t,a_t)$ is the learning rate for $(s_t,a_t)$ at time $t$.<br>Sarsa is a stochastic approximation algorithm for solving the Bellman equation of a given policy.</p>
<script type="math/tex; mode=display">
q_{\pi}(s,a) =\textcolor{blue}{\mathbb{E}[R+\gamma q_{\pi}(S',A')\mid s,a], \quad \text{for all} \quad (s,a)}</script><h4 id="7-2-2-Optimal-policy-learning-via-Sarsa"><a href="#7-2-2-Optimal-policy-learning-via-Sarsa" class="headerlink" title="7.2.2 Optimal policy learning via Sarsa"></a>7.2.2 Optimal policy learning via Sarsa</h4><p>In Algorithm 7.1,each iteration has two steps. The first step is to update the q-value of the visited state-action pair. The second step is to update the policy to an $\epsilon$-greedy one.we do not evaluate a given policy su fficiently well before updating the policy. This is based on the idea of <strong>generalized policy iteration</strong>.<br>Moreover, after the policy is updated, the policy is immediately used to generate the next experience sample. The policy here is $\epsilon$-greedy so that it is <strong>exploratory</strong>.<br>-1 The core idea is simple: that is to use an algorithm to solve the Bellman equation of a given policy.<br>-2 The complication emerges when we try to nd optimal policies and work efficiently.<br><img src="/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-05-22-16-27-35.png" alt="7.1"><br><img src="/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-05-22-16-36-18.png" alt="7.2"><br>the total reward of each episode increases gradually. That is because the initial policy is not good and hence negative rewards are frequently obtained. As the policy becomes better, the total reward increases.<br>Notably, the length of an episode may increase abruptly (e.g., the 460th episode) and the corresponding total reward also drops sharply. That is because the policy is-greedy, and there is a chance for it to take non-optimal actions. One way to resolve this problem is to use decaying $\epsilon$ whose value converges to zero gradually(Gradually reduce $\epsilon$ or deal with smooth).</p>
<h3 id="7-3-TD-learning-of-action-values-Expectded-Sarsa"><a href="#7-3-TD-learning-of-action-values-Expectded-Sarsa" class="headerlink" title="7.3 TD learning of action values : Expectded Sarsa"></a>7.3 TD learning of action values : Expectded Sarsa</h3><script type="math/tex; mode=display">
\begin{equation}
\textcolor{blue}{q_{t+1}(s_t,a_t) =q_{t}(s_t,a_t)-\alpha_t(s_t,a_t)[q_{t}(s_t,a_t)-(r_{t+1}+\gamma \mathbb{E}[q_{t}(s_{t+1},A)])]}  , \end{equation}</script><script type="math/tex; mode=display">
\begin{equation}
\textcolor{blue}{q_{t+1}(s,a) =q_{t}(s,a) \quad \text{for  all} \quad (s,a) \neq (s_t,a_t)} \end{equation}</script><p>where $\mathbb{E}[q_{t}(s_{t+1},A)]=\sum_a\pi_t(a \mid s_{t+1})q_t(s_{t+1},a)=v_t(s_{t+1})$.<br>Since the algorithm involves an expected value, it is called Expected Sarsa. Although calculating the expected value may increase the computational complexity slightly, it is beneficial in the sense that it reduces the estimation variances because it reduces the random variables in Sarsa from ${s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}}$ to ${s_t, a_t, r_{t+1}, s_{t+1}}$ .</p>
<h3 id="7-4-TD-learning-of-action-values-n-step-Sarsa"><a href="#7-4-TD-learning-of-action-values-n-step-Sarsa" class="headerlink" title="7.4 TD learning of action values : n-step Sarsa"></a>7.4 TD learning of action values : n-step Sarsa</h3><p>The n-step Sarsa algorithm is a generalization of the Sarsa algorithm. It will be shown that Sarsa and MC learning are two special cases of n-step Sarsa.<br><img src="/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-05-22-16-53-59.png" alt=""><br>In summary, n-step Sarsa is a more general algorithm because it becomes the (one step) Sarsa algorithm when $n = 1$ and the MC learning algorithm when $n = \infty$(by setting $\alpha_t = 1$).</p>
<script type="math/tex; mode=display">
\begin{equation}
\textcolor{blue}{q_{t+n}(s_t,a_t) =q_{t+n-1}(s_t,a_t)-\alpha_{t+n-1}(s_t,a_t)[q_{t+n-1}(s_t,a_t)-(r_{t+1}+\gamma r_{t+2}+\dots+ \gamma^n q_{t+n-1}(s_{t+n},a_{t+n})])}  , \end{equation}</script><p>where $q_{t+n}(s_t, a_t)$ is the estimate of $q_{\pi}(s_t, a_t)$ at time $t + n$.<br>In particular, if $n$ is selected as a large number, n-step Sarsa is close to MC learning: the estimate has a relatively high variance but a small bias. If $n$ is selected to be small, n-step Sarsa is close to Sarsa: the estimate has a relatively large bias but a low variance.</p>
<h3 id="7-5-TD-learning-of-optimal-action-values-Q-Learning"><a href="#7-5-TD-learning-of-optimal-action-values-Q-Learning" class="headerlink" title="7.5 TD learning of optimal action values : Q-Learning"></a>7.5 TD learning of optimal action values : <strong>Q-Learning</strong></h3><p>The other TD algorithms aim to solve the Bellman equation of a given policy, Q-learning aims to directly solve the Bellman optimality equation to obtain optimal policies. Recall that Sarsa can only estimate the action values of a given policy, and it must be combined with a policy improvement step to find optimal policies. By contrast, Q-learning can directly estimate optimal action values and find optimal policies.</p>
<h4 id="7-5-1-Algorithm-description"><a href="#7-5-1-Algorithm-description" class="headerlink" title="7.5.1 Algorithm description"></a>7.5.1 Algorithm description</h4><p>The Q-learning algorithm is:</p>
<script type="math/tex; mode=display">
\begin{equation}
\textcolor{blue}{q_{t+1}(s_t,a_t) =q_{t}(s_t,a_t)-\alpha_t(s_t,a_t)[q_{t}(s_t,a_t)-(r_{t+1}+\gamma \max_{a \in \mathbb{A}(s_{t+1})}q_{t}(s_{t+1},a_{t+1}))]}  , \end{equation}</script><script type="math/tex; mode=display">
\begin{equation}
\textcolor{blue}{q_{t+1}(s,a) =q_{t}(s,a) \quad \text{for  all} \quad (s,a) \neq (s_t,a_t)} \end{equation}</script><p>where $t = 0,1,2,\dots$ denotes the time step. Here, $q_t(s_t,a_t)$ is the estimate the optimal action value of (s_t,a_t) ; $\alpha_t(s_t,a_t)$ is the learning rate for $(s_t,a_t)$ at time $t$.</p>
<p>Q-learning is a stochastic approximation algorithm for solving the Bellman optimality equation expressed in terms of action values.</p>
<script type="math/tex; mode=display">
q_{\pi}(s,a) =\textcolor{blue}{\mathbb{E}[R_{t+1}+\gamma\max_{a} q(S_{t+1},a)\mid S_t=s,A_t=a], \quad \text{for all} \quad (s,a)}</script><p> Sarsa requires $(r_{t+1}, s_{t+1}, a_{t+1})$ in every iteration, whereas Q-learning merely requires $(r_{t+1}, s_{t+1})$.</p>
<h4 id="7-5-2-Off-policy-and-on-policy"><a href="#7-5-2-Off-policy-and-on-policy" class="headerlink" title="7.5.2 Off-policy and on-policy"></a>7.5.2 Off-policy and on-policy</h4><p>Q-learning slightly special compared to the other TD algorithms is that Q-learning is off-policy while the others are on-policy.<br>Two policies exist in any reinforcement learning task: a <em>behavior policy</em> and a <em>target policy</em>. </p>
<ul>
<li><strong>The behavior policy</strong> is the one used to generate experience samples.</li>
<li><strong>The target policy</strong> is the one that is constantly updated to converge to an optimal policy. </li>
</ul>
<p>When the behavior policy is the same as the target policy, such a learning process is called on-policy. Otherwise, when they are different, the learning process is called off-policy.</p>
<p>The advantage of off-policy learning is that it can learn optimal policies based on the experience samples generated by other policies, which may be, for example, a policy executed by a human operator. As an important case, <strong>the behavior policy can be selected to be exploratory.</strong> For example, if we would like to estimate the action values of all state action pairs, we must generate episodes visiting every state-action pair sufficiently many times. Although Sarsa uses $\epsilon$-greedy policies to maintain certain exploration abilities, the value of is usually small and hence <strong>the exploration ability is limited</strong>. By contrast, <u>if we can use a policy with a strong exploration ability to generate episodes and then use off-policy learning to learn optimal policies, the learning efficiency would be significantly increased.</u></p>
<p><strong>Sarsa is on-policy.</strong><br>we need samples generated by $\pi$. Therefore, $\pi$ is the behavior policy. The second step is to obtain an improved policy based on the estimated values of $\pi$. As a result, $\pi$ is the target policy that is constantly updated and eventually converges to an optimal policy. Therefore, the behavior policy and the target policy are the same.</p>
<p><strong>Monte Carlo learning is on-policy.</strong><br>A policy is used to generate samples, which is further used to estimate the action values of the policy. Based on the action values, we can improve the policy.</p>
<p><strong>Q-learning is off-policy.</strong><br>The fundamental reason is that Q-learning is an algorithm for solving the Bellman optimality equation that can directly generate the optimal values and optimal policies.we need not to evaluate the associated policy.<br>Q-learning experience sample requires $(s_t, a_t, r_{t+1},s_{t+1})$. If $(s_t, a_t)$ is given, then $r_{t+1}$ and $s_{t+1}$ do not depend on any policy!<br>The behavior policy to generate at from $s_t$ can be anything. The target policy will converge to the optimal policy.<br><strong>Note</strong>:  on-policy/off-policy is online/offline learning?<br>Online learning refers to the case where the value and the policy are updated once an experience sample is obtained. Offline learning refers to the case where the update can only be done after all experience samples have been collected(episodes).</p>
<h4 id="7-5-3-Implementation-and-Illustrative-examples"><a href="#7-5-3-Implementation-and-Illustrative-examples" class="headerlink" title="7.5.3 Implementation and Illustrative examples"></a>7.5.3 Implementation and Illustrative examples</h4><p><img src="/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-05-22-19-40-40.png" alt=""></p>
<p><img src="/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-05-22-19-41-00.png" alt=""><br>The on-policy version of Q-learning is shown in Algorithm 7.2. This implementation is similar to the Sarsa one in Algorithm 7.1. Here, the behavior policy is the same as the target policy, which is an-greedy policy.</p>
<p>The off-policy version is shown in Algorithm 7.3. The behavior policy $\pi_b$ can be any policy as long as it can generate sufficient experience samples. <strong>It is usually favorable when $\pi_b$ is exploratory</strong>. Here, the target policy $\pi_T$ is greedy rather than $\epsilon$-greedy <strong>since it is not used to generate samples and hence is not required to be exploratory.</strong> Moreover, the off-policy version of Q-learning presented here is implemented offline: all the experience samples are collected first and then processed. It can be modified to become online: the value and policy can be updated immediately once a sample is received.</p>
<p><img src="/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-05-22-19-49-51.png" alt=""><br><img src="/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-05-22-19-50-11.png" alt=""><br><strong>Ground truth</strong>: To verify the effectiveness of Q-learning, we first need to know the ground truth of the optimal policies and optimal state values. Here, the ground truth is obtained by the model-based policy iteration algorithm. The ground truth is given in Figures 7.4(a) and (b).</p>
<p><strong>Experience samples</strong>: The behavior policy has a uniform distribution: the probability of taking any action at any state is 0.2 (Figure 7.4(c)). A single episode with 100,000 steps is generated (Figure 7.4(d)). Due to the good exploration ability of the behavior policy, the episode visits every state-action pair many times.</p>
<p><strong>Learned results</strong>: Based on the episode generated by the behavior policy, the final target policy learned by Q-learning is shown in Figure 7.4(e). This policy is optimal because the estimated state value error (root-mean-square error) converges to zero as shown in Figure 7.4(f). In addition, one may notice that the learned optimal policy is not exactly the same as that in Figure 7.4(a). In fact, there exist multiple optimal policies that have the same optimal state values.</p>
<p><strong>Different initial values</strong>: Since Q-learning bootstraps, the performance of the algorithm depends on the initial guess for the action values. As shown in Figure 7.4(g), <u>when the initial guess is close to the true value, the estimate converges within approximately 10,000 steps.</u> Otherwise, the convergence requires more steps (Figure 7.4(h)). Nevertheless, these figures demonstrate that Q-learning can still converge rapidly even though the initial value is not accurate.</p>
<p><strong>Different behavior policies</strong>: <u>When the behavior policy is not exploratory, the learning performance drops significantly.</u> For example, consider the behavior policies shown in Figure 7.5. They are $\epsilon$-greedy policies with = 0.5 or 0.1 (the uniform policy in Figure 7.4(c) can be viewed as $\epsilon$-greedy with = 1). It is shown that, when decreases from 1 to 0.5 and then to 0.1, the learning speed drops significantly. That is because the exploration ability of the policy is weak and hence the experience samples are insufficient.</p>
<p><img src="/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-05-22-19-55-35.png" alt=""></p>
<p><img src="/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-05-22-19-55-45.png" alt=""></p>
<h3 id="7-6-A-unified-viewpoint"><a href="#7-6-A-unified-viewpoint" class="headerlink" title="7.6 A unified viewpoint"></a>7.6 A unified viewpoint</h3><p>In this section, we introduce a uni ed framework to accommodate all these<br> algorithms and MC learning.<br>In particular, the TD algorithms (for action value estimation) can be expressed in a unified expression:</p>
<script type="math/tex; mode=display">
\begin{equation}
\textcolor{blue}{q_{t+1}(s_t,a_t) =q_{t}(s_t,a_t)-\alpha_t(s_t,a_t)[q_{t}(s_t,a_t)-\bar{q_t}]}  , \end{equation}</script><p>where $\bar{q_t}$ is the TD target. Different TD algorithms have different $\bar{q_t}$.<br><img src="/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-05-22-20-00-27.png" alt=""><br>As can be seen, all of the algorithms aim to solve the Bellman equation except Q-learning, which aims to solve the Bellman optimality equation.</p>
<h3 id="7-7-Summary"><a href="#7-7-Summary" class="headerlink" title="7.7 Summary"></a>7.7 Summary</h3><p>we introduced include Sarsa, n-step Sarsa, and Q-learning. All these algorithms can be viewed as stochastic approximation algorithms for solving Bellman or Bellman optimality equations.<br>The TD algorithms introduced in this chapter, except Q-learning, are used to evaluate a given policy. That is to estimate a given policy’s state/action values from some experience samples. Together with policy improvement, they can be used to learn optimal policies. Moreover, these algorithms are on-policy: the target policy is used as the behavior policy to generate experience samples.<br>Q-learning is slightly special compared to the other TD algorithms in the sense that it is o-policy. The target policy can be different from the behavior policy in Q-learning. The fundamental reason why Q-learning is o-policy is that Q-learning aims to solve the Bellman optimality equation rather than the Bellman equation of a given policy.<br>It is worth mentioning that there are some methods that can convert an on-policy algorithm to be off-policy. Importance sampling is a widely used one and will be introduced in Chapter 10. Finally, there are some variants and extensions of the TD algorithms introduced in this chapter <sup><a href="#fn_5" id="reffn_5">5</a></sup>. For example, the $TD(\lambda)$ method provides a more general and uni ed framework for TD learning. More information can be found in paper<sup><a href="#fn_6" id="reffn_6">6</a></sup>.</p>
<p><sup><a href="#fn_5" id="reffn_5">5</a></sup>:H. Van Hasselt, A. Guez, and D. Silver, Deep reinforcement learning with double Q-learning, in AAAI Conference on Artificial Intelligence, vol. 30, 2016.<br><sup><a href="#fn_6" id="reffn_6">6</a></sup>:R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction (2nd Edition). MIT Press, 2018.</p>
<p>Q: What does the term learning in TD learning mean?<br> A: From a mathematical point of view, learning simply means estimation . That is to estimate state/action values from some samples and then obtain policies based on the estimated values.</p>
<p>Q: Why does Sarsa update policies to be-greedy?<br>A: That is because the policy is also used to generate samples for value estimation. Hence, it should be exploratory to generate sufficient experience samples.</p>
<p>Q: While Theorems 7.1 and 7.2 require that the learning rate t converges to zero gradually, why is it often set to be a small constant in practice?<br>A: The fundamental reason is that the policy to be evaluated keeps changing (or called nonstationary). In particular, a TD learning algorithm like Sarsa aims to estimate the action values of a given policy. If the policy is fixed, using a decaying learning rate is acceptable. However, in the optimal policy learning process, the policy that Sarsa aims to evaluate keeps changing after every iteration. We need a constant learning rate in this case; otherwise, a decaying learning rate may be too small to effectively evaluate policies. Although a drawback of constant learning rates is that the value estimate may fluctuate eventually, the fluctuation is negatable as long as the constant learning rate is sufficiently small.</p>
<p>Q: Why does the o-policy version of Q-learning update policies to be greedy instead of $\epsilon$-greedy?<br>A: That is because the target policy is not required to generate experience samples. Hence, it is not required to be exploratory.</p>
<h2 id="Approximation-Q-learning-Steps"><a href="#Approximation-Q-learning-Steps" class="headerlink" title="Approximation (Q-learning Steps)"></a>Approximation (Q-learning Steps)</h2><p>The Q-learning algorithm uses a Q-table of State-Action Values (also called Q-values). This Q-table has a row for each state and a column for each action. Each cell contains the estimated Q-value for the corresponding state-action pair.<br>Steps:</p>
<p>1.We start by initializing all the Q-values to zero. As the agent interacts with the environment and gets feedback, the algorithm iteratively improves these Q-values until they converge to the Optimal Q-values. It updates them using the Bellman equation.</p>
<p>2.Then we use the ε-greedy policy to pick the current action from the current state.</p>
<p>3.Execute this action tointhe environment to execute, and gets feedback in the form of a reward and the next state.</p>
<p>4.Now, for step #4, the algorithm has to use a Q-value from the next state in order to update its estimated Q-value (Q1) for the current state and selected action.</p>
<p>And here is where the Q-Learning algorithm uses its clever trick. The next state has several actions, so which Q-value does it use? It uses the action (a4) from the next state which has the highest Q-value (Q4). What is critical to note is that it treats this action as a target action to be used only for the update to Q1. It is not necessarily the action that it will actually end up executing from the next state when it reaches the next time step. </p>
<script type="math/tex; mode=display">Q(s_t, a_t) \gets (1-\alpha)Q(s_t, a_t) + \alpha \left( r_t + (1-d_t) \gamma \max_{a_{t+1}} Q(s_{t+1}, a_{t+1}) \right)</script><p> Here $d_t$ is the done flag. If $d_t=1$, then it means the state $s_{t+1}$ is a terminal state. Adding the $\left(1-d_t\right)$ term implies that the discounted return $G_t$ is just going to be $r_t$, since all rewards after that are going to be 0 . In some situations this can help accelerate learning.</p>
<p>In other words, there are two actions involved:</p>
<ul>
<li>Current action — the action from the current state that is actually executed in the environment, and whose Q-value is updated.</li>
<li>Target action — has the highest Q-value from the next state, and used to update the current action’s Q value.</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>集成调度数据集</title>
    <url>/2022/09/16/blog1/</url>
    <content><![CDATA[<h1 id="1-配送-121TTT"><a href="#1-配送-121TTT" class="headerlink" title="1 配送 121TTT"></a>1 配送 121TTT</h1><h2 id="1-1-配送集成ADSAS1"><a href="#1-1-配送集成ADSAS1" class="headerlink" title="1.1 配送集成ADSAS1"></a>1.1 配送集成ADSAS1</h2><p> 简介：生产与配送集成调度是近些年来热门的研究领域。大多数学者认为 <a href="https://sci-hub.wf/10.1287/opre.28.6.1436">(Potts, 1980)</a>是该领域的开篇之作。随着该领域专家学者的不断拓展和深入，集成调度的相关研究得到了空前的发展。各方面的理论和方法也不断被提出，可参照综述文献 <a href="https://sci-hub.wf/10.1287/opre.1080.0688">(Chen, 2010)</a>,<a href="https://sci-hub.wf/10.1016/j.cie.2016.12.010">(Moons et al., 2017)</a>, <a href="https://sci-hub.wf/10.1080/00207543.2020.1762019">(Kumar et al., 2020)</a>进行系统的梳理。</p>
<blockquote>
<p>根据问题特征我们对如下数据集进行简要描述：</p>
<ul>
<li>生产调度：(作业车间调度、流水车间调度（流水线、置换、混流）、分布式车间调度、柔性调度、模糊调度、动态调度 等)</li>
<li>配送调度：(运输问题、车辆路径问题（容量限制、时间窗约束、多行程等））<br>我们所提出的数据集所应用的模型涉及：<br><strong>流水线生产 &amp; 车辆路径问题（考虑容量限制、时间窗约束、多行程）&amp;模糊旅途时间</strong> </li>
</ul>
<hr>
</blockquote>
<figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">var _prevOnload = window.onload;</span><br><span class="line"></span><br><span class="line">window.onload = function () {</span><br><span class="line">    var switchLang = document.getElementsByClassName("menu-item menu-item-switch-to-chinese")[0];</span><br><span class="line">    switchLang.onclick = function () {</span><br><span class="line">        var href = window.location.href;</span><br><span class="line">        var includesKeywords = href.includes("/homepage/") || href.includes("/home/")|| href.includes("/publications/") || href.includes("/resources/")|| href.includes("/link/")|| href.includes("/archives/")|| href.includes("/Switch to Chinese/");</span><br><span class="line">        if (includesKeywords) {</span><br><span class="line">            window.location.href = href.replace('.cn/', '.cn/cn/');</span><br><span class="line"></span><br><span class="line">        }</span><br><span class="line">        else {</span><br><span class="line">            window.location.href = "https://www.huangm.cn/cn";</span><br><span class="line">        }</span><br><span class="line">        if (typeof (_prevOnload) === 'function') {</span><br><span class="line">            _prevOnload();</span><br><span class="line">        }</span><br><span class="line">        return false;</span><br><span class="line">    }</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>
<h2 id="1-1-配送集成ADSAS1-1"><a href="#1-1-配送集成ADSAS1-1" class="headerlink" title="1.1 配送集成ADSAS1"></a>1.1 配送集成ADSAS1</h2><script type="math/tex; mode=display">
\begin{equation}
 \textcolor{red} {w_{k+1}=w_k-a_k \tilde{g}\left(w_k, \eta_k\right) ,k=1,2,3,\dots}
\end{equation}</script><h3 id="1-1-配送集成ADSAS1-2"><a href="#1-1-配送集成ADSAS1-2" class="headerlink" title="1.1 配送集成ADSAS1"></a>1.1 配送集成ADSAS1</h3><p>This simple example can illustrate why the RM algorithm converges.</p>
<h3 id="1-4配送集成"><a href="#1-4配送集成" class="headerlink" title="1.4配送集成"></a>1.4配送集成</h3><ul>
<li>When $w_k&gt;w^<em>$, we have $g\left(w_k\right)&gt;0$. Then, $w_{k+1}=w_k-a_k g\left(w_k\right)&lt;w_k$. If $a_k g\left(w_k\right)$ is sufficiently small, we have $w^</em>&lt;w_{k+1}&lt;w_k$. As a result, $w_{k+1}$ is closer to $w^*$ than $w_k$.</li>
<li>When $w_k<w^*$, we="" have="" $g\left(w_k\right)<0$.="" then,="" $w_{k+1}="w_k-a_k" g\left(w_k\right)="">w_k$. If $\left|a_k g\left(w_k\right)\right|$ is sufficiently small, we have $w^<em>&gt;w_{k+1}&gt;w_k$. As a result, $w_{k+1}$ is closer to $w^</em>$ than $w_k$.</w^*$,></li>
</ul>
<script type="math/tex; mode=display">
\begin{aligned} 
\tilde{g}(w, \eta) & = w-x \\ 
& =w-x+\mathbb{E}[X] -\mathbb{E}[X] \\
& =(w-\mathbb{E}[X]) + (\mathbb{E}[X]-x)\\
& =g(w)+\eta
\end{aligned}</script><script type="math/tex; mode=display">
\begin{equation}
w_{k+1} = w_k-\alpha_k \textcolor{blue} {\nabla_w f\left(w_k, x_k\right)}, \end{equation}</script><p>where $x_k$ is the sample collected at time step $k$. It relies on stochastic samples ${x_k}$</p>
<ol>
<li><p>Compared to the gradient descent algorithm: Replace the true gradient $\mathbb{E}\left[\nabla_w f\left(w_k, X\right)\right]$ by the stochastic gradient $\nabla_w f\left(w_k, x_k\right)$. </p>
</li>
<li><p>Compared to the batch gradient descent method: let $n=1$.</p>
</li>
<li><p><strong>From GD to SGD</strong>: The stochastic gradient $\nabla_w f\left(w_k, x_k\right)$ can be viewed as a noisy measurement of the true gradient $\mathbb{E}\left[\nabla_w f(w, X)\right]$: </p>
</li>
</ol>
<script type="math/tex; mode=display">
\nabla_w f\left(w_k, x_k\right)=\mathbb{E}\left[\nabla_w f(w, X)\right]+\underbrace{\nabla_w f\left(w_k, x_k\right)-\mathbb{E}\left[\nabla_w f(w, X)\right]}_\eta .</script><script type="math/tex; mode=display">
\begin{aligned} 
\mathbb{E}(\eta) & =\mathbb{E}(\nabla_w f\left(w_k, x_k\right)-\mathbb{E}\left[\nabla_w f(w, X)\right])\\
& =\mathbb{E}_{x_k}[\nabla_w f\left(w_k, x_k\right)]-\mathbb{E}_X\left[\nabla_w f(w, X)\right]=0
\end{aligned}</script><hr>
]]></content>
  </entry>
  <entry>
    <title>如何优雅的使用 Word 进行论文书写（初级版）</title>
    <url>/2023/12/03/%E5%A6%82%E4%BD%95%E4%BC%98%E9%9B%85%E7%9A%84%E4%BD%BF%E7%94%A8-Word-%E8%BF%9B%E8%A1%8C%E8%AE%BA%E6%96%87%E4%B9%A6%E5%86%99%EF%BC%88%E5%88%9D%E7%BA%A7%E7%89%88%EF%BC%89/</url>
    <content><![CDATA[<h1 id="1-多级标题的设置"><a href="#1-多级标题的设置" class="headerlink" title="1. 多级标题的设置"></a>1. 多级标题的设置</h1><blockquote>
<p>构建一个视觉舒服且直观的大纲列表对于开始论文的书写显得格外的重要。而word默认给出的标题样式模板对于书写论文来说太疏松了。本节主要介绍如何自定义一套适合自己的word标题模板。<br>如果对各级标题进行逐个格式化，并为其手动添加序号（如1.1 |第1.1章|第一章 等）就显得不够优雅。有没有一种方式能让我们就像书写markdown一样更加的丝滑那？</p>
</blockquote>
<p>首先，我们新建一个word 对 <code>开始</code>→<code>样式</code>中的我们所预想的格式、字体大小、段落间距等进行预设。<br><strong>主要涉及正文以及标题、1，2，3级标题</strong></p>
<div align="center"><img src="/2023/12/03/%E5%A6%82%E4%BD%95%E4%BC%98%E9%9B%85%E7%9A%84%E4%BD%BF%E7%94%A8-Word-%E8%BF%9B%E8%A1%8C%E8%AE%BA%E6%96%87%E4%B9%A6%E5%86%99%EF%BC%88%E5%88%9D%E7%BA%A7%E7%89%88%EF%BC%89/titletype.jpg" width="80%" height="50%"></div>


<p><strong>注意</strong> :记着更改各级标题时,每个都要勾选基于该模板的新文档，这样下次新建 word 时直接使用已经更改好的模板即可。</p>
<p><strong>然后就是为各级标题添加序号</strong></p>
<p>进入：<code>开始</code>→<code>多级序号</code>→<code>定义新的多级列表</code> 按需求进行设置即可:</p>
<div align="center"><img src="/2023/12/03/%E5%A6%82%E4%BD%95%E4%BC%98%E9%9B%85%E7%9A%84%E4%BD%BF%E7%94%A8-Word-%E8%BF%9B%E8%A1%8C%E8%AE%BA%E6%96%87%E4%B9%A6%E5%86%99%EF%BC%88%E5%88%9D%E7%BA%A7%E7%89%88%EF%BC%89/title.NO.jpg" width="80%" height="50%"></div>

<p><strong>注意</strong> ：为各级标题添加了序号后，实际是把标题样式进行了更新，所以完成编号后需要再次为每个标题进行勾选基于该模板的新文档，不需要改动别的什么直接按  <code>确定</code> 即可，这样下次新建 word 时直接使用已经更改好的带编号的模板了。</p>
<hr>
<h1 id="2-图、表的插入与索引"><a href="#2-图、表的插入与索引" class="headerlink" title="2.图、表的插入与索引"></a>2.图、表的插入与索引</h1><h2 id="2-1-图的题注插入"><a href="#2-1-图的题注插入" class="headerlink" title="2.1 图的题注插入"></a>2.1 图的题注插入</h2><p><strong><em>这一步的前提是 你的标题编号是按上述形式插入的,而非手动写的。这样 word 才能识别你的编号从而插入题注里</em></strong><br>插入图片后  一般选用嵌入式（默认） 右键后选自 <code>插入题注</code> 比如我们想要 <code>图 1.1</code> 新建标签 <code>图</code> 选择编号的格式。示例如下：</p>
<div align="center"><img src="/2023/12/03/%E5%A6%82%E4%BD%95%E4%BC%98%E9%9B%85%E7%9A%84%E4%BD%BF%E7%94%A8-Word-%E8%BF%9B%E8%A1%8C%E8%AE%BA%E6%96%87%E4%B9%A6%E5%86%99%EF%BC%88%E5%88%9D%E7%BA%A7%E7%89%88%EF%BC%89/Fig.jpg  " width="60%" height="50%"></div>

<p>设置好后  在为接下来的插入的图进行题注时 word 会自动识别其应该是<strong>图5还是图6</strong>。无需你在考虑图几，你仅需要输入图的名字即可。</p>
<h2 id="2-2-三线表的插入"><a href="#2-2-三线表的插入" class="headerlink" title="2.2 三线表的插入"></a>2.2 三线表的插入</h2><p>首选插入一个表 然后再 <code>表格样式里选择新建表格样式</code>然后进行如下操作 </p>
<div align="center"><img src="/2023/12/03/%E5%A6%82%E4%BD%95%E4%BC%98%E9%9B%85%E7%9A%84%E4%BD%BF%E7%94%A8-Word-%E8%BF%9B%E8%A1%8C%E8%AE%BA%E6%96%87%E4%B9%A6%E5%86%99%EF%BC%88%E5%88%9D%E7%BA%A7%E7%89%88%EF%BC%89/table.jpg" width="60%" height="50%"></div>

<p><strong>注意：</strong> <em>记着要勾选基于该模板的新文档，这样下次新建时直接使用已经更改好的三线表了。<br>如果更改后发现没有找到 保存并退出 word 然后重新打开即可看到三线表出现在表格样式里。同样的方法进行表的题注的插入</em></p>
<h2 id="2-3-正文中索引图、表"><a href="#2-3-正文中索引图、表" class="headerlink" title="2.3 正文中索引图、表"></a>2.3 正文中索引图、表</h2><p>选择要插入的位置，点击功能栏 <code>插入下的交叉引用</code> 可进行如图操作</p>
<div align="center"><img src="/2023/12/03/%E5%A6%82%E4%BD%95%E4%BC%98%E9%9B%85%E7%9A%84%E4%BD%BF%E7%94%A8-Word-%E8%BF%9B%E8%A1%8C%E8%AE%BA%E6%96%87%E4%B9%A6%E5%86%99%EF%BC%88%E5%88%9D%E7%BA%A7%E7%89%88%EF%BC%89/index.jpg" width="60%" height="50%"></div>

<p> <strong>注：</strong> <em>当我们插入新图后，图的序号发生改变但是插入正文中的索引未变 ，不要慌 此时我能只需要更新插入引用的域即可 全选后按F9 更新全文的域</em></p>
<hr>
<h1 id="3-MathType-的编号、索引"><a href="#3-MathType-的编号、索引" class="headerlink" title="3. MathType 的编号、索引"></a>3. MathType 的编号、索引</h1><p>目前仅可以逐项公式的编号，即点击 <code>MathType下的右编号</code>在弹出的编辑器里输入公式 会自动的公式居中以及右侧编号。(目前还没有找到为已存在的公式进行编号的方法)</p>
<p>编号的格式可以在 <code>插入编号--格式化</code>里进行更改，选用带章节不带章节的格式以及采用何种连字符等<br>正文索引是也较为方便 点击 <code>插入引用后双击公式编号即可</code>  当你删除公式时编号会自动的更新。</p>
<p>公式的批量大小设置 在编辑器对公式的大小进行定义后</p>
<div align="center"><img src="/2023/12/03/%E5%A6%82%E4%BD%95%E4%BC%98%E9%9B%85%E7%9A%84%E4%BD%BF%E7%94%A8-Word-%E8%BF%9B%E8%A1%8C%E8%AE%BA%E6%96%87%E4%B9%A6%E5%86%99%EF%BC%88%E5%88%9D%E7%BA%A7%E7%89%88%EF%BC%89/mathtypesize.jpg" width="70%" height="50%"></div>
<div align="center"><img src="/2023/12/03/%E5%A6%82%E4%BD%95%E4%BC%98%E9%9B%85%E7%9A%84%E4%BD%BF%E7%94%A8-Word-%E8%BF%9B%E8%A1%8C%E8%AE%BA%E6%96%87%E4%B9%A6%E5%86%99%EF%BC%88%E5%88%9D%E7%BA%A7%E7%89%88%EF%BC%89/mathtypesize2.jpg" width="70%" height="50%"></div>


<h1 id="4-EndNote-的使用（中英文献混排）"><a href="#4-EndNote-的使用（中英文献混排）" class="headerlink" title="4. EndNote 的使用（中英文献混排）"></a>4. EndNote 的使用（中英文献混排）</h1><ol>
<li><p>去除endnotes 中的域 （不让其跳转，使之固定以便修改参考文献格式）<br>去除word中因使用mendeley或endnote等文献管理软件而产生的域代码。<br>%CTRL+A 全选文章；<br>%CTRL+6 去除全文域代码。</p>
</li>
<li><p>使用：CTRL+SHIFT+F9（Fn+F9）：去除选中的单个域代码，注不能跨页，有时候多选时也可能不生效，所以当参考文献留一篇索引版  ，然后边选中边去除域，去除完毕后 复制一份参考文献，然后更新参考目录  应为空才算去除完全，把刚刚复制的粘贴上，修改格式。</p>
</li>
</ol>
<p><strong>注意:</strong> 中途不能更新参考目录  ，如果更新了会将未去除域的参考文献索引重新编排。</p>
<p><strong>参考源</strong><br><a href="https://www.bilibili.com/video/BV1E5411K7By/?spm_id_from=333.788&amp;vd_source=49b8bd33ca1c49c048c6fc297b7c91d2">1.优雅搞定Word论文排版—b站视频</a></p>
<p><a href="https://www.bilibili.com/video/BV1rr4y1T71y?spm_id_from=333.999.0.0&amp;vd_source=49b8bd33ca1c49c048c6fc297b7c91d2">2.EndNote中英文文献混排格式设置—b站视频</a></p>
]]></content>
  </entry>
  <entry>
    <title>Python Basic Notes</title>
    <url>/2024/07/13/Python-Basic-Notes/</url>
    <content><![CDATA[<h1 id="PYTHON-基础"><a href="#PYTHON-基础" class="headerlink" title="PYTHON 基础"></a>PYTHON 基础</h1><h2 id="1-python-流程格式"><a href="#1-python-流程格式" class="headerlink" title="1. python 流程格式"></a>1. python 流程格式</h2><h3 id="1-1-变量赋值、运算与字符串表示、处理、判断"><a href="#1-1-变量赋值、运算与字符串表示、处理、判断" class="headerlink" title="1.1 变量赋值、运算与字符串表示、处理、判断"></a>1.1 变量赋值、运算与字符串表示、处理、判断</h3><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">name1 = name2 = name3 = <span class="string">'文件系统'</span></span><br><span class="line">name1, name2, name3 = <span class="string">"文件"</span>, <span class="string">"系统"</span>, <span class="string">"管理"</span></span><br><span class="line">name = <span class="string">""" </span></span><br><span class="line"><span class="string">ni</span></span><br><span class="line"><span class="string">hao</span></span><br><span class="line"><span class="string">a</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">##变量多行用 """....."""</span></span><br></pre></td></tr></tbody></table></figure>
<p><strong>变量运算表</strong><br>|  运算   |             描述             |<br>| :——-: | :—————————————: |<br>| + - <em> / | 加减乘除 (+可用于连接string) |<br>|    %    |      取模（102%100=2）       |<br>|   <em>*    |       幂次方（2</em></em>3=8）       |<br>|   //    |      取除数（20//3=6）       |</p>
<p><strong>字符串输出表</strong></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">方式</th>
<th style="text-align:center">意思</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">:,</td>
<td style="text-align:center">每 3 个 0 就用逗号隔开，比如 1,000</td>
</tr>
<tr>
<td style="text-align:center">:b</td>
<td style="text-align:center">该数字的二进制</td>
</tr>
<tr>
<td style="text-align:center">:d</td>
<td style="text-align:center">整数型</td>
</tr>
<tr>
<td style="text-align:center">:f</td>
<td style="text-align:center">小数模式</td>
</tr>
<tr>
<td style="text-align:center">:%</td>
<td style="text-align:center">百分比模式</td>
</tr>
</tbody>
</table>
</div>
<p><strong>1 字符串表示 ( %百分号模式 与 format功能 与 f格式化字符串)</strong><br></p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">name = <span class="string">"hm"</span></span><br><span class="line">age = <span class="number">24</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"我的名字是 %s !我 %d 岁了"</span> % (name, age))   <span class="comment">#百分号模式</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"我的名字是 {} !我 {} 岁了"</span>.<span class="built_in">format</span>(name, age))<span class="comment">#format功能</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"我的名字是 <span class="subst">{name}</span> !我 <span class="subst">{age}</span> 岁了"</span>)   <span class="comment">#f 格式化</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"我 {:.3f} 米高"</span>.<span class="built_in">format</span>(<span class="number">1.12345</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"我 {ht:.1f} 米高"</span>.<span class="built_in">format</span>(ht=<span class="number">1.12345</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"我 {:3d} 米高"</span>.<span class="built_in">format</span>(<span class="number">1</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"我 {:3d} 米高"</span>.<span class="built_in">format</span>(<span class="number">21</span>))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"You scored {:.2%}"</span>.<span class="built_in">format</span>(<span class="number">2.1234</span>))</span><br><span class="line">score = <span class="number">2.1234</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"You scored <span class="subst">{score:<span class="number">.2</span>%}</span>"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"You scored <span class="subst">{<span class="number">12</span>:5d}</span>"</span>)</span><br><span class="line"><span class="comment">#我的名字是 hm !我 24 岁了</span></span><br><span class="line"><span class="comment">#我 1.123 米高</span></span><br><span class="line"><span class="comment">#我 1.1 米高</span></span><br><span class="line"><span class="comment">#我   1 米高</span></span><br><span class="line"><span class="comment">#我  21 米高</span></span><br><span class="line"><span class="comment">#You scored 212.34%</span></span><br><span class="line"><span class="comment">#You scored 212.34%</span></span><br><span class="line"><span class="comment">#You scored    12</span></span><br></pre></td></tr></tbody></table></figure><br><strong>2 字符串处理与判断</strong><p></p>
<p>string.strip()            去除两端的空白符<br>string.replace(“原始”,”新的”)    替换字符<br>string.lower()          全部做小写处理<br>string.upper()          全部做大写处理<br>string.title()          仅开头的字母大写<br>string.split(“|”)        按要求分割以|进行分割<br>“,”.join([])            按要求合并中间添加,号<br>string.startswith(“你”)    判断是否为你字段开头<br>string.endswith(“哈哈”)    判断是否为哈哈字段结尾<br>&nbsp;</p>
<h3 id="1-2-条件判断、循环、跳出跳过（if-for-while-break-continue）"><a href="#1-2-条件判断、循环、跳出跳过（if-for-while-break-continue）" class="headerlink" title="1.2 条件判断、循环、跳出跳过（if/for/while/break/continue）"></a>1.2 条件判断、循环、跳出跳过（if/for/while/break/continue）</h3><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">## IF</span></span><br><span class="line"><span class="comment">#  冒号":"是必要格式不要遗失</span></span><br><span class="line"><span class="keyword">if</span> a==b :</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'ds'</span>)</span><br><span class="line"><span class="keyword">elif</span>(a&gt;=b):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'es'</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'ys'</span>)</span><br><span class="line"></span><br><span class="line">a = <span class="number">1</span> </span><br><span class="line">b=<span class="number">2</span></span><br><span class="line">c = b <span class="keyword">if</span> a&gt;=b <span class="keyword">else</span> a  <span class="comment">#简单的if-else 可以写成一行的简写模式</span></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">#  python 从0开始 区间取值均为前闭后开:[3,10) 基础</span></span><br><span class="line"><span class="comment"># for 循环处理有明确长度的序列，那么你第一个想到的应该是 。 </span></span><br><span class="line"><span class="comment"># while 循环无限长序列或者运行无限次数,用条件来限制他的循环次数。</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>,<span class="number">10</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"文件"</span>+ <span class="built_in">str</span>(i)) <span class="comment"># int型与string型不能直接连接需转换int型为string型(文件1)</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"文件"</span>, <span class="built_in">str</span>(i)) <span class="comment">#(文件 1) 中间有空格</span></span><br><span class="line"></span><br><span class="line">l = [i*<span class="number">2</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>,<span class="number">10</span>)] <span class="comment"># 简单的for 可以写成一行的简写模式</span></span><br><span class="line">l = [i*<span class="number">2</span> <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>,<span class="number">10</span>) <span class="keyword">if</span> i%<span class="number">2</span>==<span class="number">0</span>] <span class="comment"># 简单的for+if 可以写成一行的简写模式</span></span><br><span class="line">a=<span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> a != <span class="number">10</span>:</span><br><span class="line">    a += <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(a)</span><br><span class="line"></span><br><span class="line"><span class="comment">#  break:最主要的目的就是为了个性化的终止当前循环.</span></span><br><span class="line"><span class="comment">#  continue: 判断一个数据如果不能被处理的话，我就把它跳过，接着处理下一个数据。</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    <span class="keyword">if</span> i % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">continue</span>    <span class="comment"># 打印所以奇数，跳过偶数</span></span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">9</span>:</span><br><span class="line">        <span class="keyword">break</span>       <span class="comment"># 如果 i=9跳出循环。</span></span><br><span class="line">    <span class="built_in">print</span>(i)</span><br></pre></td></tr></tbody></table></figure>
<p>&nbsp;</p>
<h3 id="1-3-数据类型"><a href="#1-3-数据类型" class="headerlink" title="1.3 数据类型"></a>1.3 数据类型</h3><h4 id="1-3-1-List列表、Dict字典、Tuple元组、Set集合"><a href="#1-3-1-List列表、Dict字典、Tuple元组、Set集合" class="headerlink" title="1.3.1 List列表、Dict字典、Tuple元组、Set集合"></a>1.3.1 List列表、Dict字典、Tuple元组、Set集合</h4><p><strong>list “[]”</strong>—— 具有顺序、可存放不同类型；但是<br></p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">files = [<span class="number">1</span>, <span class="string">"file"</span>, [<span class="string">"2"</span>, <span class="number">3.2</span>],<span class="string">"ha"</span>]</span><br><span class="line"><span class="built_in">print</span>( files[<span class="number">2</span>:<span class="number">4</span>]) <span class="comment"># [["2", 3.2],"ha"]  取[2,3)</span></span><br><span class="line"><span class="built_in">print</span>( files[-<span class="number">3</span>:]) <span class="comment"># ["file", ["2", 3.2],"ha"] 从后往前取三个</span></span><br><span class="line"><span class="built_in">print</span>( files[<span class="number">2</span>][<span class="number">1</span>]) <span class="comment"># 3.2</span></span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> files:    </span><br><span class="line">    <span class="keyword">if</span> f == <span class="string">"f3.txt"</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"I got f3.txt"</span>)</span><br><span class="line">（OR）</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(files)):</span><br><span class="line">    <span class="keyword">if</span> files[i] == <span class="string">"f3.txt"</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"I got f3.txt"</span>)</span><br></pre></td></tr></tbody></table></figure><br><strong>Dict “{“key”:value}”</strong>—— 注意字典中的元素是<strong>没有顺序</strong>的; key-value型<br><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">files = {<span class="string">"ID"</span>: <span class="number">111</span>, <span class="string">"passport"</span>: <span class="string">"my passport"</span>, <span class="string">"books"</span>: [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]}</span><br><span class="line"><span class="built_in">print</span>(files[<span class="string">"books"</span>]) <span class="comment"># [1,2,3]</span></span><br></pre></td></tr></tbody></table></figure><br><strong>Tuple “()”</strong>—— 存放数据值不能被改变，常用来<strong>存放有些常数或固定值</strong>。特别是代码交由别人使用时，你没法控制，也不希望他们改变你的这些固定值。<br><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">files = (<span class="string">"file1"</span>, <span class="string">"file2"</span>, <span class="string">"file3"</span>)</span><br></pre></td></tr></tbody></table></figure><br><strong>Set “set([])”或”{}”</strong>—— Set常用于去重，set里面只会存在非重复的元素，不管你往里面加了多少相同元素，这些相同元素都会坍缩成一个。 这种特性，我们就可以运用它来做<strong>交集并集</strong>等操作。注意集合中的元素也<strong>没有顺序</strong>的。<br><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">my_files = <span class="built_in">set</span>([<span class="string">"file1"</span>, <span class="string">"file2"</span>, <span class="string">"file3"</span>])</span><br><span class="line">your_files = {<span class="string">"file1"</span>, <span class="string">"file3"</span>, <span class="string">"file5"</span>}</span><br><span class="line">my_files.add(<span class="string">"file4"</span>)      <span class="comment">#添加</span></span><br><span class="line">my_files.remove(<span class="string">"file3"</span>)   <span class="comment">#移除</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"交集 "</span>, your_files.intersection(my_files))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"并集 "</span>, your_files.union(my_files))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"补集 "</span>, your_files.difference(my_files))</span><br></pre></td></tr></tbody></table></figure><br><strong>list 内元素的增加与删除</strong><br><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">files = []</span><br><span class="line">files.append(<span class="string">"f"</span>+<span class="built_in">str</span>(<span class="number">1</span>)+<span class="string">".txt"</span>) <span class="comment"># 添加元素</span></span><br><span class="line"><span class="built_in">print</span>(files)</span><br><span class="line">files.pop()   <span class="comment"># 从最后一个开始 pop 出 删除元素</span></span><br><span class="line"><span class="built_in">print</span>( files)</span><br><span class="line"><span class="comment"># 扩充入另一个列表</span></span><br><span class="line">files.extend([<span class="string">"f3.txt"</span>, <span class="string">"f4.txt"</span>])</span><br><span class="line"><span class="comment"># 按位置添加</span></span><br><span class="line">files.insert(<span class="number">1</span>, <span class="string">"file5.txt"</span>)     <span class="comment"># 添加入第1位（首位是0哦）</span></span><br><span class="line"><span class="comment"># 移除某索引</span></span><br><span class="line"><span class="keyword">del</span> files[<span class="number">1</span>]</span><br><span class="line"><span class="comment"># 移除某值 </span></span><br><span class="line">files.remove(<span class="string">"f3.txt"</span>)  <span class="comment"># 当存在多个f3.txt时，从0开始移除第一个f3.txt元素</span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><br>&nbsp;<p></p>
<h4 id="1-3-2-arrly数组-——-Numpy库"><a href="#1-3-2-arrly数组-——-Numpy库" class="headerlink" title="1.3.2 arrly数组 —— Numpy库"></a>1.3.2 arrly数组 —— Numpy库</h4><p>&nbsp;</p>
<h4 id="1-3-3-arrly表-——-Panda库"><a href="#1-3-3-arrly表-——-Panda库" class="headerlink" title="1.3.3 arrly表  —— Panda库"></a>1.3.3 arrly表  —— Panda库</h4><p>&nbsp;</p>
<h3 id="1-4-一些函数的妙用"><a href="#1-4-一些函数的妙用" class="headerlink" title="1.4 一些函数的妙用"></a>1.4 一些函数的妙用</h3><h4 id="1-4-1enumerate-自动加-index"><a href="#1-4-1enumerate-自动加-index" class="headerlink" title="1.4.1enumerate 自动加 index"></a>1.4.1enumerate 自动加 index</h4><p>使用enumerate函数替换 手动计数器 count 的存在.<br></p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">count = <span class="number">0</span></span><br><span class="line">l = [<span class="number">11</span>,<span class="number">22</span>,<span class="number">33</span>,<span class="number">44</span>]</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> l:</span><br><span class="line">    <span class="keyword">if</span> count == <span class="number">2</span>:</span><br><span class="line">        data += <span class="number">11</span></span><br><span class="line">    l[count] = data</span><br><span class="line">    count += <span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(l)</span><br><span class="line"></span><br><span class="line">l = [<span class="number">11</span>,<span class="number">22</span>,<span class="number">33</span>,<span class="number">44</span>]</span><br><span class="line"><span class="keyword">for</span> count, data <span class="keyword">in</span> <span class="built_in">enumerate</span>(l,start=<span class="number">0</span>):  <span class="comment">#还可以初始化 count 开始的值</span></span><br><span class="line">    <span class="keyword">if</span> count == <span class="number">2</span>:</span><br><span class="line">        data += <span class="number">11</span></span><br><span class="line">    l[count] = data</span><br><span class="line"><span class="built_in">print</span>(l)</span><br><span class="line"><span class="comment">#&gt;[11, 22, 44, 44]</span></span><br></pre></td></tr></tbody></table></figure><p></p>
<h4 id="1-4-2-Zip-同时迭代"><a href="#1-4-2-Zip-同时迭代" class="headerlink" title="1.4.2 Zip 同时迭代"></a>1.4.2 Zip 同时迭代</h4><p>同时处理多个列表，并把它们做成一个字典。<br></p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">name = [<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>]</span><br><span class="line">score = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">bonus = [<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">d = {}</span><br><span class="line"><span class="keyword">for</span> n, s, b <span class="keyword">in</span> <span class="built_in">zip</span>(name, score, bonus):</span><br><span class="line">    d[n]=s+b</span><br><span class="line"><span class="built_in">print</span>(d)</span><br></pre></td></tr></tbody></table></figure><p></p>
<h4 id="1-4-3-reverse-amp-reversed-翻转列表"><a href="#1-4-3-reverse-amp-reversed-翻转列表" class="headerlink" title="1.4.3 reverse &amp; reversed 翻转列表"></a>1.4.3 reverse &amp; reversed 翻转列表</h4><p>同时处理多个列表，并把它们做成一个字典。<br></p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">#自己就地反转</span></span><br><span class="line">l = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">l.reverse()</span><br><span class="line"><span class="built_in">print</span>(l)</span><br><span class="line"><span class="comment">#&gt;[3, 2, 1]</span></span><br><span class="line">(<span class="keyword">or</span>)</span><br><span class="line"><span class="comment">#用在 for 循环里的翻转迭代器</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">reversed</span>(l):</span><br><span class="line">    <span class="built_in">print</span>(i)</span><br><span class="line"><span class="comment">#&gt;3 2 1</span></span><br><span class="line">(<span class="keyword">or</span>)</span><br><span class="line"><span class="comment">#copy 出一个浅拷贝副本的反转</span></span><br><span class="line">_l = l[::-<span class="number">1</span>]</span><br><span class="line"><span class="built_in">print</span>(_l)</span><br><span class="line"><span class="comment">#&gt;[3, 2, 1]</span></span><br></pre></td></tr></tbody></table></figure><p></p>
<h4 id="1-4-4-Deep-Copy-amp-Shallow-Copy-复制要点"><a href="#1-4-4-Deep-Copy-amp-Shallow-Copy-复制要点" class="headerlink" title="1.4.4 Deep Copy &amp; Shallow Copy 复制要点"></a>1.4.4 Deep Copy &amp; Shallow Copy 复制要点</h4><p>Deep copy 就是我们通常意义上的复制，把东西全部再造了一遍，彻底成为了两个独立的个体。<br>Shallow Copy, 其实也有一点地址拷贝的意思，通过地址进行映射。 所以真实的实体是没有被复制的，我只复制了这个实体的一个映射地址而已。<br>注：Python 他在创造之初，就有这么个约定，列表中直接存放的<strong>数值、字符</strong>，与存放<strong>class 实例，列表，字典</strong>不同。 对<strong>数值、字符</strong>的复制，直接是复制的值，而不是一个映射地址。<br>程序数据的无端更改其实很有可能就是浅复制造成的，改变了不该被改变的值。但是浅复制的优势就是快。复制是需要内存和时间的，因为浅复制没有真正复制。</p>
<h2 id=""><a href="#" class="headerlink" title=""></a><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">#存放的数值、字符时相当于Deep Copy</span></span><br><span class="line">l = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">_l = l.copy()</span><br><span class="line">_l[<span class="number">0</span>] = -<span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(_l)</span><br><span class="line"><span class="built_in">print</span>(l)</span><br><span class="line"><span class="comment">#&gt; [-1, 2, 3]</span></span><br><span class="line"><span class="comment">#&gt; [1, 2, 3]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#存放的列表时  .copy只是浅拷贝Shallow Copy</span></span><br><span class="line">l = [[<span class="number">1</span>],[<span class="number">2</span>],<span class="number">3</span>]</span><br><span class="line">_l = l.copy()</span><br><span class="line">_l[<span class="number">0</span>][<span class="number">0</span>] = -<span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(_l)</span><br><span class="line"><span class="built_in">print</span>(l)</span><br><span class="line"><span class="comment">#&gt; [[-1], [2], 3]</span></span><br><span class="line"><span class="comment">#&gt; [[-1], [2], 3]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#存放的列表时  如何深拷贝哪？</span></span><br><span class="line"><span class="keyword">from</span> copy <span class="keyword">import</span> deepcopy</span><br><span class="line">l = [[<span class="number">1</span>],[<span class="number">2</span>],<span class="number">3</span>]</span><br><span class="line">_l = deepcopy(l)</span><br><span class="line">_l[<span class="number">0</span>][<span class="number">0</span>] = -<span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(_l)</span><br><span class="line"><span class="built_in">print</span>(l)</span><br><span class="line"><span class="comment">#&gt; [[-1], [2], 3]</span></span><br><span class="line"><span class="comment">#&gt; [[1], [2], 3]</span></span><br></pre></td></tr></tbody></table></figure></h2><h2 id="2-python-框架格式"><a href="#2-python-框架格式" class="headerlink" title="2. python 框架格式"></a>2. python 框架格式</h2><h3 id="2-1-函数-Function-—-类-Class-—-模块-Module"><a href="#2-1-函数-Function-—-类-Class-—-模块-Module" class="headerlink" title="2.1 函数(Function) — 类 (Class) — 模块 (Module)"></a>2.1 函数(Function) — 类 (Class) — 模块 (Module)</h3><p><span class="github-emoji"><span>✅</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span><strong>函数功能</strong> ：目的用于基础功能复用，简化程序流程<br></p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">g=<span class="number">3</span></span><br><span class="line"><span class="comment"># 传入变量 其中a=1, b=1, c=1为参数默认值（可以不设置），</span></span><br><span class="line"><span class="comment"># 当未传入a,b,c参数时令其为默认。</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">f</span>(<span class="params">x, a=<span class="number">1</span>, b=<span class="number">1</span>, c=<span class="number">1</span></span>): </span><br><span class="line">    <span class="keyword">global</span> g    </span><br><span class="line">    <span class="comment">#默认时局部变量不影响函数外部，除非golbal声明，</span></span><br><span class="line">    <span class="comment">#但是要注意全局变量不能同时函数传入变量</span></span><br><span class="line">    g=<span class="number">5</span></span><br><span class="line">    <span class="keyword">return</span> a*x**<span class="number">2</span> + b*x + c*<span class="number">3</span>  <span class="comment"># 返回变量</span></span><br><span class="line"><span class="built_in">print</span>(f(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>))         <span class="comment"># 忽略参数名，按顺序传参</span></span><br><span class="line"><span class="built_in">print</span>(f(x=<span class="number">2</span>, a=<span class="number">1</span>, b=<span class="number">1</span>, c=<span class="number">0</span>)) <span class="comment"># 写上参数名，按名字传参</span></span><br><span class="line"><span class="built_in">print</span>(f(a=<span class="number">1</span>, c=<span class="number">0</span>, x=<span class="number">2</span>, b=<span class="number">1</span>)) <span class="comment"># 若用参数名，可以打乱顺序传参</span></span><br><span class="line"><span class="built_in">print</span>(f(x, a=<span class="number">2</span>))             <span class="comment"># 等价于传入 x=2,a=2, b=1, c=1</span></span><br><span class="line"><span class="built_in">print</span>(g)                     <span class="comment"># 5</span></span><br></pre></td></tr></tbody></table></figure><p></p>
<p><span class="github-emoji"><span>✅</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span><strong>类</strong> ：在程序中描述具体的一个物体。 比如一只猫有哪些特征，它能做哪些动作。 工程师想出了一个在代码中设置猫特征和动作的办法， 这就有了 class 类的概念。<br>  <strong><em><code>类的目的</code></em></strong> : 基于梳理逻辑步骤形成类,用于统筹管理函数，为函数分类使之条理清晰。 <strong>e.g.</strong> <code>数据处理类</code>（用于实现读写文件、数据提取、正则化、json序列化传输等函数功能）、<code>求解问题类</code>（问题参数、编解码目标函数计算、初始化种群、甘特图绘制等函数）、<code>算法模块</code>（naga-ii\moea/d\RL等类，每个类里包含诸如选择交叉变异的函数）、<code>工具类</code>（获取前沿面、轮盘赌等函数）</p>
<p><strong>a) 类的封装</strong>: 封装自己的属性和功能，把一个对象的属性私有化，同时提供一些可以被外界访问的属性的方法，如果存在不想被外界使用的方法，我们可私有化不提供方法给外界访问。<br></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 用 class File 来创建一个大概念（类），注意我们通常约定类的名字要首字母大写。 </span></span><br><span class="line"><span class="comment"># 然后用 my_file = File() 来创建一个具体的文件。</span></span><br><span class="line"><span class="comment"># self 类内部的索引，用于获取该类的属性或者功能</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Fjsp_RMT</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, job_num, machine_num, p1, p2, parm_data</span>):  <span class="comment">#初始化实例</span></span><br><span class="line">        self.job_num = job_num  <span class="comment"># 工件数</span></span><br><span class="line">        self.machine_num = machine_num  <span class="comment"># 机器数</span></span><br><span class="line">        self.p1 = p1  <span class="comment"># 全局选择的概率    </span></span><br><span class="line">        self._p2 = p2  <span class="comment"># 局部选择的概率  (_开头我不想让别人用这个变量，但是还是可以用）</span></span><br><span class="line">        self.__deleted = <span class="literal">False</span>  <span class="comment">#  __开头我不让别人用这个变量</span></span><br><span class="line">        self.product, self.machines, self.F_cog,self.F_time, </span><br><span class="line">        self.F_cost= parm_data[<span class="number">0</span>], parm_data[<span class="number">1</span>],parm_data[<span class="number">2</span>], parm_data[<span class="number">3</span>],parm_data[<span class="number">4</span>]</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_info</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># 类内部的功能返回时均带self</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"工件数为:"</span>+ <span class="built_in">str</span>(self.job_num) + <span class="string">" ,机器数为:"</span> +<span class="built_in">str</span>(self.machine_num)  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__force_delete</span>(<span class="params">self</span>):   <span class="comment"># 我不让别人使用这个功能</span></span><br><span class="line">        self.__deleted = <span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_soft_delete</span>(<span class="params">self</span>):     <span class="comment"># 我不想让别人使用这个功能</span></span><br><span class="line">        self.__force_delete()   <span class="comment"># 我自己可以在内部随便调用</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">fjsp_rmt=Fjsp_RMT(<span class="number">4</span>, <span class="number">3</span>, <span class="number">0.3</span>, <span class="number">0.4</span>, [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])</span><br><span class="line"><span class="built_in">print</span>(fjsp_rmt.get_info())</span><br></pre></td></tr></tbody></table></figure><br>|     私有形式      |                            特点                            |<br>| :———————-: | :————————————————————————————: |<br>| _ 一个下划线开头  | <code>弱隐藏</code> <strong>不想让</strong>别人用 （别人在必要情况下还是可以用的） |<br>| __ 两个下划线开头 |                  <code>强隐藏</code> <strong>不让</strong>别人用                   |<p></p>
<p><strong>b) 类的继承与多态</strong>:<br> <strong>继承</strong>：是使用已存在的类的定义作为基础建立新类的技术，新类的定义可以增加新的数据或新的功能，也可以用父类的功能，但不能选择性地继承父类。集成特点如下:</p>
<ol>
<li>父类变，子类就必须变</li>
<li>继承破坏了封装，对于父类而言，它的实现细节对与子类来说都是透明的。</li>
<li>继承是一种强耦合关系</li>
</ol>
<p><strong>多态</strong>：若子类重写了父类中的某些方法，在调用该些方法的时候，必定是使用子类中定义的这些方法（动态连接、动态调用）。实现多态有三个必要条件：继承、重写、调用父类的属性/函数。<br></p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Fjsp_RMT_singe</span>(<span class="title class_ inherited__">Fjsp_RMT</span>):  <span class="comment"># 继承了 Fjsp_RMT 的属性和功能</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, job_num, machine_num, p1, p2, parm_data, add</span>):</span><br><span class="line">        <span class="comment"># 将共用属性的设置导入 File 父类</span></span><br><span class="line">        <span class="comment">#继承父类super的init参数时属于外部无需self  加入self 会报错</span></span><br><span class="line">        <span class="built_in">super</span>().__init__(job_num, machine_num, p1, p2, parm_data) </span><br><span class="line">        self.add = add</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_more_info</span>(<span class="params">self</span>): <span class="comment"># 也可以在子类里复用父类功能</span></span><br><span class="line">        <span class="keyword">return</span> self.get_info() + <span class="string">",singe:"</span>+ <span class="built_in">str</span>(self.add)</span><br><span class="line">fjsp_rmt_singe=Fjsp_RMT_singe(<span class="number">4</span>, <span class="number">3</span>, <span class="number">0.3</span>, <span class="number">0.4</span>, [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>],(<span class="number">1080</span>, <span class="number">720</span>))</span><br><span class="line"><span class="built_in">print</span>(fjsp_rmt_singe.get_info())      <span class="comment"># 调用父类的功能</span></span><br><span class="line"><span class="built_in">print</span>(fjsp_rmt_singe.p1)              <span class="comment"># 调用父类的属性</span></span><br><span class="line"><span class="built_in">print</span>(fjsp_rmt_singe.add)             <span class="comment"># 调用自己的属性</span></span><br><span class="line"><span class="built_in">print</span>(fjsp_rmt_singe.get_more_info()) <span class="comment"># 调用自己加工父类的功能</span></span><br></pre></td></tr></tbody></table></figure><p></p>
<p><span class="github-emoji"><span>✅</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span><strong>模块</strong> ：module主要是为了一个相对比较大的工程，涉及到多个文件之间的互相调用关系。<br>  <code>算法模块</code>（包括naga-ii\moea/d\RL等类，每个类里包含诸如参数等属性以及选择、交叉、变异的函数功能）。</p>
<p><span class="github-emoji"><span>⭕</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2b55.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span><strong><em>统一规范：不涉及python包时均使用 import </em> as A 形式；涉及到 python包时采用from  A import <em> 形式并与<strong>init</strong>.py配合</em></strong>（原因：一般通用类都用 import numpy as np的形式，而python包中基本都是自己创建的程序类有实际用途所以包是空壳子没必要带着）<br></p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"> <span class="comment">#**如下代码显示了模块调用、导入的两种形式**</span></span><br><span class="line"> <span class="comment"># FJSP_RMT.py 和 main.py 在同一目录下 （main调用FJSP_RMT时）</span></span><br><span class="line">直接 <span class="keyword">import</span> 即可：</span><br><span class="line"><span class="keyword">import</span> FJSP_RMT <span class="keyword">as</span> problem</span><br><span class="line">或者</span><br><span class="line"><span class="keyword">from</span> b <span class="keyword">import</span> *</span><br></pre></td></tr></tbody></table></figure><br>上述两者的区别是：<p></p>
<ol>
<li>如果用 import FJSP_RMT as problem，我们在调用FJSP_RMT.py中定义的函数fun1()或类class1()时，需要写成 problem.fun1()或problem.class1()；</li>
<li>如果用 from FJSP_RMT import *，我们在调用FJSP_RMT.py中定义的函数fun1()或类class1()时，可以直接写成 fun1()或class1()；<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># Algorithm模块下的NSGA2.py 和 main.py 在不同级的目录下 （main调用Algorithm时）</span></span><br><span class="line"><span class="keyword">from</span>  Algorithm <span class="keyword">import</span> *</span><br><span class="line"><span class="comment"># 在Algorithm模块下的__init__.py中输入该行代码 .NSGA2代表返回上一目录后找到NSGA2.py</span></span><br><span class="line"><span class="keyword">from</span> .NSGA2 <span class="keyword">import</span> NSGA2    </span><br><span class="line">则我们在调用NSGA2.py中定义的函数fun1()或类class1()时，可以直接写成 fun1()或class1()；</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> Algorithm.NSGA2 <span class="keyword">as</span> NSGA2</span><br><span class="line">并把Algorithm模块下的__init__.py清空</span><br><span class="line">则我们在调用NSGA2.py中定义的函数fun1()或类class1()时，可以直接写成 NSGA2.fun1()或NSGA2.class1()</span><br></pre></td></tr></tbody></table></figure>
&nbsp;<h3 id="2-2-异常处理-try-except-raise-与-单元测试-Unittest"><a href="#2-2-异常处理-try-except-raise-与-单元测试-Unittest" class="headerlink" title="2.2 异常处理(try-except\raise) 与 单元测试 (Unittest)"></a>2.2 异常处理(try-except\raise) 与 单元测试 (Unittest)</h3><span class="github-emoji"><span>✅</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span><strong>异常处理(try-except)</strong><br>  代码出错不可避免，即便自己编写代码做到无错误 ，如果你的代码基于他人的代码，别人代码出错。即使别人的代码不出错，但是也有可能物理环境会出错呀，比如 CPU 超了，某人的服务运行失败，你恰好又是基于他提供的服务，你也跟着超时了。这些都会让你的程序报错。<strong>异常处理就是帮助我们去处理错误，预估到可能会报什么错误然后用try-except 结构包住处理，达到不妨碍接下来的代码执行。</strong><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">#try-except 结构 同时处理多个异常 </span></span><br><span class="line">d = {<span class="string">"name"</span>: <span class="string">"f1"</span>, <span class="string">"age"</span>: <span class="number">2</span>}</span><br><span class="line">l = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    v = d[<span class="string">"gender"</span>]</span><br><span class="line">    l[<span class="number">3</span>] = <span class="number">4</span></span><br><span class="line"><span class="keyword">except</span> (KeyError, IndexError) <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"key or index error for:"</span>, e)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">#try-except 结构 分开处理多个异常</span></span><br><span class="line">d = {<span class="string">"name"</span>: <span class="string">"f1"</span>, <span class="string">"age"</span>: <span class="number">2</span>}</span><br><span class="line">l = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    v = d[<span class="string">"gender"</span>]</span><br><span class="line">    l[<span class="number">3</span>] = <span class="number">4</span></span><br><span class="line"><span class="keyword">except</span> KeyError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"key error for:"</span>, e)</span><br><span class="line">    d[<span class="string">"gender"</span>] = <span class="string">"x"</span></span><br><span class="line"><span class="keyword">except</span> IndexError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"index error for:"</span>, e)</span><br><span class="line">    l.append(<span class="number">4</span>)</span><br><span class="line"><span class="built_in">print</span>(d)</span><br><span class="line"><span class="built_in">print</span>(l)</span><br></pre></td></tr></tbody></table></figure>
还有一个 try-except-else 的模式，在 else 中处理没有报错的情况。报错情况下不会执行else下的代码，不报错会执行。<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># try-except-else 结构</span></span><br><span class="line"></span><br><span class="line">l = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    l[<span class="number">3</span>] = <span class="number">4</span></span><br><span class="line"><span class="keyword">except</span> IndexError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"no error, now in else"</span>)</span><br></pre></td></tr></tbody></table></figure>
如果 else 是为了执行没有异常的状况，那么 finally 就是为了执行 不管有没有异常 的情况。无论有报错还是没报错，finally 下面的代码都会运行。<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># try-except-finally 结构</span></span><br><span class="line"></span><br><span class="line">l = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    l[<span class="number">3</span>] = <span class="number">4</span></span><br><span class="line"><span class="keyword">except</span> IndexError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(e)</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"reach finally"</span>)</span><br></pre></td></tr></tbody></table></figure>
上面这两种模式主要用在什么 case 当中呢？你想一下，是不是有些时候，不管你有没有报错，你都想让程序去执行什么。我们甚至都不需要为任何异常做任何处理。 这种时候也就是说，你有异常，我不让你终止主程序，你没有异常吧，万事大吉。<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    dddd = dddddd</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"I know there is error, so what?"</span>)</span><br></pre></td></tr></tbody></table></figure>
<span class="github-emoji"><span>✅</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span><strong>raise手动触发异常</strong><br>   raise 是你为别人犯错留下的证据，或者是告诉别人你怎么犯错的。这个信息对于别人 dubug 你的代码十分有好处。另一种情况是，你写了成百上千行代码，你也不能全记住代码的每一个细节。所以一旦报错，你也需要一个友善的错误信息提示，这时用 raise 准没错。<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">no_negative</span>(<span class="params">num</span>):</span><br><span class="line">    <span class="keyword">if</span> num &lt; <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">"I said no negative"</span>)</span><br><span class="line">    <span class="keyword">return</span> num</span><br><span class="line"><span class="built_in">print</span>(no_negative(-<span class="number">1</span>))</span><br><span class="line"><span class="comment"># Traceback (most recent call last):</span></span><br><span class="line"><span class="comment"># ValueError: I said no negative</span></span><br></pre></td></tr></tbody></table></figure>
</li>
</ol>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">异常名称</th>
<th style="text-align:center">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">BaseException</td>
<td style="text-align:center">所有异常的基类</td>
</tr>
<tr>
<td style="text-align:center">SystemExit</td>
<td style="text-align:center">解释器请求退出</td>
</tr>
<tr>
<td style="text-align:center">KeyboardInterrupt</td>
<td style="text-align:center">用户中断执行(通常是输入^C)</td>
</tr>
<tr>
<td style="text-align:center">Exception</td>
<td style="text-align:center">常规错误的基类</td>
</tr>
<tr>
<td style="text-align:center">StopIteration</td>
<td style="text-align:center">迭代器没有更多的值</td>
</tr>
<tr>
<td style="text-align:center">GeneratorExit</td>
<td style="text-align:center">生成器(generator)发生异常来通知退出</td>
</tr>
<tr>
<td style="text-align:center">StandardError</td>
<td style="text-align:center">所有的内建标准异常的基类</td>
</tr>
<tr>
<td style="text-align:center">ArithmeticError</td>
<td style="text-align:center">所有数值计算错误的基类</td>
</tr>
<tr>
<td style="text-align:center">FloatingPointError</td>
<td style="text-align:center">浮点计算错误</td>
</tr>
<tr>
<td style="text-align:center">OverflowError</td>
<td style="text-align:center">数值运算超出最大限制</td>
</tr>
<tr>
<td style="text-align:center">ZeroDivisionError</td>
<td style="text-align:center">除(或取模)零 (所有数据类型)</td>
</tr>
<tr>
<td style="text-align:center">AssertionError</td>
<td style="text-align:center">断言语句失败</td>
</tr>
<tr>
<td style="text-align:center">AttributeError</td>
<td style="text-align:center">对象没有这个属性</td>
</tr>
<tr>
<td style="text-align:center">EOFError</td>
<td style="text-align:center">没有内建输入,到达EOF 标记</td>
</tr>
<tr>
<td style="text-align:center">EnvironmentError</td>
<td style="text-align:center">操作系统错误的基类</td>
</tr>
<tr>
<td style="text-align:center">IOError</td>
<td style="text-align:center">输入/输出操作失败</td>
</tr>
<tr>
<td style="text-align:center">OSError</td>
<td style="text-align:center">操作系统错误</td>
</tr>
<tr>
<td style="text-align:center">WindowsError</td>
<td style="text-align:center">系统调用失败</td>
</tr>
<tr>
<td style="text-align:center">ImportError</td>
<td style="text-align:center">导入模块/对象失败</td>
</tr>
<tr>
<td style="text-align:center">LookupError</td>
<td style="text-align:center">无效数据查询的基类</td>
</tr>
<tr>
<td style="text-align:center">IndexError</td>
<td style="text-align:center">序列中没有此索引(index)</td>
</tr>
<tr>
<td style="text-align:center">KeyError</td>
<td style="text-align:center">映射中没有这个键</td>
</tr>
<tr>
<td style="text-align:center">MemoryError</td>
<td style="text-align:center">内存溢出错误(对于Python 解释器不是致命的)</td>
</tr>
<tr>
<td style="text-align:center">NameError</td>
<td style="text-align:center">未声明/初始化对象 (没有属性)</td>
</tr>
<tr>
<td style="text-align:center">UnboundLocalError</td>
<td style="text-align:center">访问未初始化的本地变量</td>
</tr>
<tr>
<td style="text-align:center">ReferenceError</td>
<td style="text-align:center">弱引用(Weak reference)试图访问已经垃圾回收了的对象</td>
</tr>
<tr>
<td style="text-align:center">RuntimeError</td>
<td style="text-align:center">一般的运行时错误</td>
</tr>
<tr>
<td style="text-align:center">NotImplementedError</td>
<td style="text-align:center">尚未实现的方法</td>
</tr>
<tr>
<td style="text-align:center">SyntaxError</td>
<td style="text-align:center">Python 语法错误</td>
</tr>
<tr>
<td style="text-align:center">IndentationError</td>
<td style="text-align:center">缩进错误</td>
</tr>
<tr>
<td style="text-align:center">TabError</td>
<td style="text-align:center">Tab 和空格混用</td>
</tr>
<tr>
<td style="text-align:center">SystemError</td>
<td style="text-align:center">一般的解释器系统错误</td>
</tr>
<tr>
<td style="text-align:center">TypeError</td>
<td style="text-align:center">对类型无效的操作</td>
</tr>
<tr>
<td style="text-align:center">ValueError</td>
<td style="text-align:center">传入无效的参数</td>
</tr>
<tr>
<td style="text-align:center">UnicodeError</td>
<td style="text-align:center">Unicode 相关的错误</td>
</tr>
<tr>
<td style="text-align:center">UnicodeDecodeError</td>
<td style="text-align:center">Unicode 解码时的错误</td>
</tr>
<tr>
<td style="text-align:center">UnicodeEncodeError</td>
<td style="text-align:center">Unicode 编码时错误</td>
</tr>
<tr>
<td style="text-align:center">UnicodeTranslateError</td>
<td style="text-align:center">Unicode 转换时错误</td>
</tr>
<tr>
<td style="text-align:center">Warning</td>
<td style="text-align:center">警告的基类</td>
</tr>
<tr>
<td style="text-align:center">DeprecationWarning</td>
<td style="text-align:center">关于被弃用的特征的警告</td>
</tr>
<tr>
<td style="text-align:center">FutureWarning</td>
<td style="text-align:center">关于构造将来语义会有改变的警告</td>
</tr>
<tr>
<td style="text-align:center">OverflowWarning</td>
<td style="text-align:center">旧的关于自动提升为长整型(long)的警告</td>
</tr>
<tr>
<td style="text-align:center">PendingDeprecationWarning</td>
<td style="text-align:center">关于特性将会被废弃的警告</td>
</tr>
<tr>
<td style="text-align:center">RuntimeWarning</td>
<td style="text-align:center">可疑的运行时行为(runtime behavior)的警告</td>
</tr>
<tr>
<td style="text-align:center">SyntaxWarning</td>
<td style="text-align:center">可疑的语法的警告</td>
</tr>
<tr>
<td style="text-align:center">UserWarning</td>
<td style="text-align:center">用户代码生成的警告</td>
</tr>
</tbody>
</table>
</div>
<p><span class="github-emoji"><span>✅</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span><strong>单元测试 (Unittest)</strong><br>代码调试是编写代码的至关重要的一环。尤其是你要写一些功能服务给别人用的时候，免不了要先自测有没有问题。<strong><font color="red">保证我写的，真的是我想要的。</font></strong><br><strong><font color="blue" size="3">使用场景:</font></strong> <code>程序的代码量小，且项目功能之间无耦合关系时，可运行程序打断点进行debug。但是当处理的代码量比较大，牵扯到的资源相对多，我很多时候并不知道哪里会不会有错误。 所以这种情况非常依赖于测试。让自动化测试帮我处理任何改动可能带来的问题。</code></p>
<p><strong><font color="red">unittest 规范：</font></strong>首先 unittest 不会被其他人使用到，纯粹是你自己为了验证自己写的代码有没有问题的方式。一般般采用 XXXX.py与XXXX_test.py的形成进行单元测试。另外，你可以按照 unittest 当中的 case 为蓝本，去完善你原函数的功能。 就好像有了一个目标，你要为了这个目标去开发功能一样。这样就可以先写 unittest 当中的 case，比如下面，我不会先写 my_func 里面的内容，而是先把我的测试和要验收的指标写好。然后后面我再开发功能。<br></p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># main.py  主文件用于编写程序</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">my_func1</span>(<span class="params">a</span>):</span><br><span class="line">    <span class="keyword">if</span> a == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">2</span></span><br><span class="line">    <span class="keyword">elif</span> a == -<span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">3</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">my_func2</span>(<span class="params">b</span>):</span><br><span class="line">    <span class="keyword">if</span> b != <span class="string">"yes"</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">"you can only say yes!"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        </span><br><span class="line"><span class="comment"># main_test.py  测试文件用于编写程序     </span></span><br><span class="line"><span class="keyword">import</span> unittest</span><br><span class="line"><span class="keyword">import</span> main  <span class="keyword">as</span> m</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyTestCase</span>(unittest.TestCase):       <span class="comment"># 固定格式</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">test_something1</span>(<span class="params">self</span>):             <span class="comment"># 可自定义函数功能名称</span></span><br><span class="line">        self.assertEqual(<span class="number">2</span>, m.my_func1(<span class="number">1</span>)) <span class="comment"># 测试功能</span></span><br><span class="line">        self.assertEqual(<span class="number">3</span>, m.my_func1(-<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(-<span class="number">100</span>, <span class="number">100</span>):</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">1</span> <span class="keyword">or</span> i == -<span class="number">1</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            self.assertEqual(<span class="number">1</span>, m.my_func1(i))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">test_something2</span>(<span class="params">self</span>):</span><br><span class="line">        self.assertTrue(m.my_func2(<span class="string">"yes"</span>))</span><br><span class="line">        <span class="keyword">with</span> self.assertRaises(ValueError):</span><br><span class="line">            m.my_func2(<span class="string">"nononono"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    unittest.main()</span><br><span class="line">    <span class="comment"># 当仅仅想测试某一个函数功能时，定义一个 suite 替换 unittest.main()</span></span><br><span class="line">    suite = unittest.TestSuite()</span><br><span class="line">    suite.addTest(MyTestCase(<span class="string">'test_something1'</span>))</span><br><span class="line">    unittest.TextTestRunner().run(suite)</span><br></pre></td></tr></tbody></table></figure><p></p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">assert</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">assertEqual(a, b)</td>
<td style="text-align:center">a == b</td>
</tr>
<tr>
<td style="text-align:center">assertNotEqual(a, b)</td>
<td style="text-align:center">a != b</td>
</tr>
<tr>
<td style="text-align:center">assertTrue(condition)</td>
<td style="text-align:center">condition 是不是 True</td>
</tr>
<tr>
<td style="text-align:center">assertFalse(condition)</td>
<td style="text-align:center">condition 是不是 False</td>
</tr>
<tr>
<td style="text-align:center">assertGreater(a, b)</td>
<td style="text-align:center">a &gt; b</td>
</tr>
<tr>
<td style="text-align:center">assertGreaterThan(a, b)</td>
<td style="text-align:center">a &gt;= b</td>
</tr>
<tr>
<td style="text-align:center">assertLess(a, b)</td>
<td style="text-align:center">a &lt; b</td>
</tr>
<tr>
<td style="text-align:center">assertLessEqual(a, b)</td>
<td style="text-align:center">a &lt;= b</td>
</tr>
<tr>
<td style="text-align:center">assertIs(a, b)</td>
<td style="text-align:center">a is b，a 和 b 是不是同一对象</td>
</tr>
<tr>
<td style="text-align:center">assertIsNot(a, b)</td>
<td style="text-align:center">a is not b，a 和 b 是不是不同对象</td>
</tr>
<tr>
<td style="text-align:center">assertIsNone(a)</td>
<td style="text-align:center">a is None，a 是不是 None</td>
</tr>
<tr>
<td style="text-align:center">assertIsNotNone(a)</td>
<td style="text-align:center">a is not None，a 不是 None？</td>
</tr>
<tr>
<td style="text-align:center">assertIn(a, b)</td>
<td style="text-align:center">a in b, a 在 b 里面？</td>
</tr>
<tr>
<td style="text-align:center">assertNotIn(a, b)</td>
<td style="text-align:center">a not in b，a 不在 b 里？</td>
</tr>
<tr>
<td style="text-align:center">assertRaises(err)</td>
<td style="text-align:center">通常和 with 一起用，判断 with 里的功能是否会报错</td>
</tr>
</tbody>
</table>
</div>
<h3 id="2-3-生成器与装饰器"><a href="#2-3-生成器与装饰器" class="headerlink" title="2.3 生成器与装饰器"></a>2.3 生成器与装饰器</h3><p><strong>生成器 Generator</strong>，一种占用更小内存的方式处理循环迭代，可以说生成器就是为循环设计的。生成器是一种优化代码程序节省占用内存的方法，如果当你的循环内存不足时，或者运行很慢时，生成器是你需要考虑的。</p>
<blockquote>
<p>在循环的时候，我们的目的是为了每次循环拿到一些特定数据，然后为这些数据做处理。但是无可避免的会在内存中记录这些数值， 当需要记录的数值很多的时候，我们的内存可能就吃不消了。而生成器就是用来现用现存的<br>，我们只在需要这个数据的时候生成它，生成完了我就不用了，也不需要记录。这种时候将会节约我很多内存的需求。<br></p><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">need_return</span>():</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">5</span>):</span><br><span class="line">        <span class="keyword">if</span> item % <span class="number">2</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"我要扔出去一个 item=%d 了"</span> % item)</span><br><span class="line">            <span class="keyword">yield</span> item  <span class="comment"># 这里就会返回给下面的 for 循环</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"我又回到里面了"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> need_return():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"我在外面接到了一个 item=%d\n"</span> % i)</span><br><span class="line">&gt;&gt;</span><br><span class="line"><span class="comment">#我要扔出去一个 item=0 了</span></span><br><span class="line"><span class="comment">#我在外面接到了一个 item=0</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#我又回到里面了</span></span><br><span class="line"><span class="comment">#我要扔出去一个 item=2 了</span></span><br><span class="line"><span class="comment">#我在外面接到了一个 item=2</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#我又回到里面了</span></span><br><span class="line"><span class="comment">#我要扔出去一个 item=4 了</span></span><br><span class="line"><span class="comment">#我在外面接到了一个 item=4</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#我又回到里面了</span></span><br></pre></td></tr></tbody></table></figure><br>定义生成器类<br>用一个 class 也是可以表示一个迭代器，生成器的。 如果我们将上面的逻辑转化成 <code>class</code>，这个 class 可能相对比较复杂，但是也意味着你可以有更多设置和控制发生在这个 class 里面。 里面我们申明了用于生成器的两个 method，__iter__ 和 __next__。<p></p>
</blockquote>
<p>__iter__ 的意思是，当我在外面 for 循环进行迭代时，我返回什么？在下面例子中，我就把自己这个 class 本身返回回去，继续让自己做迭代就好了。</p>
<p>__next__ 的意思是每次迭代的时候，我的函数会放出来什么元素。下面的功能中实现的就是放出来一个被计算过的 item 元素。<br></p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">NeedReturn</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, init_value=<span class="number">0</span></span>):</span><br><span class="line">        self.tmp = init_value</span><br><span class="line">        self.item = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__iter__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__next__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">            <span class="keyword">if</span> self.item == self.tmp:</span><br><span class="line">                self.tmp *= <span class="number">2</span></span><br><span class="line">                <span class="keyword">return</span> self.item  % 等价于<span class="keyword">yield</span></span><br><span class="line">            self.item += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> self.item == <span class="number">300</span>:</span><br><span class="line">                <span class="keyword">raise</span> StopIteration</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> NeedReturn(<span class="number">10</span>):</span><br><span class="line">    <span class="built_in">print</span>(i)</span><br><span class="line">&gt;&gt;</span><br><span class="line"><span class="comment">#10</span></span><br><span class="line"><span class="comment">#20</span></span><br><span class="line"><span class="comment">#40</span></span><br><span class="line"><span class="comment">#80</span></span><br><span class="line"><span class="comment">#160</span></span><br></pre></td></tr></tbody></table></figure><p></p>
<p><strong>装饰器 Decorator</strong> 它是一个装饰 Python Function 的东西。 Function 为什么要被装饰？那就是我们想为这个 Function 做些额外的事情。但是只为一个 Function 做这件事，那我们还不如直接改掉这个 Function，让它直接干这件事就好了。 Decorator 的好处就是，我们可以给批量的 Function 都做这件事。当有一批 Function 都要做些前置或者后置的工作，我们可以统一给他们装修，用一个装饰器统一处理从而减轻你的开发量，特别是针对于不同 function，要做同样的前置处理或后置处理的时候。</p>
<blockquote>
<p>比如数据库的处理，来了一条数据，我先要验证这个数据的准确性，然后进行个性化 function 的处理，最后将加工后的数据写入数据库。<br>网页个人页的鉴权，用户每一个点击，我都先需要鉴权，才能做后续不同的 function。<br></p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">authorization</span>(<span class="params">fn</span>):  <span class="comment">#authorization 与@authorization一致即可</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">check_and_do</span>(<span class="params">name</span>):</span><br><span class="line">        <span class="keyword">if</span> name != <span class="string">"hm"</span>:   <span class="comment"># 鉴权</span></span><br><span class="line">            <span class="built_in">print</span>(name + <span class="string">" has no right!"</span>)</span><br><span class="line">            <span class="keyword">return</span> </span><br><span class="line">        res = fn(name)</span><br><span class="line">        <span class="keyword">return</span> res</span><br><span class="line">    <span class="keyword">return</span> check_and_do  <span class="comment">#返回值check_and_do与函数功能check_and_do一致即可</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@authorization   </span><span class="comment">#装饰器格式</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">outer1</span>(<span class="params">name</span>):</span><br><span class="line">    <span class="built_in">print</span>(name+<span class="string">" outer1"</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@authorization</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">outer2</span>(<span class="params">name</span>):</span><br><span class="line">    <span class="built_in">print</span>(name+<span class="string">" outer2"</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">@authorization</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">outer3</span>(<span class="params">name</span>):</span><br><span class="line">    <span class="built_in">print</span>(name+<span class="string">" outer3"</span>)</span><br><span class="line"></span><br><span class="line">outer1(<span class="string">"hm"</span>)</span><br><span class="line">outer2(<span class="string">"hmm"</span>)</span><br><span class="line">outer3(<span class="string">"hm"</span>)</span><br><span class="line">&gt;&gt;</span><br><span class="line"><span class="comment">#hm outer1</span></span><br><span class="line"><span class="comment">#hmm has no right!</span></span><br><span class="line"><span class="comment">#hm outer3</span></span><br></pre></td></tr></tbody></table></figure><p></p>
</blockquote>
<h3 id="2-4-工程问题编程思路"><a href="#2-4-工程问题编程思路" class="headerlink" title="2.4 工程问题编程思路"></a>2.4 工程问题编程思路</h3><p><strong>问题聚焦：</strong> 我们面临一个什么具体问题需要进行解决，最终简化问题提取关键核心点（实际存在并且是有研究或者工程应用价值）<br><strong>产品定位：</strong> 为解决此问题我们需要做什么，实现什么功能来创造价值。</p>
<p><strong>程序设计:</strong> 梳理需要实现的各个模块功能间的逻辑关系，形成模块-类-功能的编程架构；定制解决方案系统设计功能的开发，尽可能的成系统编程、可复用代码从而优化编程架构。<br><strong>程序开发:</strong> 基于形成的模块-类-功能的编程架构进行具体开发,并根据开发过程中遇到的问题不断完善流程和克服问题。</p>
<hr>
<h2 id="3-python-文件数据处理"><a href="#3-python-文件数据处理" class="headerlink" title="3. python 文件数据处理"></a>3. python 文件数据处理</h2><h3 id="3-1文件数据读写"><a href="#3-1文件数据读写" class="headerlink" title="3.1文件数据读写"></a>3.1文件数据读写</h3><h4 id="3-1-1-基础读写"><a href="#3-1-1-基础读写" class="headerlink" title="3.1.1 基础读写"></a>3.1.1 基础读写</h4><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># with结构（将文件的打开和关闭嵌入到了一个 with 架构中不用担心忘记关闭文件）</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">"new_file2.txt"</span>, <span class="string">"w"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">"some text...\n 2nd line\n"</span>)         <span class="comment"># 在文件里写入</span></span><br><span class="line">    f.writelines([<span class="string">"some text for file2...\n"</span>, <span class="string">"2nd line\n"</span>])</span><br><span class="line">    <span class="comment"># f.writelines与f.write等价，但必须在元素末尾加 `\n`来换行不然多出来是黏连的。</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">"new_file2.txt"</span>, <span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="built_in">print</span>(f.read())        <span class="comment"># 在文件里读取</span></span><br><span class="line">    f.seek(<span class="number">0</span>)       <span class="comment"># 将开始读的位置从写入的最后位置调到开头</span></span><br><span class="line">    <span class="built_in">print</span>(f.readlines())   <span class="comment"># 与f.read()等价</span></span><br><span class="line">    f.seek(<span class="number">0</span>)       </span><br><span class="line">    <span class="built_in">print</span>(f.readline())  <span class="comment">#读取当前行，取代一次性全部读取，不让内存被一次性占满。</span></span><br><span class="line"><span class="comment"># 注：f.read()和f.readlines()使用后指针移到最后一行开始位置；</span></span><br><span class="line"><span class="comment">#     f.readline()使用后指针自动移到下一行开始位置。</span></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 编码格式与中文乱码</span></span><br><span class="line"><span class="comment"># 按二进制 binary读写时 “wb”,"rb", </span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">"chinese.txt"</span>, <span class="string">"wb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">"这是中文的，this is Chinese"</span>)<span class="comment"># 报错</span></span><br><span class="line">    f.write(<span class="string">"这是中文的，this is Chinese"</span>.encode(<span class="string">"utf-8"</span>))</span><br><span class="line">    <span class="comment"># 写入时将string → a bytes-like object 并按utf-8格式</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">"chinese.txt"</span>, <span class="string">"rb"</span>, ) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="built_in">print</span>(f.read().decode(<span class="string">'utf-8'</span>))</span><br><span class="line">    <span class="comment"># 读取时将a bytes-like object → string 按utf-8格式  </span></span><br><span class="line">（或者）</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">"chinese.txt"</span>, <span class="string">"r"</span>, encoding=<span class="string">"utf-8"</span> ) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="built_in">print</span>(f.read())  </span><br></pre></td></tr></tbody></table></figure>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">MODE</th>
<th style="text-align:center">含义</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">w</td>
<td style="text-align:center">（创建）写文本</td>
</tr>
<tr>
<td style="text-align:center">r</td>
<td style="text-align:center">读文本，文件不存在会报错</td>
</tr>
<tr>
<td style="text-align:center">a</td>
<td style="text-align:center">在文本最后添加</td>
</tr>
<tr>
<td style="text-align:center">wb</td>
<td style="text-align:center">写二进制 binary</td>
</tr>
<tr>
<td style="text-align:center">rb</td>
<td style="text-align:center">读二进制 binary</td>
</tr>
<tr>
<td style="text-align:center">ab</td>
<td style="text-align:center">添加二进制</td>
</tr>
<tr>
<td style="text-align:center">w+</td>
<td style="text-align:center"><strong>又可以读又可以（创建）写</strong></td>
</tr>
<tr>
<td style="text-align:center">r+</td>
<td style="text-align:center"><strong>又可以读又可以写, 文件不存在会报错</strong></td>
</tr>
<tr>
<td style="text-align:center">a+</td>
<td style="text-align:center">可读写，在文本最后添加</td>
</tr>
<tr>
<td style="text-align:center">x</td>
<td style="text-align:center">创建</td>
</tr>
</tbody>
</table>
</div>
<h4 id="3-1-2-Numpy库中的数据读写"><a href="#3-1-2-Numpy库中的数据读写" class="headerlink" title="3.1.2 Numpy库中的数据读写"></a>3.1.2 Numpy库中的数据读写</h4><h4 id="3-1-3-Panda库中的数据读写"><a href="#3-1-3-Panda库中的数据读写" class="headerlink" title="3.1.3 Panda库中的数据读写"></a>3.1.3 Panda库中的数据读写</h4><h3 id="3-2文件目录管理（OS库）"><a href="#3-2文件目录管理（OS库）" class="headerlink" title="3.2文件目录管理（OS库）"></a>3.2文件目录管理（OS库）</h3><figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os </span><br><span class="line"><span class="keyword">import</span> shutil</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"当前目录："</span>, os.getcwd())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"当前目录里有什么："</span>, os.listdir())</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建一个 project 的文件夹其中exist_ok=True 当project存在时不报错。（默认为False）</span></span><br><span class="line">os.makedirs(<span class="string">"user"</span>, exist_ok=<span class="literal">True</span>) <span class="comment"># 创建Dictionary文件夹</span></span><br><span class="line"><span class="comment"># 用户注册</span></span><br><span class="line"><span class="keyword">if</span> os.path.exists(<span class="string">"user/hm"</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"user exist"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    os.makedirs(<span class="string">"user/hm"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"user created"</span>)</span><br><span class="line"><span class="built_in">print</span>(os.listdir(<span class="string">"user"</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用户注销</span></span><br><span class="line"><span class="keyword">if</span> os.path.exists(<span class="string">"user/hm"</span>):</span><br><span class="line">    os.removedirs(<span class="string">"user/hm"</span>)  <span class="comment">#只能删除文件夹为空的文件夹</span></span><br><span class="line">    <span class="comment"># 注意 shutil 库太强大了，它会清空整个目录防止误删，防止误删，防止误删！</span></span><br><span class="line">    shutil.rmtree(<span class="string">"user/mofan"</span>) </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"user removed"</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"user not exist"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用户更名</span></span><br><span class="line">os.rename(<span class="string">"user/hm"</span>, <span class="string">"user/hm_new"</span>)</span><br><span class="line"><span class="built_in">print</span>(os.listdir(<span class="string">"user"</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment">#文件目录多种检验(os.path)</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">"user/hm/a.txt"</span>, <span class="string">"w"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(<span class="string">"nothing"</span>)</span><br><span class="line"><span class="built_in">print</span>(os.path.isfile(<span class="string">"user/hm/a.txt"</span>)) <span class="comment"># True</span></span><br><span class="line"><span class="built_in">print</span>(os.path.exists(<span class="string">"user/hm/a.txt"</span>)) <span class="comment"># True</span></span><br><span class="line"><span class="built_in">print</span>(os.path.isdir(<span class="string">"user/hm/a.txt"</span>)) <span class="comment"># False</span></span><br><span class="line"><span class="built_in">print</span>(os.path.isdir(<span class="string">"user/hm"</span>))  <span class="comment"># True</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#文件的复制</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">copy</span>(<span class="params">path</span>):</span><br><span class="line">    dir_name, filename = os.path.split(path)</span><br><span class="line">    new_filename = <span class="string">"new2_"</span> + filename    <span class="comment"># 新文件名</span></span><br><span class="line">    new_path = os.path.join(dir_name, new_filename) <span class="comment"># 目录重组</span></span><br><span class="line">    shutil.copy2(path, new_path)   <span class="comment"># 复制文件</span></span><br><span class="line">    <span class="keyword">return</span> os.path.isfile(new_path), new_path</span><br><span class="line">copied, new_path = copy(<span class="string">"user/hm/a.txt"</span>)</span><br><span class="line"><span class="keyword">if</span> copied:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"copied to:"</span>, new_path)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"copy failed"</span>)</span><br></pre></td></tr></tbody></table></figure>
<p><strong>（感性认识）Dictionary和package的使用，存储资源等不需要执行诸多种类的程序时采用Dictionary 否则用package</strong></p>
<blockquote>
<p> Dictionary在pycharm中是一个文件夹目录，放置资源文件，对应于在进行JavaWeb开发时用于放置css/js文件的目录，或者说在进行物体识别时，用来存储背景图像的文件夹。该文件夹其中并不包含_ _ init.py_ _文件。<br> Python package 包文件夹而言，与Dictionary不同之处在于其会自动创建_ _ init.py_ _ 文件。其包括一组模块和一个 _ _ init.py_ _ 文件。该包的使用与Python的import机制有关，这关乎到你的哪些.py文件是对外可访问的。有些时候，如果一个包下有很多模块，在调用方import如此多模块是很费事，且不优雅的，此时可以通过修改 _ _ init_ _.py来完成该任务。<br>&nbsp;</p>
</blockquote>
<h3 id="3-3正则表达式匹配-未完成"><a href="#3-3正则表达式匹配-未完成" class="headerlink" title="3.3正则表达式匹配(未完成)"></a>3.3正则表达式匹配(未完成)</h3><p>  在文件、文字查找、匹配、替换、处理时，少不了要根据特定规则来处理对应文字的情况。 正则表达式 Regular Expression（regex —— <strong>re库</strong>）就是通过用一些规则或者模板来帮你找到文字，替换文字的工具。</p>
<p><strong>正则库功能表</strong><br>|     功能      |                                           说明                                           |                                             举例                                             |<br>| :—————-: | :———————————————————————————————————————————: | :—————————————————————————————————————————————: |<br>|  re.search()  |                       扫描查找整个字符串，<strong>找到第一个模式匹配</strong>的                       |                           re.search(r”run”, I run to you) &gt; ‘run’                            |<br>|  re.match()   | 从字符的<strong>最开头匹配，找到第一个模式匹配</strong>的即使用 re.M 多行匹配，也是从最最开头开始匹配 |                            re.match(r”run”, I run to you) &gt; None                             |<br>| re.findall()  |                         <strong>返回一个不重复的 pattern 的匹配列表</strong>                          |              re.findall(rr[ua]n, I run to you. you ran to him) &gt; [‘run’, ‘ran’]              |<br>| re.finditer() |                         和 findall 一样，只是用迭代器的方式使用                          |               for item in re.finditer(rr[ua]n, I run to you. you ran to him):                |<br>|  re.split()   |                                     用正则分开字符串                                     |     re.split(rr[ua]n, I run to you. you ran to him) &gt; [‘I ‘, ‘ to you. you ‘, ‘ to him’]     |<br>|   re.sub()    |                                      用正则替换字符                                      |    re.sub(rr[ua]n, jump, I run to you. you ran to him) &gt; ‘I jump to you. you jump to him’    |<br>|   re.subn()   |                            和 sub 一样，额外返回一个替代次数                             | re.subn(rr[ua]n, jump, I run to you. you ran to him) &gt; (‘I jump to you. you jump to him’, 2) |</p>
<p><strong>通用匹配方式表</strong><br>| 特定匹配标识 |                 表达含义                 |                         实际范围                          |<br>| :—————: | :———————————————————: | :———————————————————————————-: |<br>|      \d      |                 任何数字                 |                           [0-9]                           |<br>|      \D      |                不是数字的                |                                                           |<br>|      \s      |               任何空白字符               |                       [ \t\n\r\f\v]                       |<br>|      \S      |              空白字符以外的              |                                                           |<br>|    <strong>\w</strong>    |         任何大小写字母,数字和 _          |                       [a-zA-Z0-9_]                        |<br>|      \W      |                \w 以外的                 |                                                           |<br>|      \b      |             匹配一个单词边界             | 比如 er\b 可以匹配 never 中的 er，但不能匹配 verb 中的 er |<br>|      \B      |              匹配非单词边界              |  比如 er\B 能匹配 verb 中的 er，但不能匹配 never 中的 er  |<br>|      \\      |                强制匹配 \                |                                                           |<br>|    <strong>.</strong>     |          匹配任何字符 (除了 \n)          |                                                           |<br>|    <strong>?</strong>     |            前面的模式可有可无            |                                                           |<br>|    <strong>*</strong>     |              重复零次或多次              |                                                           |<br>|    <strong>+</strong>     |              重复一次或多次              |                                                           |<br>|    {n,m}     |              重复 n 至 m 次              |                                                           |<br>|     {n}      |                重复 n 次                 |                                                           |<br>|    <strong>+?</strong>    |          非贪婪，最小方式匹配 +          |                                                           |<br>|      <em>?      |          非贪婪，最小方式匹配 </em>          |                                                           |<br>|      ??      |          非贪婪，最小方式匹配 ?          |                                                           |<br>|      ^       | 匹配一行开头，在 re.M 下，每行开头都匹配 |                                                           |<br>|      $       | 匹配一行结尾，在 re.M 下，每行结尾都匹配 |                                                           |<br>|      \A      |  匹配最开始，在 re.M 下，也从文本最开始  |                                                           |<br>|      \B      |  匹配最结尾，在 re.M 下，也从文本最结尾  |                                                           |</p>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># （举例说明）注册管理，验证邮箱是否有效。</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line"><span class="comment"># 先compile解析好一个正则 pattern，然后直接用这个 pattern 去执行查找。</span></span><br><span class="line"><span class="comment">#当需要循环查找时将compile放在循环前，search放在循环内执行从而节省执行时间</span></span><br><span class="line">ptn = re.<span class="built_in">compile</span>(<span class="string">r"\w+?@\w+?\.com"</span>)</span><br><span class="line"><span class="comment"># “\w ” 表示任何大小写字母,数字和 _ “+?”表示前面的模式至少匹配一次。</span></span><br><span class="line"><span class="comment"># 当识别 “@” 的时候做其前面的非贪婪模式匹配“\.”表示.的字面含义。如果只有“.”表示匹配任何字符。</span></span><br><span class="line">matched = ptn.search(<span class="string">"huangming98@163.com"</span>)</span><br><span class="line"><span class="built_in">print</span>(matched)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将compile解析和模式查找放在一条语句里。</span></span><br><span class="line">matched = re.search(<span class="string">r"\w+?@\w+?\.com"</span>, <span class="string">"huangming98@163.com"</span>) </span><br><span class="line"><span class="comment">#用 r"xxx" 来写一个 pattern，r 代表原生字符串，当成一个规则来记住所写 pattern 时，都需写上一个 r 在前面。</span></span><br><span class="line"><span class="built_in">print</span>( matched)</span><br><span class="line"><span class="comment"># Ans: &lt;re.Match object; span=(0, 19), match='huangming98@163.com'&gt;</span></span><br><span class="line"><span class="comment"># span=(0, 19)代表着在原始字符串中，我们找到的 pattern 是从哪一位到哪一位</span></span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">#同时满足多种条件的pattern写法</span></span><br><span class="line">re.search(<span class="string">r"ran|run"</span>, <span class="string">"I run to you"</span>)</span><br><span class="line">re.search(<span class="string">r"r[au]n"</span>, <span class="string">"I run to you"</span>)</span><br><span class="line">re.search(<span class="string">r"f(ou|i)nd"</span>, <span class="string">"I found you"</span>)</span><br></pre></td></tr></tbody></table></figure>
<figure class="highlight py"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">#中文识别</span></span><br><span class="line">re.search(<span class="string">r"不.*?爱"</span>, <span class="string">"我不是很爱你"</span>)</span><br><span class="line"><span class="comment">#&lt;re.Match object; span=(1, 5), match='不是很爱'&gt;</span></span><br><span class="line">re.search(<span class="string">r"[\u4e00-\u9fa5]+"</span>, <span class="string">"我爱Python是的。"</span>)</span><br><span class="line"><span class="comment">#&lt;re.Match object; span=(0, 2), match='我爱'&gt;</span></span><br><span class="line">re.search(<span class="string">r"[\u4e00-\u9fa5！？。，￥【】「」]+"</span>, <span class="string">"我爱你，是的！"</span>)</span><br><span class="line"><span class="comment">#&lt;re.Match object; span=(0, 7), match='我爱你，是的！'&gt;</span></span><br></pre></td></tr></tbody></table></figure>
<p><strong>在模式中获取特定信息</strong><br>我们能用 <code>group</code> 和<code>()</code>获取到特定位置的信息。只要我们在正则表达中，加入一个 <code>()</code> 选定要截取返回的位置， 他就直接返回括号里的内容。<br></p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment">## re.finditer提供了 file.group(0) 这种全匹配的信息，而re.findall未提供全匹配信息。</span></span><br><span class="line">string = <span class="string">"I have 2021-02-01.jpg, 2021-02-02.jpg, 2021-02-03.jpg"</span></span><br><span class="line"><span class="keyword">match</span> = re.finditer(<span class="string">r"(\d+?)-(\d+?)-(\d+?)\.jpg"</span>, string)</span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> <span class="keyword">match</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"matched string:"</span>, file.group(<span class="number">0</span>), <span class="string">",year:"</span>, file.group(<span class="number">1</span>),</span><br><span class="line">    <span class="string">", month:"</span>, file.group(<span class="number">2</span>), <span class="string">", day:"</span>, file.group(<span class="number">3</span>))</span><br><span class="line"><span class="comment"># &gt; matched string: 2021-02-01.jpg ,year: 2021 , month: 02 , day: 01</span></span><br><span class="line"><span class="comment"># &gt; matched string: 2021-02-02.jpg ,year: 2021 , month: 02 , day: 02</span></span><br><span class="line"><span class="comment"># &gt; matched string: 2021-02-03.jpg ,year: 2021 , month: 02 , day: 03</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">match</span> = re.findall(<span class="string">r"(\d+?)-(\d+?)-(\d+?)\.jpg"</span>, string)</span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> <span class="keyword">match</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"year:"</span>, file[<span class="number">0</span>], <span class="string">", month:"</span>, file[<span class="number">1</span>], <span class="string">", day:"</span>, file[<span class="number">2</span>])</span><br><span class="line"><span class="comment"># &gt; year: 2021 , month: 02 , day: 01</span></span><br><span class="line"><span class="comment"># &gt; year: 2021 , month: 02 , day: 02</span></span><br><span class="line"><span class="comment"># &gt; year: 2021 , month: 02 , day: 03</span></span><br></pre></td></tr></tbody></table></figure><br><strong>名字索引</strong><br>有时候 <code>group</code> 的信息太多了，括号写得太多，依靠数字位置索引识别信息容易错配？此时，我们还能用一个名字来索引匹配好的字段， 然后用 group(“索引”) 的方式获取到对应的片段。注意，<code>re.findall</code> 不提供名字索引的方法， <code>re.search</code>或者 <code>re.finditer</code> 可以用名字索引。为了索引，我们需要在括号中写上 <code>?P&lt;索引名&gt;</code> 这种模式。<br><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">match</span> = re.finditer(<span class="string">r"(?P&lt;y&gt;\d+?)-(?P&lt;m&gt;\d+?)-(?P&lt;d&gt;\d+?)\.jpg"</span>, string)</span><br><span class="line"><span class="keyword">for</span> file <span class="keyword">in</span> <span class="keyword">match</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"matched string:"</span>, file.group(<span class="number">0</span>), </span><br><span class="line">        <span class="string">", year:"</span>, file.group(<span class="string">"y"</span>), </span><br><span class="line">        <span class="string">", month:"</span>, file.group(<span class="string">"m"</span>), </span><br><span class="line">        <span class="string">", day:"</span>, file.group(<span class="string">"d"</span>))</span><br></pre></td></tr></tbody></table></figure><br><strong>多模式匹配</strong><br>在正则中还有一些特别的 <code>flags</code>，可以在 <strong><code>re.search</code>,<code>re.match()</code>,<code>re.findall()</code></strong> 等功能中使用。 主要目的也是方便我们编写正则，和用更简单的方法处理更复杂的表达式。<br>| 模式  |        全称        |                               说明                               |<br>| :—-: | :————————: | :———————————————————————————————: |<br>| re.I  |   re.IGNORECASE    |                            忽略大小写                            |<br>| re.M  |    re.MULTILINE    |                   多行模式，改变’^’和’$’的行为                   |<br>| re.S  |     re.DOTALL      |       点任意匹配模式，改变’.’的行为, 使”.“可以匹配任意字符       |<br>| re.L  |     re.LOCALE      |        使预定字符类 \w \W \b \B \s \S 取决于当前区域设定         |<br>| re.U  |     re.UNICODE     | 使预定字符类 \w \W \b \B \s \S \d \D 取决于unicode定义的字符属性 |<br>| re.X  | re.VERBOSE详细模式 |  这个模式下正则表达式可以是多行，忽略空白字符，并可以加入注释。  |<br><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 用 ^ran 固定样式开头，我是匹配不到第二行的 ran to you 的，所以我们得加上一个 re.M flag。 </span></span><br><span class="line"><span class="comment"># 注意我们提到过的 re.search() 和 re.match() 不一样，re.match() 是不管有没有 re.M flag，我的匹配都是按照最头上开始匹配。 </span></span><br><span class="line"><span class="comment">#所以在下面的实验中，re.match() 匹配不到任何东西。</span></span><br><span class="line">ptn = <span class="string">r"^ran"</span></span><br><span class="line">string = <span class="string">"""I</span></span><br><span class="line"><span class="string">ran to you"""</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"without re.M:"</span>, re.search(ptn, string))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"with re.M:"</span>, re.search(ptn, string, flags=re.M))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"with re.M and match:"</span>, re.<span class="keyword">match</span>(ptn, string, flags=re.M))</span><br><span class="line"><span class="comment"># &gt; without re.M: None</span></span><br><span class="line"><span class="comment"># &gt; with re.M: &lt;re.Match object; span=(2, 5), match='ran'&gt;</span></span><br><span class="line"><span class="comment"># &gt; with re.M and match: None</span></span><br></pre></td></tr></tbody></table></figure><br><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="comment"># 如果你想用多种 flags，也是可以的，</span></span><br><span class="line"><span class="comment">#&gt; 比如我想同时用 re.M, re.I，你只需要这样书写re.M|re.I：</span></span><br><span class="line">ptn = <span class="string">r"^ran"</span></span><br><span class="line">string = <span class="string">"""I</span></span><br><span class="line"><span class="string">Ran to you"""</span></span><br><span class="line"><span class="built_in">print</span>(re.search(ptn, string, flags=re.M|re.I))</span><br><span class="line"><span class="built_in">print</span>(re.search(<span class="string">r"(?im)^ran"</span>, string))</span><br><span class="line">re.search(<span class="string">r"(?im)^ran"</span>, string)</span><br><span class="line"><span class="comment"># &gt; &lt;re.Match object; span=(2, 5), match='Ran'&gt;</span></span><br></pre></td></tr></tbody></table></figure><br>小案例<br>这个问题就是针对文件夹内所有的文件，将里面提到的 huangming98.github.io 改成 huangm.cn。<br>首先需要简化问题，简化出来的话，其实就是要有三个步骤。<p></p>
<ol>
<li>遍历所有的文本文件</li>
<li>读取文件中文本字段</li>
<li>替换掉特定字段</li>
</ol>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment">#找到所有的文本并遍历</span></span><br><span class="line"><span class="keyword">for</span> filename <span class="keyword">in</span> os.listdir(<span class="string">"files"</span>):</span><br><span class="line">    file_path = os.path.join(<span class="string">"files"</span>, filename)</span><br><span class="line">    <span class="comment">#读取文件中文本字段</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">"r"</span>) <span class="keyword">as</span> f1:</span><br><span class="line">        string = f1.read()</span><br><span class="line">        <span class="comment">#替换掉特定字段</span></span><br><span class="line">        new_string = re.sub(<span class="string">r"huangming98.github.io"</span>, <span class="string">"huangm.cn"</span>, string)</span><br><span class="line">        <span class="comment">#测试验证；将原文本替换过的文字拷贝到新文件中，原文件不改变</span></span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(<span class="string">"files"</span>, <span class="string">"new_"</span>+filename), <span class="string">"w"</span>) <span class="keyword">as</span> f2:</span><br><span class="line">            f2.write(new_string)</span><br><span class="line"></span><br><span class="line"><span class="comment">#判断操作是否正确</span></span><br><span class="line"><span class="keyword">for</span> filename <span class="keyword">in</span> os.listdir(<span class="string">"files"</span>):</span><br><span class="line">    <span class="keyword">if</span> filename.startswith(<span class="string">"new_"</span>):</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    file_path = os.path.join(<span class="string">"files"</span>, <span class="string">"new_"</span>+filename)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="built_in">print</span>(file_path, <span class="string">": "</span>, f.read())</span><br></pre></td></tr></tbody></table></figure>
<p>&nbsp;</p>
<h3 id="3-3序列化数据传输（Json-and-Pickle）"><a href="#3-3序列化数据传输（Json-and-Pickle）" class="headerlink" title="3.3序列化数据传输（Json and Pickle）"></a>3.3序列化数据传输（Json and Pickle）</h3><p>  序列化（Serialization）：把不同类型的数据打包保存在电脑硬盘中，或者用于数据的传输（云服务）。</p>
<p><strong>Pickle</strong><br></p><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">data = {<span class="string">"filename"</span>: <span class="string">"f1.txt"</span>, <span class="string">"create_time"</span>: <span class="string">"today"</span>, <span class="string">"size"</span>: <span class="number">111</span>}</span><br><span class="line">    pickle.dumps(datas)           <span class="comment"># 将python对象编码成Bytes字符串</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">"data.pkl"</span>, <span class="string">"wb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    pickle.dump(data, f)           <span class="comment"># pickle打包到文件data.pkl里</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">"data.pkl"</span>, <span class="string">"rb"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    data = pickle.load(f)        <span class="comment"># 用 open 的方式把文件都读出来，然后再用 pickle 对其解析</span></span><br><span class="line">os.listdir()</span><br><span class="line"><span class="built_in">print</span>(data)</span><br></pre></td></tr></tbody></table></figure><br><strong>Json</strong><br>  JSON （JavaScript Object Notation）是用于存储和交换数据的语法。最初是用 JavaScript 对象表示法编写的文本，但随后成为了一种常见格式，被包括Python在内的众多语言采用。<br><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line">data = {<span class="string">"filename"</span>: <span class="string">"f1.txt"</span>, <span class="string">"create_time"</span>: <span class="string">"today"</span>, <span class="string">"size"</span>: <span class="number">111</span>}</span><br><span class="line">    json.dumps(data)             <span class="comment"># 将python对象编码成json字符串（字符串）</span></span><br><span class="line">    json.loads(data)             <span class="comment"># 将Json字符串解码成python对象</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">"data.json"</span>, <span class="string">"w"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    json.dump(data, f)             <span class="comment"># 将python中的对象转化成json储存到文件f中（文件流）</span></span><br><span class="line"> </span><br><span class="line"><span class="built_in">print</span>(<span class="string">"直接当纯文本读："</span>)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">"data.json"</span>, <span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    <span class="built_in">print</span>(f.read())</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"用 json 加载了读："</span>)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">"data.json"</span>, <span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">    new_data = json.load(f)        <span class="comment"># 加载json格式数据</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"字典读取："</span>, new_data[<span class="string">"filename"</span>])</span><br></pre></td></tr></tbody></table></figure><br>json.dumps()的一些参数 <strong>（重点）</strong><br>  因为dumps编码以后的json格式输出比较的紧凑，如果不止一行看起来就不是特别好看，就像一堆乱码似的。所以，就推出了一些可选参数来让json码的可读性更高。<br><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">json.dumps(obj, sort_keys=<span class="literal">False</span>, skipkeys=<span class="literal">False</span>, ensure_ascii=<span class="literal">True</span>, check_circular=<span class="literal">True</span>, </span><br><span class="line">allow_nan=<span class="literal">True</span>, cls=<span class="literal">None</span>, indent=<span class="literal">None</span>, separators=<span class="literal">None</span>, </span><br><span class="line">encoding=<span class="string">"utf-8"</span>, default=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">obj:就是你要转化成json的对象。</span><br><span class="line">sort_keys =<span class="literal">True</span>:是告诉编码器按照字典排序(a到z)输出。如果是字典类型的python对象，就把关键字按照字典排序。</span><br><span class="line">indent:参数根据数据格式缩进显示，读起来更加清晰。</span><br><span class="line">        {<span class="string">"name"</span>: <span class="string">"\u4f60\u731c"</span>, <span class="string">"age"</span>: <span class="number">19</span>, <span class="string">"city"</span>: <span class="string">"\u56db\u5ddd"</span>} 变为</span><br><span class="line">        {</span><br><span class="line">        <span class="string">"name"</span>: <span class="string">"\u4f60\u731c"</span>,</span><br><span class="line">        <span class="string">"age"</span>: <span class="number">19</span>,</span><br><span class="line">        <span class="string">"city"</span>: <span class="string">"\u56db\u5ddd"</span></span><br><span class="line">        }</span><br><span class="line">separators=(<span class="string">','</span>,<span class="string">':'</span>):是分隔符的意思，参数意思分别为不同<span class="built_in">dict</span>项之间的分隔符和<span class="built_in">dict</span>项内key和value之间的分隔符，把：和，后面的空格都除去了。</span><br><span class="line">    {<span class="string">"name"</span>: <span class="string">"\u4f60\u731c"</span>, <span class="string">"age"</span>: <span class="number">19</span>, <span class="string">"city"</span>: <span class="string">"\u56db\u5ddd"</span>}</span><br><span class="line">    {<span class="string">"name"</span>:<span class="string">"\u4f60\u731c"</span>,<span class="string">"age"</span>:<span class="number">19</span>,<span class="string">"city"</span>:<span class="string">"\u56db\u5ddd"</span>}</span><br><span class="line">skipkeys：默认值是<span class="literal">False</span>，如果<span class="built_in">dict</span>的keys内的数据不是python的基本类型(<span class="built_in">str</span>,unicode,<span class="built_in">int</span>,long,<span class="built_in">float</span>,<span class="built_in">bool</span>,<span class="literal">None</span>)，设置为<span class="literal">False</span>时，就会报TypeError错误。此时设置成<span class="literal">True</span>，则会跳过这类key。</span><br><span class="line">ensure_ascii=<span class="literal">True</span>：默认输出ASCLL码，如果把这个该成<span class="literal">False</span>,就可以输出中文。</span><br><span class="line">check_circular：如果check_circular为false，则跳过对容器类型的循环引用检查，循环引用将导致溢出错误(或更糟的情况)。</span><br><span class="line"></span><br><span class="line">allow_nan：如果allow_nan为假，则ValueError将序列化超出范围的浮点值(nan、inf、-inf)，严格遵守JSON规范，而不是使用JavaScript等价值(nan、Infinity、-Infinity)。</span><br><span class="line"></span><br><span class="line">default：default(obj)是一个函数，它应该返回一个可序列化的obj版本或引发类型错误。默认值只会引发类型错误。</span><br></pre></td></tr></tbody></table></figure><p></p>
<p><strong>Pickle 与 Json 对比如下：</strong><br>|     对比     |            Pickle            |                    Json                     |<br>| :—————: | :—————————————: | :————————————————————-: |<br>|   存储格式   |   Python 特定的 Bytes 格式   | 通用 JSON text 格式，可用于常用的网络通讯中 |<br>|   数据种类   | 类，功能，字典，列表，元组等 |    基本和 Pickle 一样，但不能存类，功能     |<br>| 保存后可读性 |         不能直接阅读         |                 能直接阅读                  |<br>|   跨语言性   |       只能用在 Python        |              可以跨多语言读写               |<br>|   处理时间   |       长（需编码数据）       |               短（不需编码）                |<br>|    安全性    |  不安全（除非你信任数据源）  |                  相对安全                   |</p>
<p>&nbsp;</p>
<h1 id="python-相关连接"><a href="#python-相关连接" class="headerlink" title="python 相关连接"></a>python 相关连接</h1><h2 id="1-python基础"><a href="#1-python基础" class="headerlink" title="1. python基础"></a>1. <a href="https://www.runoob.com/python3/python3-tutorial.html">python基础</a></h2><h2 id="2-python100天"><a href="#2-python100天" class="headerlink" title="2. python100天"></a>2. <a href="https://www.bookstack.cn/read/Python-100-Days/README.md">python100天</a></h2><h2 id="3-虚拟环境配置"><a href="#3-虚拟环境配置" class="headerlink" title="3.虚拟环境配置"></a>3.虚拟环境配置</h2><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">conda create -n rich python=3.7  <span class="comment">#创建环境</span></span><br><span class="line">conda activate rich  <span class="comment">#激活环境</span></span><br><span class="line">conda deactivate rich  <span class="comment">#退出环境</span></span><br><span class="line">conda remove -n rich --all  <span class="comment">#删除环境</span></span><br></pre></td></tr></tbody></table></figure>
<h2 id="4-requirements"><a href="#4-requirements" class="headerlink" title="4. requirements"></a>4. requirements</h2><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></tbody></table></figure>
<p>例如<br></p><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">pytest==6.2.5</span><br><span class="line">pytest-json-report==1.4.1</span><br><span class="line">pytest-metadata==1.11.0</span><br><span class="line">pytest-ordering==0.6</span><br><span class="line">PyTestReport==0.2.1</span><br><span class="line">python-dateutil==2.8.2</span><br></pre></td></tr></tbody></table></figure><p></p>
<ul>
<li><a href="https://github.com/liuqian62/loopmark">python图形界面示例</a><h2 id="5-mmdetetion"><a href="#5-mmdetetion" class="headerlink" title="5. mmdetetion"></a>5. mmdetetion</h2></li>
</ul>
<h3 id="5-1-教程"><a href="#5-1-教程" class="headerlink" title="5.1 教程"></a>5.1 教程</h3><ul>
<li><a href="https://blog.csdn.net/qq_16137569/article/details/121316235">博客链接</a></li>
<li><a href="https://github.com/open-mmlab/mmdetection">官网链接</a></li>
<li><a href="https://mmdetection.readthedocs.io/en/latest/">官方文档</a><h3 id="5-2-测试程序用时"><a href="#5-2-测试程序用时" class="headerlink" title="5.2 测试程序用时"></a>5.2 测试程序用时</h3></li>
</ul>
<figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">python -m torch.distributed.launch --nproc_per_node=1 --master_port=29500 tools/analysis_tools/benchmark.py <span class="variable">$cofig</span> <span class="variable">$checkpoint</span> --launcher pytorch</span><br></pre></td></tr></tbody></table></figure>
<h3 id="5-3-分布式训练"><a href="#5-3-分布式训练" class="headerlink" title="5.3 分布式训练"></a>5.3 分布式训练</h3><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">Export CUDA_VISIBLE_DEVICES=”<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>”</span><br><span class="line"></span><br><span class="line">CUDA_VISIBLE_DEVICES=”<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>” python -m torch.distributed.launch --nproc_per_node=<span class="number">4</span> tools/train.py $cofig $checkpoint --launcher pytorch</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h3 id="5-4-训练"><a href="#5-4-训练" class="headerlink" title="5.4 训练"></a>5.4 训练</h3><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">Export CUDA_VISIBLE_DEVICES=<span class="number">2</span></span><br><span class="line"></span><br><span class="line">CUDA_VISIBLE_DEVICES=<span class="number">2</span> python tools/train.py $cofig $checkpoint --gpus <span class="number">2</span></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h3 id="5-5-测算模型参数量和计算量"><a href="#5-5-测算模型参数量和计算量" class="headerlink" title="5.5 测算模型参数量和计算量"></a>5.5 测算模型参数量和计算量</h3><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">python tools/analysis_tools/get_flops.py ${CONFIG_FILE} [--shape ${INPUT_SHAPE}]</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h3 id="5-6-指定GPU运行"><a href="#5-6-指定GPU运行" class="headerlink" title="5.6 指定GPU运行"></a>5.6 指定GPU运行</h3><figure class="highlight plaintext"><table><tbody><tr><td class="code"><pre><span class="line">export CUDA_VISIBLE_DEVICES=1 </span><br><span class="line"></span><br><span class="line">CUDA_VISIBLE_DEVICE=1 python xxx.py …</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h3 id="5-7-Git"><a href="#5-7-Git" class="headerlink" title="5.7. Git"></a>5.7. Git</h3><figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">git init (初始化本地仓库)</span><br><span class="line">git remote add origin git@github.com:<span class="built_in">open</span>-mmlab/mmdetection.git</span><br><span class="line">git add .  (提交缓存)</span><br><span class="line">git status</span><br><span class="line">git commit -m “   备注   ”</span><br><span class="line"></span><br><span class="line">git pull --rebase origin master (同步)</span><br><span class="line"><span class="keyword">or</span>   git pull + git pull origin master --allow-unrelated-histories 【允许不相关历史提交，并强制合并】</span><br><span class="line">git push -u origin master （推送）</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<ul>
<li>冲突</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="code"><pre><span class="line">git add . </span><br><span class="line"></span><br><span class="line">git rebase --<span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">git pull --rebase origin master </span><br><span class="line"></span><br><span class="line">git push origin master</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure>
<h2 id="6-OpenCV教程"><a href="#6-OpenCV教程" class="headerlink" title="6. OpenCV教程"></a>6. OpenCV教程</h2><ul>
<li><a href="https://woshicver.com/">OpenCV中文官方文档</a></li>
<li><a href="https://opencv.apachecn.org/#/">OpenCV 4.0 中文文档</a></li>
</ul>
<h2 id="7-pyechart"><a href="#7-pyechart" class="headerlink" title="7. pyechart"></a>7. pyechart</h2><ul>
<li><a href="https://gallery.pyecharts.org/#/README">pyechart中文文档</a></li>
<li><a href="https://github.com/pyecharts/pyecharts">pyechart的github地址</a></li>
<li><a href="https://github.com/pyecharts/pyecharts-gallery">pyechart图库</a></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>Practical English Writing Course Notes</title>
    <url>/2024/05/25/Practical-English-Writing-Course-Notes/</url>
    <content><![CDATA[<h2 id="1-write-base"><a href="#1-write-base" class="headerlink" title="1. write base"></a>1. write base</h2><h3 id="1-1-Paragraph-analysis-and-write"><a href="#1-1-Paragraph-analysis-and-write" class="headerlink" title="1.1 Paragraph analysis and write:"></a>1.1 Paragraph analysis and write:</h3><p><strong>Topic sentence</strong>:<br>Purpose:to tell readers the main idea of the paragraph(what will be discussed); Components:topic(the general subject) + controlling idea(the specific aspect of the subject it focuses on).</p>
<p><strong>Supporting sentences</strong>: to develop the main idea; through details, facts, examples, reasons, etc. <em>unity of ideas</em>: no irrelevant sentences &amp; connectors.</p>
<p><strong>Concluding sentence</strong>: to bring the paragraph to an end restatement, summary, &amp; result <em>signal:</em> finally, thus, therefore, etc.More general than the supporting sentences.<br><img src="/2024/05/25/Practical-English-Writing-Course-Notes/2024-05-23-16-59-19.png" alt=""></p>
<h3 id="1-2-Process-Essay-analysis-and-write"><a href="#1-2-Process-Essay-analysis-and-write" class="headerlink" title="1.2 Process Essay  analysis and write:"></a>1.2 Process Essay  analysis and write:</h3><p><img src="/2024/05/25/Practical-English-Writing-Course-Notes/2024-05-23-18-56-49.png" alt=""><br><img src="/2024/05/25/Practical-English-Writing-Course-Notes/2024-05-23-18-56-36.png" alt=""></p>
<p><strong>Introduction</strong></p>
<ul>
<li>to introduce the topic</li>
<li>to give readers the background information </li>
<li>to <strong>catch</strong> readers’ interest</li>
</ul>
<p><strong>concluding paragraph</strong></p>
<ul>
<li><strong>Summarize</strong> the main points.</li>
<li><strong>Restate</strong> the thesis statement to emphasize the main idea.</li>
<li>Add some <strong>comments</strong>: making a predication, giving advice, or showing results of the situation/topic.</li>
<li>Do not add new information.</li>
<li>Do not review only some of the information in your essay.</li>
</ul>
<h2 id="2-sentence-skills"><a href="#2-sentence-skills" class="headerlink" title="2. sentence skills"></a>2. sentence skills</h2><h3 id="2-1-Basic-sentence-skills"><a href="#2-1-Basic-sentence-skills" class="headerlink" title="2.1 Basic sentence skills"></a>2.1 Basic sentence skills</h3><p><strong>Run-ons</strong> (标点符号不能连接两个句子，不规范，连词可以)<br>[<span class="github-emoji"><span>❌</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/274c.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>]The bus stopped suddenly，I spilled coffee all over my skirt.<br>[<span class="github-emoji"><span>✅</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>]The bus stopped suddenly, <strong>and</strong> I spilled coffee all over my skirt.  </p>
<p><strong>Subject-Verb agreement</strong> (主谓一致和就近原则)</p>
<ol>
<li>An old chair with broken legs (<strong>has</strong>, have) sat in our basement for years. </li>
<li>There (<strong>is</strong>, are)  a pen, two books and many pencils on the desk.</li>
<li>Not only my teachers but also my wife (encourage, <strong>encourages</strong>) me to work hard in college. </li>
<li>Neither of those websites (<strong>is</strong>,  are) credible.</li>
</ol>
<p><strong>Pronoun agreement and reference</strong></p>
<ol>
<li>Alicia showed me <strong>her</strong> antique wedding band. </li>
<li>Students enrolled in the art class must provide <strong>their</strong> own tools.</li>
<li>Kia is really a shy person, but she keeps <strong>it</strong> hidden. (refer her shyness<br>)</li>
</ol>
<p><strong>Misplaced modifiers:</strong> Words that do not describe what the  writer intended them to describe due to awkward placement(place words as close as possible to what they describe)<br>[<span class="github-emoji"><span>❌</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/274c.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>]David  couldn’t  drive to work in his small sports car with a broken leg.<br>[<span class="github-emoji"><span>✅</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2705.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>]With a broken leg, David couldn’t drive to work in his small sports car.</p>
<h3 id="2-2-Combining-Sentences-with-Coordinating-Conjunctions"><a href="#2-2-Combining-Sentences-with-Coordinating-Conjunctions" class="headerlink" title="2.2 Combining Sentences with Coordinating Conjunctions"></a>2.2 Combining Sentences with Coordinating Conjunctions</h3><p><u>Clause 1</u>, <em>conjunction</em> <u>clause 2</u></p>
<p><img src="/2024/05/25/Practical-English-Writing-Course-Notes/2024-05-23-17-27-03.png" alt=""></p>
<h3 id="2-3-Combining-Sentences-with-Transitions"><a href="#2-3-Combining-Sentences-with-Transitions" class="headerlink" title="2.3 Combining Sentences with Transitions"></a>2.3 Combining Sentences with Transitions</h3><p><strong>Linking transitions</strong> are words or expressions that connect independent (main) clauses together.<br><img src="/2024/05/25/Practical-English-Writing-Course-Notes/2024-05-23-18-37-40.png" alt=""><br>eg:These people can communicate well through body language <strong>;  in addition,</strong> they have a good sense of balance and hand-eye coordination. </p>
<p><img src="/2024/05/25/Practical-English-Writing-Course-Notes/2024-05-23-18-39-44.png" alt=""></p>
<p><strong>Do not use a SEMICOLON if you do not have a complete clause after the linking transition.</strong><br>[<span class="github-emoji"><span>❌</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/274c.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>]Many people use a mother’s or grandmother’s cure for a cold;  for example, chicken soup. </p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">sig</th>
<th style="text-align:center">english</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">,</td>
<td style="text-align:center">comma</td>
</tr>
<tr>
<td style="text-align:center">.</td>
<td style="text-align:center">full stop/ period</td>
</tr>
<tr>
<td style="text-align:center">:</td>
<td style="text-align:center">colon</td>
</tr>
<tr>
<td style="text-align:center">;</td>
<td style="text-align:center">semi-colon</td>
</tr>
<tr>
<td style="text-align:center">!</td>
<td style="text-align:center">exclamation mark</td>
</tr>
<tr>
<td style="text-align:center">“  ”</td>
<td style="text-align:center">quotation mark</td>
</tr>
<tr>
<td style="text-align:center">X’s</td>
<td style="text-align:center">apostrophe</td>
</tr>
<tr>
<td style="text-align:center">?</td>
<td style="text-align:center">question mark</td>
</tr>
<tr>
<td style="text-align:center">w-w</td>
<td style="text-align:center">hyphen/ splash</td>
</tr>
<tr>
<td style="text-align:center">(  )</td>
<td style="text-align:center">parenthesis/ bracket</td>
</tr>
</tbody>
</table>
</div>
<h3 id="2-4-Combining-clauses-with-subordinating-conjunctions"><a href="#2-4-Combining-clauses-with-subordinating-conjunctions" class="headerlink" title="2.4 Combining clauses with subordinating conjunctions"></a>2.4 Combining clauses with subordinating conjunctions</h3><p><strong>As soon as</strong> he said that <strong>,</strong> Rosa stood up.<br>Peter almost <em>knocked</em> me down <strong>before</strong> he <em>saw</em> me.<br>| Subordinator |           Meaning            |                                 Example                                 |<br>| :—————: | :—————————————: | :——————————————————————————————————-: |<br>|    After     |          later than          |       He travelled to England <strong>after</strong> he wrote his first book.        |<br>|    Before    |         earlier than         |           Peter almost knocked me down <strong>before</strong> he saw me.            |<br>|    Until     |       up to that time        |                  Let’s wait <strong>until</strong> the rain stops.                   |<br>|    Since     |        from that time        |   I have been practicing yoga <strong>since</strong> I took a yoga course in 2020.   |<br>|    While     |       during that time       |             My father fell asleep <strong>while</strong> he was reading.             |<br>|     When     |     at/during that time      |           <strong>When</strong> he came in, I was listening to the radio.            |<br>|   Whenever   | at any time that/ every time |     You can take a short relaxation break <strong>whenever</strong> you need it.     |<br>|      As      |      immediately after       | <strong>As</strong> the president spoke, the people in the audience listened to him. |<br>|  As soon as  |                              |          The atmosphere changed <strong>as soon as</strong> Mary walked in.          |</p>
<h3 id="2-5-A-summary-of-connectors"><a href="#2-5-A-summary-of-connectors" class="headerlink" title="2.5 A summary of connectors"></a>2.5 A summary of connectors</h3><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">title</th>
<th style="text-align:center">Connectors indicating additional information</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Coordinating conjunction</td>
<td style="text-align:center">and</td>
</tr>
<tr>
<td style="text-align:center">Transition</td>
<td style="text-align:center">in addition, additionally, furthermore, moreover, also, besides</td>
</tr>
<tr>
<td style="text-align:center">Correlative conjunctions</td>
<td style="text-align:center">not only…but also</td>
</tr>
<tr>
<td style="text-align:center">Prepositional phrases</td>
<td style="text-align:center">in addition to, along with, as well as, besides, apart from</td>
</tr>
</tbody>
</table>
</div>
<p>eg1: High-level positions are stressful at times <strong>; furthermore,   (.Furthermore,)</strong> they can be harmful to your health.<br>eg2: <strong>Along with</strong> being stressful at times, high-level positions can also be harmful to your health. </p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">title</th>
<th style="text-align:center">Connectors indicating sequence, time, and order</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Transition</td>
<td style="text-align:center">first, second, next, to start/ begin with, at the outset, at the commencement, in the first/second place, at this point, meanwhile, in the meantime, subsequently, later, then, ultimately, finally, lastly</td>
</tr>
<tr>
<td style="text-align:center">Subordinating conjunction</td>
<td style="text-align:center">before, after, until, since, while, when, whenever, as, as soon as</td>
</tr>
<tr>
<td style="text-align:center">Prepositional phrase</td>
<td style="text-align:center">in, at, on, for, during, within, from, prior to, previous to, before, after</td>
</tr>
<tr>
<td style="text-align:center">Prepositional phrases</td>
<td style="text-align:center">in addition to, along with, as well as, besides, apart from</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">title</th>
<th style="text-align:center">Connectors indicating cause and effect</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Coordinating conjunction</td>
<td style="text-align:center">for, so</td>
</tr>
<tr>
<td style="text-align:center">Transition</td>
<td style="text-align:center">therefore, hence, as a result, consequently, accordingly</td>
</tr>
<tr>
<td style="text-align:center">Subordinating conjunction</td>
<td style="text-align:center">because, as, since, if (result of the condition)</td>
</tr>
<tr>
<td style="text-align:center">Prepositional phrases</td>
<td style="text-align:center">due to, because of, as a result of, owing to, on account of, for the sake of, for the purpose of, in the wake of, be the reason for, be responsible for</td>
</tr>
<tr>
<td style="text-align:center">Verbal/Noun/Adverb phrase</td>
<td style="text-align:center">contribute to, result from, cause, lead to, result in, thereby(adv)</td>
</tr>
<tr>
<td style="text-align:center">-ing clauses of result</td>
<td style="text-align:center">The carcinogenic substances are extracted from the soil, leaving the soil uncontaminated.</td>
</tr>
</tbody>
</table>
</div>
<p>eg1: We increased our prices, <strong>for</strong> the cost of materials rose sharply last year.<br>eg2: The cost of materials rose sharply last year. <strong>Accordingly,</strong> we were forced to increase our prices.<br>eg3: Many animals lost their habitats <strong>due to</strong> the wildfires.<br>eg4: Environmental education <strong>contributes to</strong> people’s increasing green thinking in China.<br>eg5: Regular exercise strengthens the heart, <strong>thereby</strong> reducing the risk of heart attack.<br>eg6: The carcinogenic substances (致癌物) are extracted from the soil <strong>; hence,</strong> the soil is left uncontaminated (未被污染的).<br>eg7: The carcinogenic substances are extracted from the soil <strong>, leaving</strong> the soil uncontaminated.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">title</th>
<th style="text-align:center">Connectors indicating comparison and contrast</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Coordinating conjunction</td>
<td style="text-align:center">and (comparison/similarity) but, yet (contrast/difference)</td>
</tr>
<tr>
<td style="text-align:center">Transition</td>
<td style="text-align:center">similarly, likewise, also, too however, nevertheless, in contrast, on the contrary, conversely</td>
</tr>
<tr>
<td style="text-align:center">Subordinating conjunction</td>
<td style="text-align:center">as, while, whereas,although</td>
</tr>
<tr>
<td style="text-align:center">Prepositional/noun/verb phrases</td>
<td style="text-align:center">like, alike, similar to, the same as, similarity, share… in common, unlike, dissimilar to, differ, different from (in), difference, notwithstanding,  despite, in spite of, instead</td>
</tr>
</tbody>
</table>
</div>
<p>eg1. Most men tend to feel comfortable with “public” speaking, <strong>while/whereas</strong> most women enjoy “private” speaking.<br>eg2. Many environmental problems result from air pollutants <strong>, such as</strong> sulphur dioxide and carbon dioxide.</p>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">title</th>
<th style="text-align:center">Connectors indicating exemplification</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Transition</td>
<td style="text-align:center">for example, for instance</td>
</tr>
<tr>
<td style="text-align:center">Prepositional/verb phrase/other</td>
<td style="text-align:center">such as, like, a prime/good example of, take… for example one case in point is that…</td>
</tr>
<tr>
<td style="text-align:center">Correlative conjunctions</td>
<td style="text-align:center">not only…but also</td>
</tr>
<tr>
<td style="text-align:center">Prepositional phrases</td>
<td style="text-align:center">in addition to, along with, as well as, besides, apart from</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">title</th>
<th style="text-align:center">Connectors indicating summary</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">Transition</td>
<td style="text-align:center">in summary, in short, in a nutshell, in a word, in simple terms</td>
</tr>
<tr>
<td style="text-align:center">Prepositional/ noun/verb phrases</td>
<td style="text-align:center">to sum up, to make a long story short, to put it in a nutshell, to put it crudely, summarize</td>
</tr>
<tr>
<td style="text-align:center">Correlative conjunctions</td>
<td style="text-align:center">not only…but also</td>
</tr>
<tr>
<td style="text-align:center">Prepositional phrases</td>
<td style="text-align:center">in addition to, along with, as well as, besides, apart from</td>
</tr>
</tbody>
</table>
</div>
<h2 id="3-the-typical-structure-of-a-research-paper"><a href="#3-the-typical-structure-of-a-research-paper" class="headerlink" title="3 the typical structure of a research paper"></a>3 the typical structure of a research paper</h2><p><img src="/2024/05/25/Practical-English-Writing-Course-Notes/2024-05-24-14-43-36.png" alt=""></p>
<h3 id="3-1-Title"><a href="#3-1-Title" class="headerlink" title="3.1 Title"></a>3.1 Title</h3><p>All words in the title should be chosen with great care!<br>“the fewest possible words that adequately describe the contents of the paper”  (Gastel &amp; Day, 2016, p.41)</p>
<p><strong>Functions of Title</strong><br>(1) Generalize the research paper<br>(2) Attract appropriate readers<br>(3) Facilitate the retrieval<br><strong>Brief （简洁）</strong><br>11-15 words now being the most common length.</p>
<p><strong>Accurate (准确)</strong><br>Accurate, clear, and exact<br>Without any misunderstandings<br>Grammatically accurate (word order)</p>
<p><strong>Distinctive (区别性)</strong><br>Be specific and distinctive to distinguish it from other treatments of the general subjects.<br>The research design identification is essential. </p>
<h3 id="3-1-Abstract"><a href="#3-1-Abstract" class="headerlink" title="3.1 Abstract"></a>3.1 Abstract</h3><p><strong>Abstracts are highly promotional and designed to “hook the reader”.</strong></p>
<ol>
<li>summarize the important parts of the article (100-250 words)</li>
<li>arouse readers’ interest in reading the article</li>
<li>make journal editors want  to read and publish the article help readers remember key points from your paper</li>
<li>be accurate, nonevaluative, coherent and  readable (APA, 2010)</li>
</ol>
<p><strong>Five questions to answer in Abstract</strong> </p>
<ol>
<li>What is the <em>background</em> of the study in the research field? What <em>motivated</em> you to do this study? <strong>(Introduction:background and motivated)</strong> </li>
<li>What is the purpose of the study? What are the research questions?  <strong>(Purpose and significance)</strong></li>
<li>How were the data collected and analyzed? <strong>(Method:key skill\Method\idea\application)</strong></li>
<li>What were the results or major findings? <strong>(Result:Present main findings/results )</strong></li>
<li>What conclusion have you drawn? What is the <em>contribution</em> of the study to the existing body of knowledge? <strong>(Conclusion:gap/impact/solution/expand knowledge/inspire)</strong></li>
</ol>
<p><img src="/2024/05/25/Practical-English-Writing-Course-Notes/2024-05-24-15-08-26.png" alt="summary"></p>
<h3 id="3-2-Introduction"><a href="#3-2-Introduction" class="headerlink" title="3.2 Introduction"></a>3.2 Introduction</h3><h4 id="3-2-1-Introduction-structure"><a href="#3-2-1-Introduction-structure" class="headerlink" title="3.2.1 Introduction structure"></a>3.2.1 Introduction structure</h4><p><strong>Move 1: Establishing a territory</strong></p>
<ul>
<li>Step 1. Claiming centrality </li>
<li>Step 2. Topic generalizations of increasing specificity</li>
<li>Step 3. Reviewing items of previous literature </li>
</ul>
<p><strong>Move 2: Establishing a niche</strong></p>
<ul>
<li>Step 1A. Indicating a gap</li>
<li>Step 1B. Adding to what is known </li>
<li>Step 2.  Presenting positive justification (Optional) </li>
</ul>
<p><strong>Move 3: Presenting the present work</strong> </p>
<ul>
<li>Step 1. Announcing present research descriptively and/or purposively</li>
<li>Step 2. Presenting research questions or hypothesis (optional)</li>
<li>Step 3. Definitional clarifications (optional)</li>
<li>Step 4. Summarizing methods (optional)</li>
<li>Step 5. Announcing principle outcomes (Possible in some fields)</li>
<li>Step 6. Stating the value of the present research (Possible in some fields)</li>
<li>Step 7. Outlining the structure of the paper (Possible in some fields)</li>
</ul>
<h4 id="3-2-2-Claiming-centrality-Establishing-significance"><a href="#3-2-2-Claiming-centrality-Establishing-significance" class="headerlink" title="3.2.2 Claiming centrality/Establishing significance"></a>3.2.2 Claiming centrality/Establishing significance</h4><p><img src="/2024/05/25/Practical-English-Writing-Course-Notes/2024-05-24-15-21-05.png" alt=""></p>
<h4 id="3-2-3-Indicating-a-gap"><a href="#3-2-3-Indicating-a-gap" class="headerlink" title="3.2.3 Indicating a gap"></a>3.2.3 Indicating a gap</h4><ol>
<li><strong>However</strong>, research evaluating creeping bentgrass reflectance under water stress and varying nitrogen rates for correlation with turf quality …<strong>has not been documented</strong>. </li>
<li><strong>Nonetheless</strong>, data to assess the feasibility of extractive reserves are <strong>limited. Few studies</strong> of rainforest extraction have presented a quantitative ecological framework…  </li>
<li><strong>While</strong> this body of research has enriched our knowledge about peer interaction in peer feedback, <strong>scant attention has been paid to</strong> the specific strategies EFL students use during peer feedback to cope with the linguistic and cognitive challenges that arise during the peer interaction. </li>
<li>Furthermore, <strong>little is known about</strong> the boundary conditions of the relationship between economic strain and social cohesion, especially the role that….</li>
<li><strong>Few researchers have addressed the problem</strong> of… </li>
<li><strong>There remains a need for an efficient method</strong> that can… </li>
<li>The high absorbance makes this <strong>an impractical option</strong> in cases…</li>
<li><strong>Nonetheless</strong>, these methods <strong>do not sufficiently</strong> address …</li>
<li>These can be <strong>time-consuming and technically difficult</strong> to perform.</li>
<li><strong>Although</strong> this approach improves performance, it results in <strong>an unacceptable</strong> number of…</li>
<li>One <strong>primary problem</strong> with … is that…</li>
<li><strong>Notwithstanding</strong> the merits and proliferation of chatbots, their incorporation into writing classrooms <strong>remains insufficient</strong>.</li>
<li><strong>Nevertheless, there is a notable scarcity of research dedicated to investigating how</strong> students effectively compose text with chatbots.<br><img src="/2024/05/25/Practical-English-Writing-Course-Notes/2024-05-24-15-31-27.png" alt=""></li>
</ol>
<h4 id="3-2-4-Presenting-the-present-work"><a href="#3-2-4-Presenting-the-present-work" class="headerlink" title="3.2.4 Presenting the present work"></a>3.2.4 Presenting the present work</h4><p><img src="/2024/05/25/Practical-English-Writing-Course-Notes/2024-05-24-15-32-50.png" alt=""></p>
<h3 id="3-3-Review-of-the-literature"><a href="#3-3-Review-of-the-literature" class="headerlink" title="3.3 Review of the literature"></a>3.3 Review of the literature</h3><p>A literature review <strong>summarizes, interprets, and critically evaluates</strong> significant literature published on a specific topic.  </p>
<h4 id="3-3-1-Why-Literature-Review-matters"><a href="#3-3-1-Why-Literature-Review-matters" class="headerlink" title="3.3.1 Why Literature Review matters?"></a>3.3.1 Why Literature Review matters?</h4><ol>
<li><strong>Demonstrate</strong> your knowledge of the topic and scholarly context </li>
<li>Show <strong>respect to “giants”</strong> in your field</li>
<li>Develop a theoretical <strong>framework and methodology</strong> for your research</li>
<li><strong>Position yourself</strong> in relation to other researchers and theorists </li>
<li><strong>Situate</strong> your work within the social and disciplinary context</li>
<li>Prepare and <strong>justify the research questions or hypotheses</strong> of your study</li>
<li>Show how your research addresses <strong>a gap or contributes</strong> to a debate</li>
<li>Add <strong>authority</strong> to the present study  (by “quoting” from the authorities to support your study)</li>
</ol>
<h4 id="3-3-2-What-features-good-Literature-Review"><a href="#3-3-2-What-features-good-Literature-Review" class="headerlink" title="3.3.2 What features good Literature Review?"></a>3.3.2 What features good Literature Review?</h4><ol>
<li><strong>A coherent argument</strong> </li>
<li><strong>A critical evaluation</strong> of what has been done: summarising the findings of related studies and establishing gaps or weaknesses in the present knowledge to pave the way for new knowledge claims (Thompson, 2009)</li>
<li>The overall rhetorical organisation tends to follow the CARS model (Swales, 1990) of Introduction</li>
<li>A <em>separate</em> section/chapter or being <em>integrated</em> into Introduction<h4 id="3-3-3-The-processes-of-conducting-Literature-Review"><a href="#3-3-3-The-processes-of-conducting-Literature-Review" class="headerlink" title="3.3.3 The processes of conducting Literature Review"></a>3.3.3 The processes of conducting Literature Review</h4>Step 1. Clearly define topic &amp; research questions<br>Step 2. Search for literature using key words and reference lists<br>Step 3. Select and evaluate literature<br>Step 4. Analyze and summarize the literature<br>Step 5. Identify themes, debates, and gaps<br>Step 6. Outline your literature review’s structure</li>
</ol>
<ul>
<li>Chronological: trace the development of the topic over time; avoid simply listing and summarizing sources in order</li>
<li>Thematic: use subsections to address different aspects of the topic</li>
<li>Methodological: compare the results and conclusions that emerge from different approaches</li>
<li>Theoretical: argue for the relevance of a specific theoretical approach; create a framework for your research</li>
</ul>
<p>Step 7. Write your literature review</p>
<h3 id="3-4-Methodology"><a href="#3-4-Methodology" class="headerlink" title="3.4 Methodology"></a>3.4 Methodology</h3><h4 id="3-4-1-Purposes-of-Methodology"><a href="#3-4-1-Purposes-of-Methodology" class="headerlink" title="3.4.1 Purposes of Methodology"></a>3.4.1 Purposes of Methodology</h4><p>To present <strong>a clear and detailed description</strong> of how an experiment or an investigation was done<br>To provide careful instructions so that the process of your experiment or investigation can be easily <strong>visualized and replicated</strong> (Cotos et al., 2017)<br>To provide the information the reader needs to <strong>judge the study’s validity</strong> (Cargill &amp; O’Connor, 2009) </p>
<h4 id="3-4-2-Components-of-Methodology"><a href="#3-4-2-Components-of-Methodology" class="headerlink" title="3.4.2 Components of Methodology"></a>3.4.2 Components of Methodology</h4><p><strong>1.Generalization/ Introduction</strong><br>Gives an overview of the entire section<br>Provides necessary background information<br>Introduces the principal activity<br>Identifies &amp; justifies the methodological approach</p>
<p><strong>2. Methods/Procedures</strong><br>Rules of thumb for describing the methods/procedures<br>If the method has been published, provide the reference.<br>If the method is well-established, just mention its name.<br>If a new method is introduced, try to provide the detailed information precisely and concisely.<br>Questions of HOW and HOW MUCH should be answered precisely to make the procedures clear and replicable </p>
<p><strong>3. Data Analysis</strong><br>This part deals with the procedures of analyzing the data collected in the previous section and verify its validity. </p>
<p>For some discipline/journals, data analysis is integrated into the Results section. </p>
<h3 id="3-5-Results"><a href="#3-5-Results" class="headerlink" title="3.5 Results"></a>3.5 Results</h3><p><img src="/2024/05/25/Practical-English-Writing-Course-Notes/2024-05-24-15-56-11.png" alt=""></p>
<h4 id="3-5-1-Why-do-we-need-to-write-Results-in-text"><a href="#3-5-1-Why-do-we-need-to-write-Results-in-text" class="headerlink" title="3.5.1 Why do we need to write Results in text?"></a>3.5.1 Why do we need to write Results in text?</h4><ul>
<li><strong>To highlight our interesting and significant results</strong> </li>
<li>To talk about the results as <strong>a solution</strong> to the problems raised in Introduction</li>
<li><strong>Numbers don’t speak for themselves!</strong> We should tell the readers what these numbers mean.   </li>
</ul>
<h4 id="3-5-2-Example-REVIEW-COMMENT"><a href="#3-5-2-Example-REVIEW-COMMENT" class="headerlink" title="3.5.2 Example:REVIEW COMMENT"></a>3.5.2 Example:REVIEW COMMENT</h4><p>COMMENT1: At times this paper reads like a thesis. The authors seem to have included all their results, with the consequence that I am not sure which findings are significant and which are not. However, I also suspect that some contradictory findings have not been included. So although I generally recommend brevity, this should not include leaving out key findings that do not support the authors’ line of logic.</p>
<p>COMMENT2: Rather than highlighting the results that are significant or relevant, the authors have merely repeated in the text everything that they have put in their figures and tables, which seem to include every piece of data that the authors have elaborated in the last three years. This makes for very tedious reading. Moreover, I felt that I was not given the tools to understand for myself the significance of their data.</p>
<h4 id="3-5-3-A-model-to-structure-Results-section"><a href="#3-5-3-A-model-to-structure-Results-section" class="headerlink" title="3.5.3 A model to structure Results section"></a>3.5.3 A model to structure Results section</h4><p>Move 1.  Stating the preparatory information<br>1) Revisiting  the research aim/ existing research<br>2) Revisiting/expanding methodology<br>3) General overview of results </p>
<p>Move 2. Announcing results<br>1) Invitation to view results<br>2) Specify key results in detail, with or without explanations</p>
<p>Move 3.  Commenting on results<br>1) Compare results with other research<br>2) Compare results with model predictions<br>3) Problems with results<br>4) Possible implications of results </p>
<p><strong>Move 1. example sentence</strong></p>
<ul>
<li>The results are divided into two parts as follows: </li>
<li>It is apparent that in all/most/the majority of cases…</li>
<li>Using the method described above, we obtained …</li>
<li>Levels of weight loss were similar in all cases.</li>
<li>In general, coefficients for months close to the mean flowering data were negative.</li>
</ul>
<p><strong>Move 2. example sentence</strong><br>Invitation to review results </p>
<ul>
<li>The stress data in Fig. 18 indicate/demonstrate/ display/ reveal/suggest a more reasonable relationship.</li>
<li>Figure 3 illustrates/reports/presents/shows the findings of the spatial time activity modelling.</li>
<li>The results are summarised in Table 4.</li>
<li>The rate constants shown in Table 1 demonstrate that the reactivity is much greater at neutral pH.</li>
<li>As shown/detailed/listed/illustrated in Figure 1, … </li>
</ul>
<p>Presenting specific/key results in detail</p>
<ul>
<li>… accelerate/change/decline/decrease/ drop/ expand/ fall/ increase..</li>
<li>is/are/was/were constant/ different/equal/ higher/ identical/ lower/ unchanged/ uniform/ unaffected.. </li>
<li>It eventually levelled off at a terminal velocity of 300 m/s. </li>
<li>It can be observed from Fig. 2 that there was only a very small enhancement when H2O2 was present.</li>
<li>Comparing Figs. 4 and 5, it is obvious that a significant improvement was obtained in the majority of cases.</li>
</ul>
<p><strong>Move 3. example sentence</strong><br>Comparison with other results </p>
<ul>
<li>The SFS results obtained here are in exceptionally good agreement with existing FE results.</li>
<li>Our concordance scores strongly confirm previous predictions.</li>
<li>The numerical model tends to give predictions that are parallel to the experimental data from corresponding tests.</li>
<li>This is consistent with results obtained in [1]. </li>
<li>verify/ validate/ support/ reinforce/ refute/ match/ extend/ expand/ be unlike/ be inconsistent with/ disprove/ contrary to…</li>
</ul>
<p>Stating problems with results </p>
<ul>
<li>The correlation between the two methods was somewhat less in the case of a central concentrated point load.</li>
<li>It should, however, be noted that in FE methods, the degree of mesh refinement may affect the results.</li>
<li>Minimise the problem: despite this/ negligible/slightly… </li>
<li>It is difficult to simulate the behaviour of the joints realistically.</li>
<li>Suggest reasons for the problem: unavailable/ possible source of error/ not examined in this study… </li>
</ul>
<h4 id="3-5-4-example-sentence"><a href="#3-5-4-example-sentence" class="headerlink" title="3.5.4 example sentence"></a>3.5.4 example sentence</h4><p><strong>The data don’t speak for themselves! The vocabulary we use to describe the data will influence how readers interpret our results.</strong><br>nearly<br>extremely (high/low)<br>practically<br>fewer (than)<br>significant<br>under<br>close (to)<br>considerable<br>barely<br>at least<br>a great deal (of)<br>far (above/below)<br>as few as 45<br>only<br>even (higher/lower)<br><img src="/2024/05/25/Practical-English-Writing-Course-Notes/2024-05-24-16-11-31.png" alt=""></p>
<p><strong>Statements of negative results</strong> </p>
<ol>
<li><strong>No significant difference</strong> between the two groups was evident.</li>
<li><strong>None of</strong> these differences were <strong>statistically significant</strong>.</li>
<li><strong>No evidence was found</strong> for non-linear associations between X and Y.<br>Only <strong>trace amounts</strong> of X were detected in …</li>
<li><strong>There was no evidence that</strong> X has an influence on …</li>
<li>Overall, X <strong>did not affect</strong> males and females differently in these measures.</li>
<li><strong>There was no observed difference in</strong> the number of…</li>
</ol>
<p><strong>Statements of positive results</strong></p>
<ol>
<li><strong>Strong evidence</strong> of X was found when …</li>
<li>This result is <strong>significant</strong> at the p = 0.05 level.</li>
<li>A <strong>positive correlation</strong> was found between X and Y.</li>
<li>There was a <strong>significant positive correlation</strong> between …</li>
<li>The difference between the X and Y groups was <strong>significant</strong>.</li>
<li>There was <strong>a significant difference</strong> between the two conditions …</li>
</ol>
<p><strong>highlight findings</strong></p>
<ol>
<li>This result is <strong>somewhat counterintuitive</strong>.</li>
<li>The <strong>more surprising correlation</strong> is with the …</li>
<li>The <strong>most surprising aspect</strong> of the data is in the …</li>
<li>The correlation between X and Y is <strong>interesting</strong> because …</li>
<li>The most <strong>striking</strong> result to emerge from the data is that …</li>
<li><strong>Interestingly,</strong> there were also differences in the ratios of …</li>
<li><strong>The single most striking observation to</strong> emerge from the data comparison was …</li>
</ol>
<h3 id="3-6-Discussion-and-Conclusion"><a href="#3-6-Discussion-and-Conclusion" class="headerlink" title="3.6 Discussion and Conclusion"></a>3.6 Discussion and Conclusion</h3><p>(Discussion)To interpret and describe <strong>the significance of your findings</strong> in light of what was already known about the research problem being investigated<br>To explain any <strong>new understanding or fresh insights</strong> about the problem after you’ve taken the findings into consideration<br>To explain how your study has <strong>moved the reader‘s understanding of the research problem forward</strong> from where you left them at the end of introduction/Literature Review</p>
<p>(Conclusion)To <strong>summarize</strong> the research<br>To <strong>elaborate</strong> the well-reasoned argument<br>To emphasize the <strong>significance</strong> by elaborating on the applications, implications, and/or future work<br>To move readers from the world of your paper back to their own world (Rosen et al., 1999)</p>
<h4 id="3-6-1-Example-REVIEW-COMMENT"><a href="#3-6-1-Example-REVIEW-COMMENT" class="headerlink" title="3.6.1 Example:REVIEW COMMENT"></a>3.6.1 Example:REVIEW COMMENT</h4><p>The Discussion fails to relate the findings and observations to other relevant studies, and there appears to be no discussion on the implications and limitations of these findings.</p>
<p>In my work as a reviewer, I often have difficulty in understanding how significant the authors feel their work is, and why their findings add value. This is because authors are not explicit enough – they don’t signal to<br>me (and the reader) that they are about to say, or are now saying, something important. The result is that their achievement may be hidden in the middle of a nondescript sentence in a nondescript paragraph … and no one will notice it.</p>
<h4 id="3-6-2-A-tentative-model-for-Discussion-A-summary"><a href="#3-6-2-A-tentative-model-for-Discussion-A-summary" class="headerlink" title="3.6.2 A tentative model for Discussion: A summary"></a>3.6.2 A tentative model for Discussion: A summary</h4><p>Move 1: Background information: review of research purposes/theory/ methodology</p>
<ul>
<li>Revisiting previous research </li>
<li>Revisiting previous sections</li>
</ul>
<p>Move 2: Reporting /summarizing results: integration of specific results</p>
<ul>
<li>Summarizing/revisiting key results</li>
</ul>
<p>Move 3: Explaining and evaluating results: establishing meaning &amp; significance</p>
<ul>
<li>Interpretation of/ Explanation for results</li>
<li>Comparisons with existing research</li>
<li>Achievement or contribution  </li>
<li>Implications/applications </li>
<li>Limitations </li>
<li>Future work</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>about</title>
    <url>/about/index.html</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></tbody></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></tbody></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></tbody></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tbody><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></tbody></table></figure>
<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>Welcome To My Academic Homepage</title>
    <url>/index.html</url>
    <content><![CDATA[<h1 id="News-boom"><a href="#News-boom" class="headerlink" title="News(:boom:)"></a>News(<span class="github-emoji"><span>💥</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f4a5.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>)</h1><div class="note danger">
            <font color="#8B0000" style="font-weight: bold; font-size: 16pt; text-align: center;"> Seeking Joint PhD Supervision, Postdoctoral Opportunities, and International Collaboration （Released on November 2025）</font><p>I am a PhD candidate in intelligent manufacturing and robotics, with <strong>research expertise</strong> in:<br>(1) Human–robot collaboration (perception–decision–execution–learning closed loop);<br>(2) Management, control, and optimization of complex manufacturing systems;<br>(3) Robot cluster control and decision-making.<br>My <strong>technical skill set</strong> includes:<br>(1) Deep reinforcement learning, evolutionary computation, and swarm intelligence algorithms;<br>(2) Applications of large language models (LLMs) in intelligent manufacturing and robotics;<br>(3) Mathematical modeling,  linearization, and the use of commercial solvers.<br><strong>My expected graduation date is June 2027, and I am actively seeking:</strong></p><p>-1. <font color="blue" style="font-weight: bold;"><strong>One-Year Joint PhD / Visiting Research Opportunity (via CSC Funding)</strong></font><br>I will apply for the China Scholarship Council (CSC) program to undertake a one-year joint PhD/visiting research placement at a leading university or research institute.</p><p>-2. <font color="blue" style="font-weight: bold;"><strong>Postdoctoral Position After Graduation (Starting August 2027)</strong></font><br>I am seeking postdoctoral positions in Europe, the United States, Singapore, or other regions with strong research programs in robotics, artificial intelligence, and advanced manufacturing.</p><p>-3. <font color="blue" style="font-weight: bold;"><strong>International Collaborative Research &amp; Publication Partnerships</strong></font><br>I welcome opportunities for joint publications, collaborative research projects, and long-term academic cooperation.</p><p><strong>If my research background aligns with your interests, I would be pleased to connect.</strong> <font color="blue" style="font-weight: bold;"><strong>My email: huangming@bit.edu.cn</strong></font></p>
          </div>
<hr>
<h1 id="Biography"><a href="#Biography" class="headerlink" title="Biography"></a>Biography</h1><p>Ming Huang was born in Xingtai, Hebei Province, China in 1999. Currently, I am pursuing a doctoral degree in Mechanical Engineering at Beijing Institute of Technology.</p>
<p>If you have any further questions, please feel free to contact me.<br>My E-mail（ <span class="github-emoji"><span>✉</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2709.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> ）： huangming @bit.edu.cn.<br>Address：School of Mechanical Engineering, Beijing Institute of Technology, Beijing  100081, China </p>
<!-- 让表格居中显示的风格 -->
<style>
.center 
{
  display: table;
  margin-left: 0px;
  margin-right: 0px;
}
table th:first-of-type {
    width: 420pt;
}
table th:nth-of-type(2) {
    width: 640pt;
}
</style>

<div class="center">

<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><span class="github-emoji"><span>🔍</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f50d.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>   Interests</th>
<th style="text-align:left"><span class="github-emoji"><span>🎓</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>   Education</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><li><b>Deep reinforcement learning, Evolutionary computing，LLM</b></li><li> <b>Robotic swarm scheduling</b> </li><li><b>Human-Robot Collaboration Optimization</b></li><li>Multi-stage integrated scheduling</li><li>Complex System Modeling and Optimization</li><li>Dynamic multi-objective optimization</li></td>
<td style="text-align:left"><li><b>2023. 09 - 2027. 07 D.Eng in Mechanical Engineering-- Beijing Institute of Technology</b></li><li>2020. 09 - 2023. 07 M.Eng. in Industrial Engineering-- Wuhan University of Technology</li><li>2016. 09 - 2020. 07 B.Eng. in Quality Management Engineering-- He Bei University </li><li>2013. 09 - 2016. 07 Student in the first senior high school of Xingtai City, Hebei</li></td>
</tr>
</tbody>
</table>
</div>
<div class="tabs" id="yearlist"><ul class="nav-tabs"><li class="tab"><a href="#yearlist-1">2023 YearList <span class="github-emoji"><span>📋</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f4cb.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></a></li><li class="tab active"><a href="#yearlist-2">2024 YearList <span class="github-emoji"><span>📋</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f4cb.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></a></li></ul><div class="tab-content"><div class="tab-pane" id="yearlist-1"><p><strong>2023 To Do list <span class="github-emoji"><span>🎯</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f3af.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></strong><br><span class="github-emoji"><span>✔</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> Going to pass the PhD application examination<br><span class="github-emoji"><span>✔</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> Getting a Master’s Degree:<br><span class="github-emoji"><span>➖</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2796.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> Acceptance of three papers or more（Progress: 2/3）<br><span class="github-emoji"><span>✖</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2716.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> Go to improve English listening skills<br><span class="github-emoji"><span>✔</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> To control the weight between 55-60kg, a strong body is the basis of scientific research.<br><span class="github-emoji"><span>➖</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2796.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> Go save up money because maybe try hair transplants to fight hair loss.<br><strong><em>Words I Wanted To Say：There are many things worth doing in one’s life, and the most significant is to do it right now.<span class="github-emoji"><span>🏃</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c3.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></em></strong></p></div><div class="tab-pane active" id="yearlist-2"><!--**2024 To Do list :dart:**
:black_square_button: Acceptance of three papers or more :memo:（Progress: 1/3）
:black_square_button: Read three literary novels :books:（Progress: 0/3）
:black_square_button: Improvement of English listening and communication skills :earth_americas: 
:black_square_button: Keep your weight between 55-60kg, and don't let abdominal muscles disappear! :muscle:
:black_square_button: Time into 25 minutes for a 5k run :running:
:black_square_button: To save 30,000 yuan and become financially independent. :moneybag:
***Words I Wanted To Say：The goal of life should not be success and fame, but rather to live your life the way you want.:low_brightness:***--></div></div></div>
<hr>
<h1 id="Supervisor"><a href="#Supervisor" class="headerlink" title="Supervisor"></a>Supervisor</h1><ul>
<li><a href="https://me.bit.edu.cn/szdw/jsml/zzgcx/gygcyjs/sssds13/13ddf65bcbdf4f5c83acd440457ee9de.htm"><strong>Sihan Huang</strong></a> associate professor（My doctoral  supervisor）<br><br></li>
<li><a href="http://smee.whut.edu.cn/rsgz/szdw/201901/t20190103_351115.shtml"><strong>Baigang Du</strong></a> associate professor（My master’s supervisor）<br><br></li>
<li><a href="http://smee.whut.edu.cn/rsgz/szdw/201510/t20151014_200361.shtml"><strong>Jun Guo</strong></a> associate professor（My associate master’s supervisor）</li>
</ul>
<hr>
<h1 id="Awards"><a href="#Awards" class="headerlink" title="Awards"></a>Awards</h1><!-- 标签形式 -->
<div class="note info">
            <ul><li><font c"blue"="">2025. 11</font> “Huawei Cup” The 22th China Post-granduate Mathematical Contest — <strong>National First Prize</strong> and <strong>Huawei Special Second Prize</strong><br>Modeling topic: Research on Kernel Scheduling Optimization in General Neural Network Processor Competitions </li></ul>
          </div>
<div class="note info">
            <ul><li><font color="blue">2025. 11</font> The 3rd Human-Centric Smart Manufacturing Academic Conference — <strong>Outstanding Presentation Award</strong><br>Presentation topic: Optimization of Ultra High Flexibility Robotic Manufacturing System with Human Robot Collaboration: A Multi-Objective Multi-Agent Deep Reinforcement Learning </li></ul>
          </div>
<div class="note info">
            <ul><li><font color="blue">2025. 11</font> Outstanding Student for the 2024-2025  Year at Beijing Institute of Technology — <strong>Personal Honor</strong> </li></ul>
          </div>
<div class="note info">
            <ul><li><font color="blue">2024. 12</font> 2024 World Intelligent Manufacturing Conference – Human-Centered Smart Manufacturing Session &amp; The 2rd Human-Centric Smart Manufacturing Academic Conference — <strong>Outstanding Presentation Award</strong><br>Presentation topic: Autonomous Collaboration Strategies for Mobile Operations in Robot Clusters. </li></ul>
          </div>
<div class="note info">
            <ul><li><font color="blue">2024. 11</font> Outstanding Student for the 2023-2024  Year at Beijing Institute of Technology — <strong>Personal Honor</strong> </li></ul>
          </div>
<div class="note info">
            <ul><li><font color="blue">2023. 11</font> “Huawei Cup” The 20th China Post-granduate Mathematical Contest — <strong>National Second Prize</strong><br>Modeling topic: Research on Optimization of Evaluation Plans for Large scale Innovation Competitions </li></ul>
          </div>
<div class="note info">
            <ul><li><font color="blue">2023. 06</font> Outstanding graduate of Wuhan University of Technology — <strong>Personal Honor</strong> </li></ul>
          </div>
<div class="note info">
            <ul><li><font color="blue">2022. 12</font> “Huawei Cup” The 19th China Post-granduate Mathematical Contest — <strong>National Second Prize</strong><br>Modeling topic: Scheduling optimization problem for automotive manufacturing painting-final assembly cache sequencing zone </li></ul>
          </div>
<div class="note info">
            <ul><li><font color="blue">2022. 06</font> The 12th Wuhan University of Technology Post-graduate Mathematical Contest  — <strong>First Prize</strong><br>Modeling topic: Research on resource-constrained vehicle routing problem </li></ul>
          </div>
<div class="note info">
            <ul><li><font color="blue">2021. 12</font> “Huawei Cup” The 18th China Post-granduate Mathematical Contest — <strong>National Third Prize</strong><br>Modeling topic: Airline crew optimization scheduling problem </li></ul>
          </div>
<div class="note info">
            <ul><li><font color="blue">2021. 01</font> “Cloud Talk New Technology” Popular Science Contest— <strong>Third Prize</strong><br>Entry: OpenCV-based smart power plant inspection trolley </li></ul>
          </div>
<hr>
<h1 id="Journal-Reviewer"><a href="#Journal-Reviewer" class="headerlink" title="Journal Reviewer"></a>Journal Reviewer</h1><p>IEEE Transactions on Systems Man Cybernetics-Systems (SCI, IF=8.7).<br>Journal of King Saud University Science (SCI, IF=3.6)<br>International Journal of Intelligent Robotics and Applications(SCI, IF=2.0)</p>
<hr>
<h1 id="Conference-presentation"><a href="#Conference-presentation" class="headerlink" title="Conference presentation"></a>Conference presentation</h1><div class="note success">
            <p>-1. <font color="blue">(2025. 11)</font> The 3rd Human-Centric Smart Manufacturing Academic Conference, Beijing, China. — <strong>Outstanding Presentation Award</strong><br>Presentation topic: Optimization of Ultra High Flexibility Robotic Manufacturing System with Human Robot Collaboration: A Multi-Objective Multi-Agent Deep Reinforcement Learning</p>
          </div>
<div class="note success">
            <p>-2. <font color="blue">(2025. 07)</font>.  2025 Annual Conference of the Industrial Big Data and Intelligent Systems Division of the Chinese Mechanical Engineering Society &amp; The 8th Academic Conference on Big Data-Driven Intelligent Manufacturing, Yichang, China. <strong>(Forum Presentation)</strong><br>Presentation topic: A Normalized Tchebycheff Aggregation-Based Multi-Objective Deep Reinforcement Learning for Dynamic Scheduling of Robotic Manufacturing Systems.</p>
          </div>
<div class="note success">
            <p>-3. <font color="blue">(2025. 05)</font>. Frontier Forum on Digital Twin Technologies and Application Models &amp; The 19th Session (2025 Issue 3) of the “Strive for Excellence” PhD Student Workshop by the China Graphics Society, Yichang, China. <strong>(Invited Presentation at the Graduate Forum)</strong><br>Presentation topic: Digital Twin-driven Multi-objective Deep Reinforcement Learning for Dynamic and Flexible Island Assembly Line Scheduling.</p>
          </div>
<div class="note success">
            <p>-4. <font color="blue">(2024. 12)</font>. 2024 World Intelligent Manufacturing Conference – Human-Centered Smart Manufacturing Session &amp; The 2rd Human-Centric Smart Manufacturing Academic Conference, Nanjing, China. <strong>(Outstanding Presentation Award)</strong> <a href="../zip"> (Poster) </a> <strong><code>PDF</code></strong><br>Presentation topic: Autonomous Collaboration Strategies for Mobile Operations in Robot Clusters.</p>
          </div>
<div class="note success">
            <p>-5. <font color="blue">(2024. 12)</font>.. 2024 National Doctoral Forum on Industrial Engineering &amp; the 772nd Tsinghua University Doctoral Academic Forum, Beijing, China. <strong>(Forum Presentation)</strong><br>Presentation topic: Fuzzy Superposition Operation and Knowledge-driven Co-evolutionary Algorithm for Integrated Production Scheduling and Vehicle Routing Problem with Soft Time Windows and Fuzzy Travel Times.</p>
          </div>
<hr>
<h1 id="Project"><a href="#Project" class="headerlink" title="Project"></a>Project</h1><h3 id="Fund"><a href="#Fund" class="headerlink" title="Fund"></a>Fund</h3><div class="note warning">
            <ol><li><p><font color="blue">2024. 12-2027. 11</font> The National Key Research and Development Program of China :<br>Research on Cyber-Physical Fusion Modeling Technology for Control System Equipment Targeting Multiple Optimization Objectives in Flexible Manufacturing （Project Participants）</p></li><li><p><font color="blue">2025. 01-2027. 12</font> The National Natural Science Foundation of China  (Youth Fund) :<br>Research on Adaptive Reconfiguration Optimization Theory for Smart Manufacturing Systems with Multiple Uncertain Disturbances （Project Participants）</p></li><li><p><font color="blue">2024. 07-2027. 12</font> Beijing Natural Science Foundation under Grant :<br>Key Technologies and System Applications of Digital Twin Workshop for 3C Intelligent Manufacturing （Project Participants）</p></li><li><p><font color="blue">2022. 12-2024. 10</font> The National Key Research and Development Program of China:<br>Research on Key Technologies for Dynamic Reconfiguration and XXXX of Production Lines （Project Participants）</p></li><li><p><font color="blue">2022. 01-2022. 12</font> Independent Innovation Foundation of Wuhan University of Technology —— Postgraduate Personal Exploration Project :<br>Research on Integrated  Production and Distribution Scheduling with Flexible Batching and Soft Time Windows （<strong>Project Principals</strong>）</p></li><li><p><font color="blue">2018. 01-2020. 12 </font> National Natural Science Foundation of China (Youth Fund) ：<br>A game method of supply and demand allocation of complex product system manufacturing services for social collaboration （Project Participants）</p></li></ol>
          </div>
<h3 id="Internship"><a href="#Internship" class="headerlink" title="Internship"></a>Internship</h3><div class="note warning">
            <h4 id="—-2021-07-2022-07-Tuoxing-Zhiwang-Technology-Co-Ltd-Wuhan-（Java-Back-End-Development）"><a href="#—-2021-07-2022-07-Tuoxing-Zhiwang-Technology-Co-Ltd-Wuhan-（Java-Back-End-Development）" class="headerlink" title="— 2021. 07-2022. 07 Tuoxing Zhiwang Technology Co., Ltd.  Wuhan （Java Back End Development）"></a>— 2021. 07-2022. 07 <a href="http://www.tonshinet.com/">Tuoxing Zhiwang Technology Co., Ltd.  Wuhan</a> （Java Back End Development）</h4><ol><li><p><font color="blue">2021. 07-2021. 12 </font> Development of Wind Plant Operation and Maintenance Application Platform</p></li><li><p><font color="blue">2022. 01-2022. 07 </font> Tangshan Sinoma Heavy Machinery PMS Improvement Project<br>-Development of External Supplier Price Comparison Management Platform </p></li></ol>
          </div>
<p><a href="https://clustrmaps.com/site/1bqdf" title="Visit tracker"><img src="//clustrmaps.com/map_v2.png?cl=555555&amp;w=500&amp;t=m&amp;d=lPGunm_IA1w4rADFSr_XZ_r1KQCj0GoM-18qXvbMs1E&amp;co=ffffff&amp;ct=555555"></a></p>
</div>]]></content>
  </entry>
  <entry>
    <title>Scholars Link</title>
    <url>/link/index.html</url>
    <content><![CDATA[<h3 id="Smart-Manufacturing-Operations-Research"><a href="#Smart-Manufacturing-Operations-Research" class="headerlink" title="Smart Manufacturing &amp; Operations Research"></a><p align="center"><font face="TIME NEW ROMANS" size="5" color="black">Smart Manufacturing &amp; Operations Research</font></p></h3><h5 id="Digital-Twin-amp-Human-Centric-Smart-Manufacturing"><a href="#Digital-Twin-amp-Human-Centric-Smart-Manufacturing" class="headerlink" title="Digital Twin &amp; Human Centric Smart Manufacturing"></a>Digital Twin &amp; Human Centric Smart Manufacturing</h5><ul>
<li><a href="https://www.kth.se/profile/lihuiw/"><strong>Lihui Wang</strong></a>（KTH Royal Institute of Technology-Industrial Production System）</li>
<li><a href="https://shi.buaa.edu.cn/taofei/zh_CN/index.htm"><strong>Fei Tao</strong></a>（Beihang University-Institute of Science and Technology）</li>
<li><a href="https://me.bit.edu.cn/szdw/jsml/zzgcx/gygcyjs/sssds13/13ddf65bcbdf4f5c83acd440457ee9de.htm"><strong>Sihan Huang</strong></a>（Beijing Institute of Technology - School of Mechanical Engineering）</li>
<li><a href="https://person.zju.edu.cn/baicun#949984"><strong>Baicun Wang</strong></a>（Zhejiang University - School of Mechanical Engineering）<h5 id="Shop-Scheduling"><a href="#Shop-Scheduling" class="headerlink" title="Shop Scheduling"></a>Shop Scheduling</h5></li>
<li><a href="https://www.au.tsinghua.edu.cn/info/1107/1558.htm"><strong>Ling Wang</strong></a>（Tsinghua University-Department of Automation）</li>
<li><a href="http://mse.hust.edu.cn/info/1143/1387.htm"><strong>Liang Gao</strong></a>（Huazhong University of Science and Technology - School of Mechanical Science &amp; Engineering）</li>
<li><a href="http://sa.whut.edu.cn/yjspy/dsdw/201902/t20190225_353630.shtml"><strong>Deming Lei</strong></a>（Wuhan University of Technology - School of Automation）</li>
<li><a href="http://ischedulings.com/project.html"><strong>Junqing Li</strong></a>（Liaocheng University - School of Computer Science）</li>
<li><a href="https://scholar.must.edu.mo/scholar/100666"><strong>Kaizhou Gao</strong></a>（Macau University of Science and Technology - Institute of Systems Engineering）<h5 id="Integrated-Scheduling-amp-Vehicle-Routing"><a href="#Integrated-Scheduling-amp-Vehicle-Routing" class="headerlink" title="Integrated Scheduling &amp; Vehicle Routing"></a>Integrated Scheduling &amp; Vehicle Routing</h5></li>
<li><a href="http://smee.whut.edu.cn/rsgz/szdw/201901/t20190103_351115.shtml"><strong>Baigang Du</strong></a>（Wuhan University of Technology - School of Mechanical and Electrical Engineering）</li>
<li><a href="https://bs.scu.edu.cn/gongye/201904/5178.html"><strong>Dujuan Wang</strong></a>（Sichuan University-Business School）</li>
<li><a href="http://bs.scu.edu.cn/gongye/201904/5203.html"><strong>Zhaoxia Guo</strong></a>（Sichuan University - School of Business）</li>
<li><a href="http://ems.whu.edu.cn/info/1718/10649.htm"><strong>Kai Wang</strong></a>（Wuhan University-School of Economics and Management）</li>
<li><a href="http://cm.hust.edu.cn/info/1745/24587.htm"><strong>Hu Qin</strong></a>（Huazhong University of Science and Technology-School of Management）</li>
<li><a href="http://cm.hust.edu.cn/info/1746/24605.htm"><strong>Kunpeng Li</strong></a>（Huazhong University of Science and Technology-School of Management）</li>
<li><a href="https://gr.xjtu.edu.cn/web/ya.liu/%E4%B8%AA%E4%BA%BA%E7%AE%80%E4%BB%8B"><strong>Ya Liu</strong></a>（Xi’an Jiaotong University-School of Management）</li>
<li><a href="http://faculty.hfut.edu.cn/~ZNBjIb/zh_CN/index.htm"><strong>Bayi Cheng</strong></a>（Hefei University of Technology-School of Management）</li>
</ul>
<h5 id="Resource-and-Project-Scheduling"><a href="#Resource-and-Project-Scheduling" class="headerlink" title="Resource and Project Scheduling"></a>Resource and Project Scheduling</h5><ul>
<li><a href="http://202.118.1.237/sxliu/index.html"><strong>Shixin Liu</strong></a>（Northeastern University - School of Information Science and Engineering）</li>
<li><a href="http://smee.whut.edu.cn/rsgz/szdw/201506/t20150602_169283.shtml"><strong>Yibing Li</strong></a>（Wuhan University of Technology - School of Mechanical and Electrical Engineering） <h5 id="Disassembly-Line-Balancing-Assembly-Line-Scheduling"><a href="#Disassembly-Line-Balancing-Assembly-Line-Scheduling" class="headerlink" title="Disassembly Line Balancing, Assembly Line Scheduling"></a>Disassembly Line Balancing, Assembly Line Scheduling</h5></li>
<li><a href="https://faculty.swjtu.edu.cn/zhangzeqiang/zh_CN/index/97881/list/index.htm"><strong>Zeqiang Zhang</strong></a>（Southwest Jiaotong University-School of Mechanical Engineering）</li>
<li><a href="http://smee.whut.edu.cn/rsgz/szdw/201510/t20151014_200361.shtml"><strong>Jun Guo</strong></a>（Wuhan University of Technology - School of Mechanical and Electrical Engineering）</li>
</ul>
<h3 id="Evolutionary-Computation"><a href="#Evolutionary-Computation" class="headerlink" title="Evolutionary Computation"></a><p align="center"><font face="TIME NEW ROMANS" size="5" color="black">Evolutionary Computation</font></p></h3><ul>
<li><a href="https://www.egr.msu.edu/~kdeb/index.shtml"><strong>Kalyanmoy Deb</strong></a>（<a href="https://www.egr.msu.edu/~kdeb/codes.shtml">NSGA-II</a>）</li>
<li><a href="https://www.surrey.ac.uk/people/yaochu-jin#publications"><strong>Yaochu Jin</strong></a>（Westlake University, China; University of Surrey, UK）</li>
<li><a href="https://www.cs.cityu.edu.hk/~qzhan7/index.html"><strong>Qingfu Zhang</strong></a>（MOEA/D City University of Hong Kong - Department of Computer Science）</li>
<li><a href="http://wky.ahu.edu.cn/2022/0301/c13481a280306/page.htm"><strong>Ye Tian</strong></a>（<a href="https://github.com/BIMK/PlatEMO">platEMO</a> Anhui University）</li>
<li><strong>Handing Wang</strong>（Xidian University）<h3 id="Related-Academic-Links"><a href="#Related-Academic-Links" class="headerlink" title="Related Academic Links"></a><p align="center"><font face="TIME NEW ROMANS" size="5" color="black">Related Academic Links</font></p></h3></li>
<li><a href="https://cuglirui.github.io/index.html"><strong>Rui Li</strong></a> (Flexible job shop scheduling <a href="https://cuglirui.github.io/downloads.htm">FJSP</a> <code>matlab</code>)</li>
<li><a href="https://github.com/dengfaheng"><strong>Faheng Deng</strong></a> (Vehicle routing problem <a href="https://github.com/dengfaheng">VRP，TSP</a> <code>java</code>)</li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>Academic Publications</title>
    <url>/publications/index.html</url>
    <content><![CDATA[<div class="note success">
            <h3 id="Master’s-Thesis"><a href="#Master’s-Thesis" class="headerlink" title="Master’s Thesis"></a>Master’s Thesis</h3><ol><li><strong>Huang, M.</strong>, 2023. Research on integrated production and distribution scheduling for fast moving consumer goods manufacturing enterprise. Wuhan University of Technology.<a href="../pdf/Thesis-快速消费品制造企业生产-配送集成调度研究.pdf"> (Master’s Thesis) </a> <strong><code>PDF</code></strong>;</li></ol>
          </div>
<div class="note success">
            <h3 id="Published-papers"><a href="#Published-papers" class="headerlink" title="Published papers"></a>Published papers</h3><ol><li><p><strong>M. Huang</strong>, S. Huang, B. Du<em>, J. Guo and Y. Li. Fuzzy Superposition Operation and Knowledge-driven Co-evolutionary Algorithm for Integrated Production Scheduling and Vehicle Routing Problem with Soft Time Windows and Fuzzy Travel Times. </em>IEEE Transactions on Fuzzy Systems<em>, 2024, doi: 10.1109/TFUZZ.2024.3388003.（SCI Q1 , IF 11.9, TOP）<br><a href="https://ieeexplore.ieee.org/document/10497842"> (Paper) </a> <strong><code>HTML</code></strong> ; <a href="../zip/KDCEA-Code.zip"> (KDCEA) </a> <strong><code>Matlab code</code></strong> ; <a href="../zip/IPSVRP-STW-FTT-Dateset.zip">(IPSVRP-STW&amp;FTT Dateset)</a> <strong><code>.zip</code></strong>;<a href="../zip/IPSVRP-STW-FTT-AMPL.zip"> (AMPL) </a> <strong><code>.mod</code></strong> ; <a href="../zip/IPSVRP-STW-FTT-CASE.zip">(CASE)</a> <em>*<code>.zip</code></em></em></p></li><li><p><strong>Huang, M.</strong>, Du, B. G., Guo, J<em>. A hybrid collaborative framework for integrated production scheduling and vehicle routing problem with batch manufacturing and soft time windows. </em>Computers &amp; Operations Research<em>, 2023, 159, 106346. <a href="https://doi.org/10.1016/j.cor.2023.106346">https://doi.org/10.1016/j.cor.2023.106346</a> .（SCI Q1 , IF 4.6, ABS-3）<br><a href="https://www.sciencedirect.com/science/article/pii/S0305054823002101"> (Paper) </a> <strong><code>HTML</code></strong>; <a href="link"> (HCF-IMOEA) </a> <strong><code>Matlab code</code></strong> ; <a href="../zip/IPSVRP-BM-STW-Dateset.zip">(IPSVRP-BM&amp;STW Dateset)</a> <em>*<code>.zip</code></em></em></p></li><li><p><strong>Huang, M.</strong>, Du, B. G<em>., Guo, J, Li, Y, B. Integrated  production and distribution scheduling optimization of considering soft time windows and fuzzy travel times. </em>Control Theory and Applications(chinese)<em>, 2023, 40(x): 1–10. DOI: 10.7641/CTA.2023.20920.（EI index，IF:2.781）<br><a href="https://kns.cnki.net/kcms2/article/abstract?v=xNq_RSSxttstktCq-VWRIQS1e6fIAq26YV2DyMFsCVB9NtbxKqFgDZtEE5MiKN1dG5PLmIItcZsrbA9y-tAUQMMLpbYZJEJr4NENoL1ughgxkkSL6gjYdqp7OcHpgSim7TjKF6gvjYE=&amp;uniplatform=NZKPT&amp;language=CHS"> (Paper) </a><strong><code>HTML</code></strong> ; <a href="../zip/NSGA-II-AVNS-Code.zip"> (NSGA-II-AVNS) </a> <strong><code>Matlab code</code></strong> ; <a href="../zip/Fuzzy-IPDS-Dateset.zip">(Fuzzy-IPDS Dateset)</a> <em>*<code>.zip</code></em></em></p></li></ol><!--  1. **Ming Huang**, Sihan Huang*, Jianpeng Chen, Wei Dong, Baicun Wang, Bing Ruan, Yunpeng Gao, Guoxin Wang, Yan Yan. Dynamic scheduling optimization of island assembly lines under uncertain disturbances by multi-objective deep reinforcement learning *JOURNAL OF MECHANICAL ENGINEERING(chinese)*  （EI index，IF:）--><p>JOURNAL OF MECHANICAL ENGINEERING(chinese)<a href="https://kns.cnki.net/kcms2/article/abstract?v=sTAINAsmd8-D4oLYcJmyoLGK9gexKDP_iRPM8GsNJpuWU8ZEjxX863J2C0K68YqR2ipstFRNvj00VT0Vdp1SpKfH5UK9eMZuwJQkfgk90Ufkv4BDiEzf5jq1mut1MjG333KGXiViqqcihzrMzmnf7C6iGbcnFSyrJDpgK1iNu1ihnQvS7GiDAg==&amp;uniplatform=NZKPT&amp;language=CHS"> (Paper) </a><strong><code>HTML</code></strong> ; <a href=""> (MO-D3QN) </a> <strong><code>python code</code></strong> ; <a href="../zip/IAS-Dataset.zip">(IAS Dateset)</a> <strong><code>.zip</code></strong></p>
          </div>
<div class="note warning">
            <h3 id="Working-papers"><a href="#Working-papers" class="headerlink" title="Working papers"></a>Working papers</h3><ol><li><strong>Ming Huang</strong>, Sihan Huang<em>, Bing Ruan, Wei Dong, Jianpeng Chen, Baicun Wang, Baigang Du, Guoxin Wang, Yan Yan, Lihui Wang. A normalized Tchebycheff aggregation based multi-objective deep reinforcement learning for dynamic flexible island assembly system scheduling.<br><a href=""> (Paper) </a><strong><code>HTML</code></strong> ; <a href=""> (NT-D3QN) </a> <strong><code>python code</code></strong> ; <a href="../zip/DFIASSP-Dataset.zip">(DFIASSP-Dataset)</a> <strong><code>.zip</code></strong> ; <a href="../zip/DFIASSP-Case.zip">(DFIASSP-Case)</a> <strong><code>.zip</code></strong>;<a href="../zip/DFIASSP-AMPL.zip"> (AMPL) </a> <em>*<code>.mod</code></em></em> </li></ol>
          </div>
<div class="note warning">
            <h3 id="Conference-papers"><a href="#Conference-papers" class="headerlink" title="Conference papers"></a>Conference papers</h3><p align="left"><font face="黑体" size="5" color="blue"></font></p><li><font face="黑体" size="5" color="blue">None </font></li><p></p>
          </div>]]></content>
  </entry>
  <entry>
    <title>Welcome To My Academic Homepage</title>
    <url>/home/index.html</url>
    <content><![CDATA[<h1 id="News-boom"><a href="#News-boom" class="headerlink" title="News(:boom:)"></a>News(<span class="github-emoji"><span>💥</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f4a5.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>)</h1><div class="note danger">
            <font color="#8B0000" style="font-weight: bold; font-size: 16pt; text-align: center;"> Seeking Joint PhD Supervision, Postdoctoral Opportunities, and International Collaboration （Released on November 2025）</font><p>I am a PhD candidate in intelligent manufacturing and robotics, with <strong>research expertise</strong> in:<br>(1) Human–robot collaboration (perception–decision–execution–learning closed loop);<br>(2) Management, control, and optimization of complex manufacturing systems;<br>(3) Robot cluster control and decision-making.<br>My <strong>technical skill set</strong> includes:<br>(1) Deep reinforcement learning, evolutionary computation, and swarm intelligence algorithms;<br>(2) Applications of large language models (LLMs) in intelligent manufacturing and robotics;<br>(3) Mathematical modeling,  linearization, and the use of commercial solvers.<br><strong>My expected graduation date is June 2027, and I am actively seeking:</strong></p><p>-1. <font color="blue" style="font-weight: bold;"><strong>One-Year Joint PhD / Visiting Research Opportunity (via CSC Funding)</strong></font><br>I will apply for the China Scholarship Council (CSC) program to undertake a one-year joint PhD/visiting research placement at a leading university or research institute.</p><p>-2. <font color="blue" style="font-weight: bold;"><strong>Postdoctoral Position After Graduation (Starting August 2027)</strong></font><br>I am seeking postdoctoral positions in Europe, the United States, Singapore, or other regions with strong research programs in robotics, artificial intelligence, and advanced manufacturing.</p><p>-3. <font color="blue" style="font-weight: bold;"><strong>International Collaborative Research &amp; Publication Partnerships</strong></font><br>I welcome opportunities for joint publications, collaborative research projects, and long-term academic cooperation.</p><p><strong>If my research background aligns with your interests, I would be pleased to connect.</strong> <font color="blue" style="font-weight: bold;"><strong>My email: huangming@bit.edu.cn</strong></font></p>
          </div>
<hr>
<h1 id="Biography"><a href="#Biography" class="headerlink" title="Biography"></a>Biography</h1><p>Ming Huang was born in Xingtai, Hebei Province, China in 1999. Currently, I am pursuing a doctoral degree in Mechanical Engineering at Beijing Institute of Technology.</p>
<p>If you have any further questions, please feel free to contact me.<br>My E-mail（ <span class="github-emoji"><span>✉</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2709.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> ）： huangming @bit.edu.cn.<br>Address：School of Mechanical Engineering, Beijing Institute of Technology, Beijing  100081, China </p>
<!-- 让表格居中显示的风格 -->
<style>
.center 
{
  display: table;
  margin-left: 0px;
  margin-right: 0px;
}
table th:first-of-type {
    width: 420pt;
}
table th:nth-of-type(2) {
    width: 640pt;
}
</style>

<div class="center">

<div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:left"><span class="github-emoji"><span>🔍</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f50d.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>   Interests</th>
<th style="text-align:left"><span class="github-emoji"><span>🎓</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f393.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>   Education</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><li><b>Deep reinforcement learning, Evolutionary computing，LLM</b></li><li> <b>Robotic swarm scheduling</b> </li><li><b>Human-Robot Collaboration Optimization</b></li><li>Multi-stage integrated scheduling</li><li>Complex System Modeling and Optimization</li><li>Dynamic multi-objective optimization</li></td>
<td style="text-align:left"><li><b>2023. 09 - 2027. 07 D.Eng in Mechanical Engineering-- Beijing Institute of Technology</b></li><li>2020. 09 - 2023. 07 M.Eng. in Industrial Engineering-- Wuhan University of Technology</li><li>2016. 09 - 2020. 07 B.Eng. in Quality Management Engineering-- He Bei University </li><li>2013. 09 - 2016. 07 Student in the first senior high school of Xingtai City, Hebei</li></td>
</tr>
</tbody>
</table>
</div>
<div class="tabs" id="yearlist"><ul class="nav-tabs"><li class="tab"><a href="#yearlist-1">2023 YearList <span class="github-emoji"><span>📋</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f4cb.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></a></li><li class="tab active"><a href="#yearlist-2">2024 YearList <span class="github-emoji"><span>📋</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f4cb.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></a></li></ul><div class="tab-content"><div class="tab-pane" id="yearlist-1"><p><strong>2023 To Do list <span class="github-emoji"><span>🎯</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f3af.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></strong><br><span class="github-emoji"><span>✔</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> Going to pass the PhD application examination<br><span class="github-emoji"><span>✔</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> Getting a Master’s Degree:<br><span class="github-emoji"><span>➖</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2796.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> Acceptance of three papers or more（Progress: 2/3）<br><span class="github-emoji"><span>✖</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2716.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> Go to improve English listening skills<br><span class="github-emoji"><span>✔</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2714.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> To control the weight between 55-60kg, a strong body is the basis of scientific research.<br><span class="github-emoji"><span>➖</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/2796.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span> Go save up money because maybe try hair transplants to fight hair loss.<br><strong><em>Words I Wanted To Say：There are many things worth doing in one’s life, and the most significant is to do it right now.<span class="github-emoji"><span>🏃</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f3c3.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span></em></strong></p></div><div class="tab-pane active" id="yearlist-2"><!--**2024 To Do list :dart:**
:black_square_button: Acceptance of three papers or more :memo:（Progress: 1/3）
:black_square_button: Read three literary novels :books:（Progress: 0/3）
:black_square_button: Improvement of English listening and communication skills :earth_americas: 
:black_square_button: Keep your weight between 55-60kg, and don't let abdominal muscles disappear! :muscle:
:black_square_button: Time into 25 minutes for a 5k run :running:
:black_square_button: To save 30,000 yuan and become financially independent. :moneybag:
***Words I Wanted To Say：The goal of life should not be success and fame, but rather to live your life the way you want.:low_brightness:***--></div></div></div>
<hr>
<h1 id="Supervisor"><a href="#Supervisor" class="headerlink" title="Supervisor"></a>Supervisor</h1><ul>
<li><a href="https://me.bit.edu.cn/szdw/jsml/zzgcx/gygcyjs/sssds13/13ddf65bcbdf4f5c83acd440457ee9de.htm"><strong>Sihan Huang</strong></a> associate professor（My doctoral  supervisor）<br><br></li>
<li><a href="http://smee.whut.edu.cn/rsgz/szdw/201901/t20190103_351115.shtml"><strong>Baigang Du</strong></a> associate professor（My master’s supervisor）<br><br></li>
<li><a href="http://smee.whut.edu.cn/rsgz/szdw/201510/t20151014_200361.shtml"><strong>Jun Guo</strong></a> associate professor（My associate master’s supervisor）</li>
</ul>
<hr>
<h1 id="Awards"><a href="#Awards" class="headerlink" title="Awards"></a>Awards</h1><!-- 标签形式 -->
<div class="note info">
            <ul><li><font color="blue">2025. 11</font> “Huawei Cup” The 22th China Post-granduate Mathematical Contest — <strong>National First Prize</strong> and <strong>Huawei Special Second Prize</strong><br>Modeling topic: Research on Kernel Scheduling Optimization in General Neural Network Processor Competitions </li></ul>
          </div>
<div class="note info">
            <ul><li><font color="blue">2025. 11</font> The 3rd Human-Centric Smart Manufacturing Academic Conference — <strong>Outstanding Presentation Award</strong><br>Presentation topic: Optimization of Ultra High Flexibility Robotic Manufacturing System with Human Robot Collaboration: A Multi-Objective Multi-Agent Deep Reinforcement Learning </li></ul>
          </div>
<div class="note info">
            <ul><li><font color="blue">2025. 11</font> Outstanding Student for the 2024-2025  Year at Beijing Institute of Technology — <strong>Personal Honor</strong> </li></ul>
          </div>
<div class="note info">
            <ul><li><font color="blue">2024. 12</font> 2024 World Intelligent Manufacturing Conference – Human-Centered Smart Manufacturing Session &amp; The 2rd Human-Centric Smart Manufacturing Academic Conference — <strong>Outstanding Presentation Award</strong><br>Presentation topic: Autonomous Collaboration Strategies for Mobile Operations in Robot Clusters. </li></ul>
          </div>
<div class="note info">
            <ul><li><font color="blue">2024. 11</font> Outstanding Student for the 2023-2024  Year at Beijing Institute of Technology — <strong>Personal Honor</strong> </li></ul>
          </div>
<div class="note info">
            <ul><li><font color="blue">2023. 11</font> “Huawei Cup” The 20th China Post-granduate Mathematical Contest — <strong>National Second Prize</strong><br>Modeling topic: Research on Optimization of Evaluation Plans for Large scale Innovation Competitions </li></ul>
          </div>
<div class="note info">
            <ul><li><font color="blue">2023. 06</font> Outstanding graduate of Wuhan University of Technology — <strong>Personal Honor</strong> </li></ul>
          </div>
<div class="note info">
            <ul><li><font color="blue">2022. 12</font> “Huawei Cup” The 19th China Post-granduate Mathematical Contest — <strong>National Second Prize</strong><br>Modeling topic: Scheduling optimization problem for automotive manufacturing painting-final assembly cache sequencing zone </li></ul>
          </div>
<div class="note info">
            <ul><li><font color="blue">2022. 06</font> The 12th Wuhan University of Technology Post-graduate Mathematical Contest  — <strong>First Prize</strong><br>Modeling topic: Research on resource-constrained vehicle routing problem </li></ul>
          </div>
<div class="note info">
            <ul><li><font color="blue">2021. 12</font> “Huawei Cup” The 18th China Post-granduate Mathematical Contest — <strong>National Third Prize</strong><br>Modeling topic: Airline crew optimization scheduling problem </li></ul>
          </div>
<div class="note info">
            <ul><li><font color="blue">2021. 01</font> “Cloud Talk New Technology” Popular Science Contest— <strong>Third Prize</strong><br>Entry: OpenCV-based smart power plant inspection trolley </li></ul>
          </div>
<hr>
<h1 id="Journal-Reviewer"><a href="#Journal-Reviewer" class="headerlink" title="Journal Reviewer"></a>Journal Reviewer</h1><p>IEEE Transactions on Systems Man Cybernetics-Systems (SCI, IF=8.7).<br>Journal of King Saud University Science (SCI, IF=3.6)<br>International Journal of Intelligent Robotics and Applications(SCI, IF=2.0)</p>
<hr>
<h1 id="Conference-presentation"><a href="#Conference-presentation" class="headerlink" title="Conference presentation"></a>Conference presentation</h1><div class="note success">
            <p>-1. <font color="blue">(2025. 11)</font> The 3rd Human-Centric Smart Manufacturing Academic Conference, Beijing, China. — <strong>Outstanding Presentation Award</strong><br>Presentation topic: Optimization of Ultra High Flexibility Robotic Manufacturing System with Human Robot Collaboration: A Multi-Objective Multi-Agent Deep Reinforcement Learning</p>
          </div>
<div class="note success">
            <p>-2. <font color="blue">(2025. 07)</font>.  2025 Annual Conference of the Industrial Big Data and Intelligent Systems Division of the Chinese Mechanical Engineering Society &amp; The 8th Academic Conference on Big Data-Driven Intelligent Manufacturing, Yichang, China. <strong>(Forum Presentation)</strong><br>Presentation topic: A Normalized Tchebycheff Aggregation-Based Multi-Objective Deep Reinforcement Learning for Dynamic Scheduling of Robotic Manufacturing Systems.</p>
          </div>
<div class="note success">
            <p>-3. <font color="blue">(2025. 05)</font>. Frontier Forum on Digital Twin Technologies and Application Models &amp; The 19th Session (2025 Issue 3) of the “Strive for Excellence” PhD Student Workshop by the China Graphics Society, Yichang, China. <strong>(Invited Presentation at the Graduate Forum)</strong><br>Presentation topic: Digital Twin-driven Multi-objective Deep Reinforcement Learning for Dynamic and Flexible Island Assembly Line Scheduling.</p>
          </div>
<div class="note success">
            <p>-4. <font color="blue">(2024. 12)</font>. 2024 World Intelligent Manufacturing Conference – Human-Centered Smart Manufacturing Session &amp; The 2rd Human-Centric Smart Manufacturing Academic Conference, Nanjing, China. <strong>(Outstanding Presentation Award)</strong> <a href="../zip"> (Poster) </a> <strong><code>PDF</code></strong><br>Presentation topic: Autonomous Collaboration Strategies for Mobile Operations in Robot Clusters.</p>
          </div>
<div class="note success">
            <p>-5. <font color="blue">(2024. 12)</font>.. 2024 National Doctoral Forum on Industrial Engineering &amp; the 772nd Tsinghua University Doctoral Academic Forum, Beijing, China. <strong>(Forum Presentation)</strong><br>Presentation topic: Fuzzy Superposition Operation and Knowledge-driven Co-evolutionary Algorithm for Integrated Production Scheduling and Vehicle Routing Problem with Soft Time Windows and Fuzzy Travel Times.</p>
          </div>
<hr>
<h1 id="Project"><a href="#Project" class="headerlink" title="Project"></a>Project</h1><h3 id="Fund"><a href="#Fund" class="headerlink" title="Fund"></a>Fund</h3><div class="note warning">
            <ol><li><p><font color="blue">2024. 12-2027. 11</font> The National Key Research and Development Program of China :<br>Research on Cyber-Physical Fusion Modeling Technology for Control System Equipment Targeting Multiple Optimization Objectives in Flexible Manufacturing （Project Participants）</p></li><li><p><font color="blue">2025. 01-2027. 12</font> The National Natural Science Foundation of China  (Youth Fund) :<br>Research on Adaptive Reconfiguration Optimization Theory for Smart Manufacturing Systems with Multiple Uncertain Disturbances （Project Participants）</p></li><li><p><font color="blue">2024. 07-2027. 12</font> Beijing Natural Science Foundation under Grant :<br>Key Technologies and System Applications of Digital Twin Workshop for 3C Intelligent Manufacturing （Project Participants）</p></li><li><p><font color="blue">2022. 12-2024. 10</font> The National Key Research and Development Program of China:<br>Research on Key Technologies for Dynamic Reconfiguration and XXXX of Production Lines （Project Participants）</p></li><li><p><font color="blue">2022. 01-2022. 12</font> Independent Innovation Foundation of Wuhan University of Technology —— Postgraduate Personal Exploration Project :<br>Research on Integrated  Production and Distribution Scheduling with Flexible Batching and Soft Time Windows （<strong>Project Principals</strong>）</p></li><li><p><font color="blue">2018. 01-2020. 12 </font> National Natural Science Foundation of China (Youth Fund) ：<br>A game method of supply and demand allocation of complex product system manufacturing services for social collaboration （Project Participants）</p></li></ol>
          </div>
<h3 id="Internship"><a href="#Internship" class="headerlink" title="Internship"></a>Internship</h3><div class="note warning">
            <h4 id="—-2021-07-2022-07-Tuoxing-Zhiwang-Technology-Co-Ltd-Wuhan-（Java-Back-End-Development）"><a href="#—-2021-07-2022-07-Tuoxing-Zhiwang-Technology-Co-Ltd-Wuhan-（Java-Back-End-Development）" class="headerlink" title="— 2021. 07-2022. 07 Tuoxing Zhiwang Technology Co., Ltd.  Wuhan （Java Back End Development）"></a>— 2021. 07-2022. 07 <a href="http://www.tonshinet.com/">Tuoxing Zhiwang Technology Co., Ltd.  Wuhan</a> （Java Back End Development）</h4><ol><li><p><font color="blue">2021. 07-2021. 12 </font> Development of Wind Plant Operation and Maintenance Application Platform</p></li><li><p><font color="blue">2022. 01-2022. 07 </font> Tangshan Sinoma Heavy Machinery PMS Improvement Project<br>-Development of External Supplier Price Comparison Management Platform </p></li></ol>
          </div>
<p><a href="https://clustrmaps.com/site/1bqdf" title="Visit tracker"><img src="//clustrmaps.com/map_v2.png?cl=555555&amp;w=500&amp;t=m&amp;d=lPGunm_IA1w4rADFSr_XZ_r1KQCj0GoM-18qXvbMs1E&amp;co=ffffff&amp;ct=555555"></a></p>
</div>]]></content>
  </entry>
  <entry>
    <title>Datasets and Algorithm Codes</title>
    <url>/resources/index.html</url>
    <content><![CDATA[<h1 id="Datesets"><a href="#Datesets" class="headerlink" title="Datesets"></a>Datesets</h1><h4 id="1-Integrated-Production-and-Distribution-Scheduling-Problem-IPDS"><a href="#1-Integrated-Production-and-Distribution-Scheduling-Problem-IPDS" class="headerlink" title="1. Integrated Production and Distribution Scheduling Problem(IPDS)"></a>1. Integrated Production and Distribution Scheduling Problem(IPDS)</h4><p><font color="#428BCA" size="5"><strong>Original Intention</strong></font>:<br>IPDS involves the optimization of production scheduling and distribution scheduling. Individually, production scheduling or distribution scheduling has been studied for a long time and has rich benchmark datasets and excellent solution algorithms. <strong>However, there is a paucity of benchmark datasets in IPDS studies.</strong> Due to the complexity and specificity of the problem, most studies have adopted randomly generated (or partially randomly generated) datasets for computational experiments, but few studies have published their test datasets for use and validation by other researchers. In order to provide a uniform benchmark for evaluating and measuring the performance of algorithms for the same and similar studies, we publish the used datasets and algorithms to facilitate the computational experiments of other researchers.</p>
<div class="note info">
            <ul><li><strong>IPSVRP-BM&amp;STW Dateset（<span class="github-emoji"><span>🌟</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f31f.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>）</strong><br>（Beat Production Lines &amp; Vehicle Routing Problem with Soft Time Window and Heterogeneous vehicles）<br>DataSet：<a href="../zip/IPSVRP-BM-STW-Dateset.zip">IPSVRP-BM&amp;STW Dateset</a><br><a href="../pdf/A%20hybrid%20collaborative%20framework%20for%20integrated%20production%20scheduling%20and%20vehicle%20routing%20problem%20with%20batch%20manufacturing%20and%20soft%20time%20windows.pdf">The paper</a> proposing this dataset<br><a href="link">A blog</a> introducing the formation process of this dataset</li></ul>
          </div>
<div class="note info">
            <ul><li><strong>Fuzzy-IPDS Dateset（<span class="github-emoji"><span>🌟</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f31f.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>）</strong><br>（Beat Production Lines &amp; Vehicle Routing Problem with Soft Time Window and Heterogeneous vehicles &amp; Fuzzy Travel time）<br>DataSet：<a href="../zip/Fuzzy-IPDS-Dateset.zip">Fuzzy-IPDS Dateset</a><br><a href="link">The paper</a> proposing this dataset<br><a href="link">A blog</a> introducing the formation process of this dataset</li></ul>
          </div>
<div class="note info">
            <ul><li><strong>IPSVRP-STW&amp;FTT Dateset（<span class="github-emoji"><span>🌟</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f31f.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>）</strong><br>（Beat Production Lines &amp; Soft Time Window &amp; A limited number of Muti-trip Heterogeneous &amp; Fuzzy Travel time）<br>DataSet：<a href="../zip/IPSVRP-STW-FTT-Dateset.zip">IPSVRP-STW&amp;FTT Dateset</a><br><a href="link">The paper</a> proposing this dataset<br><a href="link">A blog</a> introducing the formation process of this dataset</li></ul>
          </div>
<h4 id="2-Datasets-of-other-researchers"><a href="#2-Datasets-of-other-researchers" class="headerlink" title="2. Datasets of other researchers"></a>2. Datasets of other researchers</h4><ul>
<li><p>VRPTW Datesets: <strong><a href="https://www.sintef.no/projectweb/top/vrptw/">Solomon-Benchmark</a></strong></p>
</li>
<li><p>VRP Datesets: <strong><a href="https://neo.lcc.uma.es/vrp/vrp-instances/">Vrp-Instances</a></strong></p>
</li>
<li><p>Test datasets based on problem instances: <strong><a href="http://people.brunel.ac.uk/~mastjjb/jeb/info.html">OR-Library by J E Beasley</a></strong></p>
</li>
</ul>
<hr>
<h1 id="Algorithm-Code"><a href="#Algorithm-Code" class="headerlink" title="Algorithm Code"></a>Algorithm Code</h1><div class="note info">
            <ol><li><a href="link"> HCF-IMOEA </a> <strong><code>Matlab code</code></strong> （<span class="github-emoji"><span>🌟</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f31f.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>）<br><a href="https://www.sciencedirect.com/science/article/pii/S0305054823002101">This paper</a> <strong><code>HTML</code></strong> proposing this algorithm</li></ol>
          </div>
<div class="note info">
            <ol><li><a href="../zip/NSGA-II-AVNS-Code.zip"> NSGA-II-AVNS </a> <strong><code>Matlab code</code></strong> （<span class="github-emoji"><span>🌟</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f31f.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>）<br><a href="https://kns.cnki.net/kcms2/article/abstract?v=xNq_RSSxttstktCq-VWRIQS1e6fIAq26YV2DyMFsCVB9NtbxKqFgDZtEE5MiKN1dG5PLmIItcZsrbA9y-tAUQMMLpbYZJEJr4NENoL1ughgxkkSL6gjYdqp7OcHpgSim7TjKF6gvjYE=&amp;uniplatform=NZKPT&amp;language=CHS">This paper</a><strong><code>HTML</code></strong> proposing this algorithm</li></ol>
          </div>
<div class="note info">
            <ol><li><a href="../zip/KDCEA-Code.zip"> KDCEA </a> <strong><code>Matlab code</code></strong> （<span class="github-emoji"><span>🌟</span><img src="https://github.githubassets.com/images/icons/emoji/unicode/1f31f.png?v8" aria-hidden="true" onerror="this.parent.classList.add('github-emoji-fallback')"></span>）<br><a href="https://ieeexplore.ieee.org/document/10497842">This paper</a> <strong><code>HTML</code></strong> proposing this algorithm</li></ol>
          </div>
<hr>
]]></content>
  </entry>
</search>
