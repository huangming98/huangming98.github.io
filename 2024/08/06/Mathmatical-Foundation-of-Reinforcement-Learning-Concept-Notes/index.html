<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|Roboto:300,300italic,400,400italic,700,700italic|JetBrains Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.huangm.cn","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","Muse | Mist":180,"width":250,"display":"always","padding":20,"offset":12,"onmobile":true},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":"ture","lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Learning Notes for the Course of &quot;Mathematical Foundation of Reinforcement Learning&quot;（1~3）">
<meta property="og:type" content="article">
<meta property="og:title" content="Mathematical Foundation of Reinforcement Learning-Concept Notes">
<meta property="og:url" content="https://www.huangm.cn/2024/08/06/Mathmatical-Foundation-of-Reinforcement-Learning-Concept-Notes/index.html">
<meta property="og:site_name" content="Ming Huang&#39;s Homepage">
<meta property="og:description" content="Learning Notes for the Course of &quot;Mathematical Foundation of Reinforcement Learning&quot;（1~3）">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://www.huangm.cn/2024/08/06/Mathmatical-Foundation-of-Reinforcement-Learning-Concept-Notes/grid%20world.jpg">
<meta property="og:image" content="https://www.huangm.cn/2024/08/06/Mathmatical-Foundation-of-Reinforcement-Learning-Concept-Notes/state%20and%20action.jpg">
<meta property="og:image" content="https://www.huangm.cn/2024/08/06/Mathmatical-Foundation-of-Reinforcement-Learning-Concept-Notes/2024-04-11-12-33-12.png">
<meta property="og:image" content="https://www.huangm.cn/2024/08/06/Mathmatical-Foundation-of-Reinforcement-Learning-Concept-Notes/2024-04-11-13-19-07.png">
<meta property="og:image" content="https://www.huangm.cn/2024/08/06/Mathmatical-Foundation-of-Reinforcement-Learning-Concept-Notes/2024-04-12-21-11-39.png">
<meta property="og:image" content="https://www.huangm.cn/2024/08/06/Mathmatical-Foundation-of-Reinforcement-Learning-Concept-Notes/2024-04-12-21-14-38.png">
<meta property="og:image" content="https://www.huangm.cn/2024/08/06/Mathmatical-Foundation-of-Reinforcement-Learning-Concept-Notes/2024-04-12-22-00-41.png">
<meta property="og:image" content="https://www.huangm.cn/2024/08/06/Mathmatical-Foundation-of-Reinforcement-Learning-Concept-Notes/2024-04-18-22-25-49.png">
<meta property="article:published_time" content="2024-08-06T03:30:06.000Z">
<meta property="article:modified_time" content="2024-10-23T03:55:12.000Z">
<meta property="article:author" content="Ming Huang">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.huangm.cn/2024/08/06/Mathmatical-Foundation-of-Reinforcement-Learning-Concept-Notes/grid%20world.jpg">

<link rel="canonical" href="https://www.huangm.cn/2024/08/06/Mathmatical-Foundation-of-Reinforcement-Learning-Concept-Notes/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Mathematical Foundation of Reinforcement Learning-Concept Notes | Ming Huang's Homepage</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>


<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Ming Huang's Homepage</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/home" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-publications">

    <a href="/publications/" rel="section"><i class="fa fa-book fa-fw"></i>Publications</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="fa fa-cloud fa-fw"></i>Resources</a>

  </li>
        <li class="menu-item menu-item-link">

    <a href="/link/" rel="section"><i class="fa fa-link fa-fw"></i>Scholars Link</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Blog</a>

  </li>
        <li class="menu-item menu-item-switch-to-chinese">

    <a href="https://www.huangm.cn/cn/" rel="section"><i class="fa fa-language fa-fw"></i>Switch to Chinese</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://www.huangm.cn/2024/08/06/Mathmatical-Foundation-of-Reinforcement-Learning-Concept-Notes/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/pic.png">
      <meta itemprop="name" content="Ming Huang">
      <meta itemprop="description" content="Let’s do something extraordinary together.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ming Huang's Homepage">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Mathematical Foundation of Reinforcement Learning-Concept Notes
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-08-06 11:30:06" itemprop="dateCreated datePublished" datetime="2024-08-06T11:30:06+08:00">2024-08-06</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-10-23 11:55:12" itemprop="dateModified" datetime="2024-10-23T11:55:12+08:00">2024-10-23</time>
              </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>41k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>1:15</span>
            </span>
            <div class="post-description">Learning Notes for the Course of "Mathematical Foundation of Reinforcement Learning"（1~3）</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="Reference-Sources"><a href="#Reference-Sources" class="headerlink" title="Reference Sources:"></a>Reference Sources:</h2><ol>
<li><a target="_blank" rel="noopener" href="https://github.com/MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning"><em>Shiyu Zhao</em>. 《Mathematical Foundation of Reinforcement Learning》Chapter 1-3</a>.</li>
<li><a target="_blank" rel="noopener" href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#">OpenAI Spinning Up</a></li>
<li><a target="_blank" rel="noopener" href="https://lyk-love.cn/tags/reinforcement-learning/">Lu yukuan’s notes</a></li>
</ol>
<h2 id="1-Basic-Concepts"><a href="#1-Basic-Concepts" class="headerlink" title="1. Basic Concepts"></a>1. Basic Concepts</h2><h3 id="1-1-A-grid-world-example-Consistent-example"><a href="#1-1-A-grid-world-example-Consistent-example" class="headerlink" title="1.1 A grid world example (Consistent example)"></a>1.1 A grid world example (Consistent example)</h3><div align="center"><img src="/2024/08/06/Mathmatical-Foundation-of-Reinforcement-Learning-Concept-Notes/grid%20world.jpg" width="50%" alt="Figure 1.2: The grid world example is used throughout the book."></div>

<p>Consider an example as shown in Figure, where a robot moves in a grid world. The robot, called agent, can move across adjacent cells in the grid. At each time step, it can only occupy a single cell.The white cells are <em>accessible</em> for entry, and the orange cells are <em> forbidden </em>. There is a <em>target</em> cell that the robot would like to reach. We will use such grid world examples throughout  RL since they are intuitive for illustrating new concepts and algorithms.</p>
<p>How can the goodness of a policy be defined? The idea is that the agent should reach the target <strong>without entering any forbidden cells, taking unnecessary detours, or colliding with the boundary of the grid</strong>.It would be trivial to plan a path to reach the target cell if the agent knew the map of the grid world.<em>The task becomes nontrivial if the agent does not know any information about the environment in advance.</em> Then,  <strong>the agent must interact with the environment to find a good policy by trial and error.</strong> To do that, the concepts presented in the rest of the chapter are necessary.</p>
<h3 id="1-2-State-and-action-s-n-a-m"><a href="#1-2-State-and-action-s-n-a-m" class="headerlink" title="1.2 State and action ( $ s_n, a_m $ )"></a>1.2 State and action ( $ s_n, a_m $ )</h3><p><strong><em>State</em> describes the agent’s status with respect to the environment.</strong> The set of all the states is called the state space, denoted as $\mathcal{S}=\left\{s_1,s_2, \ldots, s_n\right\}$.</p>
<p>For each state, the agent can take actions $\mathcal{A}$. Different states can have different action spaces ($\mathcal{A}(s)$ when at state $s$) . The set of all actions is called the action space, denoted as $\mathcal{A}=\left\{a_1, a_2,\ldots, a_m\right\}$.</p>
<div align="center"><img src="/2024/08/06/Mathmatical-Foundation-of-Reinforcement-Learning-Concept-Notes/state%20and%20action.jpg" width="70%" alt="Figure 1.3: Illustrations of the state and action concepts."></div>

<p>In this RL serie, we consider the most general case: $\mathcal{A}\left(s_i\right)=\mathcal{A}=\left\{a_1, \ldots, a_5\right\}$ for all $i$.</p>
<h3 id="1-3-States-and-Observations"><a href="#1-3-States-and-Observations" class="headerlink" title="1.3 States and Observations"></a>1.3 States and Observations</h3><p>You may see the terminology “observation”. It’s similar to “states”. Whatt’s the difference?</p>
<ul>
<li>A <strong>state</strong> $s$ is a complete description of the state of the world. There is no  information about the world which is hidden from the state. </li>
<li><p>An <strong>observation</strong> $o$ is a partial description of a state, which may omit information.</p>
</li>
<li><p>The environment is <strong>fully observed</strong> when the agent is able to observe the complete state of the environment.</p>
</li>
<li>The environment is <strong>partially observed</strong> when the agent can only see a partial observation.</li>
</ul>
<p>Reinforcement learning notation sometimes puts the symbol for state ($s$), in places where it would be technically more appropriate to write the symbol for observation($o$). </p>
<p>Specifically, this happens when talking about how the agent decides an action: we often signal in notation that the action is conditioned on the state, when in practice, <u>the action is conditioned on the observation because the agent does not have access to the state</u>.</p>
<h3 id="1-4-State-transition-p-s-n-mid-s-1-a-2-x"><a href="#1-4-State-transition-p-s-n-mid-s-1-a-2-x" class="headerlink" title="1.4 State transition ( $p(s_{n} \mid s_1, a_{2})=x$ );"></a>1.4 State transition ( $p(s_{n} \mid s_1, a_{2})=x$ );</h3><p>When taking an action, the agent may move from one state to another. Such a process is called <strong>state transition</strong>. For example, if the agent is at state $s_1$ and selects action $a_2$ , then the agent moves to state $s_2$. Such a process can be expressed as</p>
<script type="math/tex; mode=display">
s_1 \stackrel{a_2}{\rightarrow} s_2</script><ul>
<li>the agent will be bounced back because it is impossible for the agent to exit the state space. Hence, we have $s_1 \stackrel{a_1}{\rightarrow} s_1$.</li>
<li>What is the next state when the agent attempts to enter a forbidden cell, for example, taking action $a_2$ at state $s_5$ ? Two different scenarios may be encountered. </li>
<li><ol>
<li>In the first scenario, although $s_6$ is forbidden, it is still accessible. In this case, the next state is $s_6$; hence, the state transition process is $s_5 \stackrel{a_2}{\longrightarrow} s_6$. </li>
</ol>
</li>
<li><ol>
<li>In the second scenario, $s_6$ is not accessible because, for example, it is surrounded by walls. In this case, the agent is bounced back to $s_5$ if it attempts to move rightward; hence, the state transition process is $s_5 \stackrel{a_2}{\rightarrow} s_5$.</li>
</ol>
</li>
<li>Which scenario should we consider? The answer depends on the physical environment. In this series, <u>we consider the first scenario where the forbidden cells are accessible, although stepping into them may get punished</u>. This scenario is more general and interesting. Moreover, since we are considering a simulation task, <strong>we can define the state transition process however we prefer.</strong> In real-world applications, the state transition process is determined by real-world dynamics.</li>
</ul>
<p>Mathematically, the state transition process can be described by conditional probabilities.  $p(s_{n} \mid s_1, a_{2})=x , x\in(0,1)$; </p>
<h3 id="1-5-Policy-pi-a-mid-s"><a href="#1-5-Policy-pi-a-mid-s" class="headerlink" title="1.5 Policy ($\pi(a \mid s)$)"></a>1.5 Policy ($\pi(a \mid s)$)</h3><p><strong>A policy tells the agent which actions to take at every state.</strong> Intuitively, policies can be depicted as arrows (see Figure 1.4(a)). Following a policy, the agent can generate a trajectory starting from an initial state (see Figure 1.4(b)).</p>
<p></p><div align="center"><img src="/2024/08/06/Mathmatical-Foundation-of-Reinforcement-Learning-Concept-Notes/2024-04-11-12-33-12.png" width="80%" alt="Figure 1.4: A policy represented by arrows and some trajectories obtained by starting from different initial states."></div><br>A policy can be deterministic or stochastic.<p></p>
<ol>
<li><p>Deterministic policy<br> A <strong>deterministic</strong> policy is usually denoted by $\mu$ :</p>
<script type="math/tex; mode=display">
 a_t=\mu\left(s_t\right) .</script></li>
<li><p>Stochastic policy (conditional probabilities)<br> A <strong>stochastic</strong> policy is usually denoted by $\pi$ :</p>
<script type="math/tex; mode=display">
 a_t \sim \pi\left(\cdot \mid s_t\right) .</script><p> In this case, at state $s$, the probability of choosing action $a$ is $\pi(a \mid s)$. It holds that $\sum_{a \in \mathcal{A}(s)} \pi(a \mid s)=1$ for any $s \in \mathcal{S}$.</p>
</li>
</ol>
<p>In RL, the policies are often <strong>parameterized</strong>, the parameters are commonly denoted as $\theta$ or $\phi$ and are written as a subscript on the policy symbol:</p>
<script type="math/tex; mode=display">
\begin{aligned}
a_t & = \mu_{\theta}(s_t) \\ 
a_t & \sim \pi_{\theta}(\cdot | s_t).
\end{aligned}</script><h3 id="1-6-Rewards-r-s-a-p-r-mid-s-a"><a href="#1-6-Rewards-r-s-a-p-r-mid-s-a" class="headerlink" title="1.6 Rewards ($r(s, a)$, $p(r \mid s,a)$)"></a>1.6 Rewards ($r(s, a)$, $p(r \mid s,a)$)</h3><p>After executing an action at a state, the agent obtains a reward, denoted as $r$, as feedback from the environment. The reward is a function of the state $s$ and action $a$. Hence, it is also denoted as $r(s, a)$.  Its value can be a positive or negative real number or zero. </p>
<p>In the grid world example, the rewards are designed as follows:</p>
<ol>
<li>If the agent attempts to exit the boundary, let $r_{\text {boundary }}=-1$.</li>
<li>If the agent attempts to enter a forbidden cell, let $r_{\text {forbidden }}=-1$.</li>
<li>If the agent reaches the target state, let $r_{\text {target }}=+1$.</li>
<li>Otherwise, the agent obtains a reward of $r_{\text {other }}=0$.</li>
</ol>
<p><strong>A reward can be interpreted as a human-machine interface</strong> , with which we can guide the agent to behave as we expect. For example, with the rewards designed above, we can expect that the agent tends to avoid exiting the boundary or stepping into the forbidden cells. <u>Designing appropriate rewards is an important step in reinforcement learning. This step is, <strong>however, nontrivial for complex tasks since it may require the user to understand the given problem well.</strong> Nevertheless, it may still be much easier than solving the problem with other approaches that require a professional background or a deep understanding of the given problem.</u></p>
<p>One question that beginners may ask is as follows: if given the table of rewards, can we find good policies by simply selecting the actions with the greatest rewards? The answer is no. That is because these rewards are immediate rewards that can be obtained after taking an action. To determine a good policy, we must consider the total reward obtained in the long run (see Section 1.7 for more information). <strong>An action with the greatest immediate reward may not lead to the greatest total reward.</strong> It is possible that the immediate reward is negative while the future reward is positive. Thus, which actions to take should be determined by the return (i.e., the total reward) rather than the immediate reward to avoid short-sighted decisions.</p>
<h3 id="1-7-Trajectories-and-Return"><a href="#1-7-Trajectories-and-Return" class="headerlink" title="1.7 Trajectories and Return"></a>1.7 Trajectories and Return</h3><p>A <strong>trajectory</strong> $\tau$ is a state-action-reward chain $\tau = (s_1, a_1, s_2, a_2, …)$.</p>
<p></p><div align="center"><img src="/2024/08/06/Mathmatical-Foundation-of-Reinforcement-Learning-Concept-Notes/2024-04-11-13-19-07.png" width="100%" alt="Figure 1.6: Trajectories obtained by following two policies. The trajectories are indicated by red dashed lines."></div><br>For example, given the policy shown in Figure 1.6(a), if the agent can move along a trajectory as follows:<p></p>
<script type="math/tex; mode=display">
s_1 \underset{r=0}{\stackrel{a_2}{\longrightarrow}} s_2 \underset{r=0}{\stackrel{a_3}{\longrightarrow}} s_5 \underset{r=0}{\stackrel{a_3}{\longrightarrow}} s_8 \underset{r=1}{\stackrel{a_2}{\longrightarrow}} s_9 .</script><p>The undiscounted <strong>return</strong> of this trajectory is defined as the sum of all the rewards collected along the trajectory:</p>
<script type="math/tex; mode=display">
\text { return }=0+0+0+1=1 .</script><p>This return is <strong>undiscounted</strong>. But in RL we usually consider <strong>discounted</strong> return with a discount rate $\gamma \in (0,1)$ especially for the trajectory of infinite length.</p>
<script type="math/tex; mode=display">
\text { return }= 0+ \gamma 0+ \gamma^2 0+ \gamma^3 1 = \gamma^3 .</script><p><strong>The introduction of the discount rate is useful for the following reasons.</strong> </p>
<ol>
<li>it <em>removes the stop criterion</em> and <em>allows for infinitely long trajectories</em>. </li>
<li><em>the discount rate can be used to adjust the emphasis placed on near- or far-future rewards.</em> In particular, if is close to 0, then the agent places more emphasis on rewards obtained in the near future. The resulting policy would be short-sighted. If is close to 1, then the agent places more emphasis on the far future rewards. The resulting policy is far-sighted and dares to take risks of obtaining negative rewards in the near future. </li>
</ol>
<p><u>Returns are also called <em>total rewards</em> or <em>cumulative rewards</em>.</u></p>
<h3 id="1-8-Episodes"><a href="#1-8-Episodes" class="headerlink" title="1.8 Episodes"></a>1.8 Episodes</h3><p>When interacting with the environment by following a policy, the agent may <u>stop at some terminal states</u>. In this case, the resulting trajectory is <u>finite</u>, and is called an <strong>episode</strong> (or a <strong>trial</strong>, <strong>rollout</strong>).</p>
<p>However, some tasks may have no terminal states. In this case, the resulting trajectory is <u>infinite</u>.</p>
<p>Tasks with episodes are called <u>episodic tasks</u>.  some tasks have no terminal states are called <u>continuing tasks</u>.</p>
<p>In fact, we can treat episodic and continuing tasks in a unified mathematical manner by converting episodic tasks to continuing ones. We have two options:</p>
<ol>
<li>First, if we treat the terminal state as a special state, we can specifically design its action space or state transition so that the agent stays in this state forever. Such states are called absorbing states, meaning that the agent never leaves a state once reached.</li>
<li>Second, if we treat the terminal state as a normal state, we can simply set its action space to the same as the other states, and the agent may leave the state and come back again. <u>Since a positive reward of $r=1$ can be obtained every time $s_9$ is reached, the agent will eventually learn to stay at $s_9$ forever (self-circle)</u> to collect more rewards.</li>
</ol>
<p>In this RL series, we consider the second scenario where the target state is treated as a normal state whose action space is $\mathcal{A}\left(s_9\right)=\left\{a_1, \ldots, a_5\right\}$.</p>
<h3 id="1-9-Markov-decision-processes"><a href="#1-9-Markov-decision-processes" class="headerlink" title="1.9 Markov decision processes"></a>1.9 Markov decision processes</h3><p>This section presents the basic RL concepts in a more formal way under the framework of Markov decision processes (MDPs).</p>
<p>An MDP is a general framework for describing stochastic dynamical systems. The key ingredients of an MDP are listed below.</p>
<p><strong>Sets</strong>:</p>
<ul>
<li>— State space: the set of all states, denoted as $\mathcal{S}$.</li>
<li>— Action space: a set of actions, denoted as $\mathcal{A}(s)$, associated with each state $s \in \mathcal{S}$.</li>
<li>— <strong>Reward set</strong>: a set of rewards, denoted as $\mathcal{R}(s, a)$, associated with each state-action pair $(s, a)$.</li>
</ul>
<p>Note:</p>
<ol>
<li>The state, action, reward at time index $t$ are denoted as $s_t, a_t, r_t$ separately.</li>
<li>The reward depends on the state $s$ and action $a$, but not the next state $s^{\prime}$. because since $s^{\prime}$ also depends on $s$ and $a$ i.e. $p(s^{\prime}\mid s,a)$ ($s,a$后下一状态可能是随机的，$s’$代表的下一状态是固定的一种情况，因此与$s’$无关只与$s,a$有关).</li>
</ol>
<p><strong>Model</strong>:</p>
<ul>
<li><strong>State transition probability</strong>: At state $s$, when taking action $a$, the probability of transitioning to state $s^{\prime}$ is $p\left(s^{\prime} \mid s, a\right)$. It holds that $\sum_{s^{\prime} \in \mathcal{S}} p\left(s^{\prime} \mid s, a\right)=1$ for any $(s, a)$.</li>
<li><strong>Reward probability</strong>: At state $s$, when taking action $a$, the probability of obtaining reward $r$ is $p(r \mid s, a)$. It holds that $\sum_{r \in \mathcal{R}(s, a)} p(r \mid s, a)=1$ for any $(s, a)$.</li>
</ul>
<p>Note:</p>
<script type="math/tex; mode=display">
p(r \mid s,a )=\sum_{s^{\prime} \in \mathcal{S}}p(r \mid s,a,s^{\prime} )p(s^{\prime} \mid s,a)</script><p><strong>Policy</strong>: At state $s$, the probability of choosing action $a$ is $\pi(a \mid s)$. It holds that $\sum_{a \in \mathcal{A}(s)} \pi(a \mid s)=1$ for any $s \in \mathcal{S}$.</p>
<p><strong>Markov property</strong>:</p>
<p>The name Markov Decision Process refers to the fact that the system obeys the Markov property, <strong>the memoryless property of a stochastic process.</strong> </p>
<p>Mathematically, it means that</p>
<ol>
<li><p>The state is markovian:</p>
<script type="math/tex; mode=display">
p(s_{t+1} \mid s_t, s_{t-1}, \cdots, s_0)=p(s_{t+1} \mid s_t)</script></li>
<li><p>The state transition is markovian:</p>
<script type="math/tex; mode=display">
p\left(s_{t+1} \mid s_t, a_t, s_{t-1}, a_{t-1}, \ldots, s_0, a_0\right)=p\left(s_{t+1} \mid s_t, a_t\right)</script></li>
<li><p>The action itself doesn’t have markov property, but it’s defined to only rely on current state and policy $a_t \sim \pi\left(\cdot \mid s_t\right)$,</p>
<script type="math/tex; mode=display">
p(a_{t+1}|s_{t+1}, s_t, \cdots, s_0) =  p(a_{t+1}|s_{t+1}).</script><p>$a_t$ doesn’t depends on $s_{t-1}, s_{t-2}, \cdots$.</p>
</li>
<li><p>The reward $r_{t+1}$ itself doesn’t have markov property, but it’s defined to only rely on $s_t, a_t$:</p>
<script type="math/tex; mode=display">
p\left(r_{t+1} \mid s_t, a_t, s_{t-1}, a_{t-1}, \ldots, s_0, a_0\right)=p\left(r_{t+1} \mid s_t, a_t\right).</script></li>
</ol>
<p>$t$ represents the current time step and $t+1$ represents the next time step. </p>
<p>Here, $p\left(s^{\prime} \mid s, a\right)$ and $p(r \mid s, a)$ for all $(s, a)$ are called the <strong>model or dynamics</strong>. The model can be either stationary or nonstationary (or in other words, time-invariant or time-variant). A stationary model does not change over time; a nonstationary model may vary over time. For instance, in the grid world example, if a forbidden area may pop up or disappear sometimes, the model is nonstationary. Unless specifically stated, <u>we only consider stationary models</u>.</p>
<p><strong>note:</strong> model or dynamics are essentially state transitions and reward acquisition.</p>
<h3 id="1-10-Markov-processes"><a href="#1-10-Markov-processes" class="headerlink" title="1.10 Markov processes"></a>1.10 Markov processes</h3><p>One may have heard about the Markov processes (MPs). What is the difference between an MDP and an MP? The answer is that, <u>once the policy in an MDP is fixed, the MDP degenerates into an MP.</u> In this book, the terms “Markov process” and “Markov chain” are used interchangeably when the context is clear. Moreover, this book mainly considers finite MDPs where the numbers of  states and actions are finite. </p>
<h3 id="1-11-What-is-reinforcement-learning"><a href="#1-11-What-is-reinforcement-learning" class="headerlink" title="1.11 What is reinforcement learning"></a>1.11 What is reinforcement learning</h3><p>Reinforcement learning can be described as an <strong>agent-environment interaction process</strong>. The agent is a decision-maker that can <em>sense its state, maintain policies, and execute actions</em>. Everything outside of the agent is regarded as the environment.<br>In the grid world examples, the agent and environment correspond to the robot and grid world, respectively. After the agent decides to <strong>take an action</strong>, the actuator executes such a decision. Then, the state of the agent would be changed and <strong>a reward can be obtained</strong>. By using interpreters, the agent can interpret the new state and the reward. Thus, a closed loop can be formed.</p>
<hr>
<h2 id="2-State-Values-Bellman-Equation-and-Action-Values"><a href="#2-State-Values-Bellman-Equation-and-Action-Values" class="headerlink" title="2. State Values, Bellman Equation and Action Values"></a>2. State Values, Bellman Equation and Action Values</h2><p><strong>ABSTRACT</strong>: The <strong>state value</strong> is the average reward an agent can obtain if it follows a given policy, which is used as a metric to evaluate whether a policy is good or not. <strong>Bellman equation</strong> describes the relationships between the values of all states, which is an important tool for analyzing state values. By solving the Bellman equation, we can obtain the state values. This process is called policy evaluation, which is a fundamental concept in reinforcement learning. The <strong>action value</strong> is introduced to describe the value of taking one action at a state.  Action values play a more direct role than state values when we attempt to find optimal policies. </p>
<h3 id="2-1-State-Values"><a href="#2-1-State-Values" class="headerlink" title="2.1 State Values"></a>2.1 State Values</h3><p>The goal of RL is to find an excellent policy that achieves the <strong>maximal return (or rewards)</strong>. Although returns can be used to evaluate policies, it is more formal to use state values to evaluate policies(the policy or system model may be stochastic): policies that generate greater state values are better. Therefore, state values constitute a core concept in reinforcement learning. How to calculate it? This question is answered in the next section 2.2.<br>Consider a random state-action-reward trajectory for a sequence of time steps $t = 0, 1, 2 \dots$:</p>
<script type="math/tex; mode=display">
S_t \xrightarrow{A_t} R_{t+1}, S_{t+1} \stackrel{A_{t+1}}{\longrightarrow} R_{t+2}, S_{t+2} \stackrel{A_{t+2}}{\longrightarrow} R_{t+3}, \ldots</script><p>The discounted return of the trajectory is defined as</p>
<script type="math/tex; mode=display">
\begin{aligned}
G_t & =R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\ldots \\
& =R_{t+1}+\gamma\left(R_{t+2}+\gamma R_{t+3}+\ldots\right) \\
& =R_{t+1}+\gamma G_{t+1}
\end{aligned}</script><p>where $R_{t+1}$ is the immediate reward of arriving state $S_{t+1}$ or leaving $S_t$, $\gamma$ is the discount rate. Note that $S_t,A_t,R_{t+1} G_t$ are all random variables.</p>
<p><u>The expectation (or called expected value or mean) of $G_t$ is called the <strong>state-value function</strong> or simply <strong>state value</strong></u>:</p>
<script type="math/tex; mode=display">
\begin{equation} 
v_\pi(s)=\mathbb{E}\left[G_t \mid S_t=s\right]
\end{equation}</script><p>Remarks:</p>
<ul>
<li>It is a function of $s$. It is a conditional expectation with the condition that the agent starts from state $s$.</li>
<li>It is based on the policy $\pi$. For a different policy, the state value may be different.</li>
<li>It represents the “value” of a state. If the state value is greater, then the policy is better because greater cumulative rewards can be obtained.</li>
</ul>
<p><u>The relationship between state values and returns is further clarified as follows</u>. When both <strong>the policy and the system model are deterministic</strong>, starting from a state always leads to the same trajectory. In this case, the return obtained starting from a state is equal to the value of that state. By contrast, when either <strong>the policy or the system model is stochastic</strong>, starting from the same state may generate different trajectories. In this case, the returns of different trajectories are different, and the state value is the mean of these returns.</p>
<h3 id="2-2-Bellman-Equation"><a href="#2-2-Bellman-Equation" class="headerlink" title="2.2 Bellman Equation"></a>2.2 Bellman Equation</h3><p>the Bellman equation is a set of linear equations that describe the relationships between the values of all the states. Given a policy, finding out the corresponding state values from the Bellman equation is called <strong>policy evaluation</strong>. It is an important process in many reinforcement learning algorithms.<br>Recalling the definition of the state value , we substitute $G(t) = R_{t+1}+\gamma G_{t+1}$ into it:</p>
<script type="math/tex; mode=display">
\begin{aligned}
v_\pi(s) & =\mathbb{E}\left[G_t \mid S_t=s\right] \\
& =\mathbb{E}\left[R_{t+1}+\gamma G_{t+1} \mid S_t=s\right] \\
& =\mathbb{E}\left[R_{t+1} \mid S_t=s\right]+\gamma \mathbb{E}\left[G_{t+1} \mid S_t=s\right]
\end{aligned}</script><p>Next we calculate the two terms of the last line, respectively.</p>
<h4 id="2-2-1-First-term-the-mean-of-immediate-rewards"><a href="#2-2-1-First-term-the-mean-of-immediate-rewards" class="headerlink" title="2.2.1 First term: the mean of immediate rewards"></a>2.2.1 First term: the mean of immediate rewards</h4><p>First, calculate the first term $\mathbb{E}\left[R_{t+1} \mid S_t=s\right]$, is the mean of immediate rewards. </p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbb{E}\left[R_{t+1} \mid S_t=s\right] & =\sum_{a \in \mathcal A(s)} \pi(a \mid s) \mathbb{E}\left[R_{t+1} \mid S_t=s, A_t=a\right] \\
& =\sum_{a \in \mathcal A(s)}\pi(a \mid s)\sum_{r \in \mathcal R(s,a)} p(r \mid s, a) r .
\end{aligned}</script><p>Given events $R_{t+1} = r, S_t = s, A_t = a$, the deduction is quite simple.</p>
<p>background knowledge1(<strong>Total Probability Theorem</strong>): From the definition of <em>conditional expectation</em>: given event $A$ and  a discrete random variable $X$,  the conditional expectation is</p>
<script type="math/tex; mode=display">
   \mathrm{E}(X \mid A)=\sum x P(X=x \mid A) .</script><p>A more verbose version of deduction is:</p>
<script type="math/tex; mode=display">
\mathbb{E}\left[R_{t+1} \mid S_t=s\right] \triangleq \sum_r p(r | s) r.</script><script type="math/tex; mode=display">
p(r | s) = \sum_{a \in \mathcal A(s)} \pi(a|s) . p(r | s, a)</script><h4 id="2-2-2-Second-term-the-mean-of-future-rewards"><a href="#2-2-2-Second-term-the-mean-of-future-rewards" class="headerlink" title="2.2.2 Second term: the mean of future rewards"></a>2.2.2 Second term: the mean of future rewards</h4><p>Second, calculate the second term $\mathbb{E}\left[G_{t+1} \mid S_t=s\right]$ , is the mean of future rewards.</p>
<script type="math/tex; mode=display">
\begin{aligned}
\mathbb{E}\left[G_{t+1} \mid S_t=s\right] & =\sum_{s^{\prime}\in \mathcal S} \mathbb{E}\left[G_{t+1} \mid S_t=s, S_{t+1}=s^{\prime}\right] p\left(s^{\prime} \mid s\right) \\
& =\sum_{s^{\prime}\in \mathcal S} \mathbb{E}\left[G_{t+1} \mid S_{t+1}=s^{\prime}\right] p\left(s^{\prime} \mid s\right) \text{(due to the memoryless Markov property.)}\\
& =\sum_{s^{\prime}\in \mathcal S} v_\pi\left(s^{\prime}\right) p\left(s^{\prime} \mid s\right) \\
& =\sum_{s^{\prime}\in \mathcal S} v_\pi\left(s^{\prime}\right) \sum_{a \in \mathcal A(s)} p\left(s^{\prime} \mid s, a\right) \pi(a \mid s)
\end{aligned}</script><h4 id="2-2-3-Formula-simplification"><a href="#2-2-3-Formula-simplification" class="headerlink" title="2.2.3 Formula simplification"></a>2.2.3 Formula simplification</h4><script type="math/tex; mode=display">
\begin{aligned}
v_\pi(s) & =\mathbb{E}\left[R_{t+1} \mid S_t=s\right]+\gamma \mathbb{E}\left[G_{t+1} \mid S_t=s\right], \\
& =\underbrace{\sum_{a \in \mathcal A(s)} \pi(a \mid s) \sum_{r \in \mathcal R(s,a)} p(r \mid s, a) r}_{\text {mean of immediate rewards }}+\underbrace{\gamma \sum_{s^{\prime}\in \mathcal S} \pi(a \mid s) \sum_{a \in \mathcal A(s)} p\left(s^{\prime} \mid s, a\right) v_\pi\left(s^{\prime}\right),}_{\text {mean of future rewards }}  \\
& =\sum_{a \in \mathcal A(s)} \pi(a \mid s)\left[\sum_{r \in \mathcal R(s,a)} p(r \mid s, a) r+\gamma \sum_{s^{\prime}\in \mathcal S} p\left(s^{\prime} \mid s, a\right) v_\pi\left(s^{\prime}\right)\right], \quad \forall s \in \mathcal{S} .
\end{aligned}</script><p>The above equation is called the <strong>Bellman equation</strong>, which characterizes the relationship among the state-values functions of different states.</p>
<ul>
<li>$\pi(a \mid s)$ is a given policy , $r$ is designed, $p(r \mid s, a)$, $p\left(s^{\prime} \mid s, a\right)$ represent the system model,which can be obtained. </li>
<li>Only $v_{\pi} (s)$ and $v_{\pi} (s^{\prime})$ are unknown state values to be calculated. It may be confusing to beginners how to calculate the unknown $v_{\pi} (s)$ given that it relies on another unknown $v_{\pi} (s^{\prime})$. It must be noted that the Bellman equation refers to a set of linear equations for all states rather than a single equation. (<strong>Bootstrapping</strong>)</li>
</ul>
<h3 id="2-3-Examples"><a href="#2-3-Examples" class="headerlink" title="2.3 Examples"></a>2.3 Examples</h3><h4 id="2-3-1-For-deterministic-policy"><a href="#2-3-1-For-deterministic-policy" class="headerlink" title="2.3.1 For deterministic policy"></a>2.3.1 For deterministic policy</h4><p></p><div align="center"><img src="/2024/08/06/Mathmatical-Foundation-of-Reinforcement-Learning-Concept-Notes/2024-04-12-21-11-39.png" width="50%" alt="Figure 2.4: An example for demonstrating the Bellman equation. The policy in this example is deterministic."></div><br>Consider the first example shown in Figure 2.4, where the policy is deterministic. We next write out the Bellman equation and then solve the state values from it.<p></p>
<script type="math/tex; mode=display">
\begin{aligned}
& v_\pi\left(s_1\right)=0+\gamma v_\pi\left(s_3\right), \\
& v_\pi\left(s_2\right)=1+\gamma v_\pi\left(s_4\right), \\
& v_\pi\left(s_3\right)=1+\gamma v_\pi\left(s_4\right), \\
& v_\pi\left(s_4\right)=1+\gamma v_\pi\left(s_4\right) .
\end{aligned}</script><p>We can solve the state values from these equations. Since the equations are simple, we can manually solve them. More complicated equations can be solved by the iterative algorithm.</p>
<script type="math/tex; mode=display">
\begin{aligned}
& v_\pi\left(s_4\right)=\frac{1}{1-\gamma}, \\
& v_\pi\left(s_3\right)=\frac{1}{1-\gamma}, \\
& v_\pi\left(s_2\right)=\frac{1}{1-\gamma}, \\
& v_\pi\left(s_1\right)=\frac{\gamma}{1-\gamma} .
\end{aligned}</script><p>Furthermore, if we set $\gamma=0.9$, then</p>
<script type="math/tex; mode=display">
\begin{aligned}
& v_\pi\left(s_4\right)=\frac{1}{1-0.9}=10, \\
& v_\pi\left(s_3\right)=\frac{1}{1-0.9}=10, \\
& v_\pi\left(s_2\right)=\frac{1}{1-0.9}=10, \\
& v_\pi\left(s_1\right)=\frac{0.9}{1-0.9}=9 .
\end{aligned}</script><h4 id="2-3-2-For-stochastic-policy"><a href="#2-3-2-For-stochastic-policy" class="headerlink" title="2.3.2 For stochastic policy"></a>2.3.2 For stochastic policy</h4><p></p><div align="center"><img src="/2024/08/06/Mathmatical-Foundation-of-Reinforcement-Learning-Concept-Notes/2024-04-12-21-14-38.png" width="50%" alt="Figure 2.5: An example for demonstrating the Bellman equation. The policy in this example is stochastic."></div><br>Consider the second example shown in Figure 2.5, where the policy is stochastic. The state transition probability is deterministic, and the reward probability is also deterministic. I,e, the system model is deterministic.<p></p>
<script type="math/tex; mode=display">
v_\pi\left(s_1\right)=0.5\left[0+\gamma v_\pi\left(s_3\right)\right]+0.5\left[-1+\gamma v_\pi\left(s_2\right)\right]</script><p>Similarly, it can be obtained that</p>
<script type="math/tex; mode=display">
\begin{aligned}
& v_\pi\left(s_2\right)=1+\gamma v_\pi\left(s_4\right), \\
& v_\pi\left(s_3\right)=1+\gamma v_\pi\left(s_4\right), \\
& v_\pi\left(s_4\right)=1+\gamma v_\pi\left(s_4\right) .
\end{aligned}</script><p>simple, we can solve the state values manually and obtain</p>
<script type="math/tex; mode=display">
\begin{aligned}
v_\pi\left(s_4\right) & =\frac{1}{1-\gamma}, \\
v_\pi\left(s_3\right) & =\frac{1}{1-\gamma}, \\
v_\pi\left(s_2\right) & =\frac{1}{1-\gamma}, \\
v_\pi\left(s_1\right) & =0.5\left[0+\gamma v_\pi\left(s_3\right)\right]+0.5\left[-1+\gamma v_\pi\left(s_2\right)\right], \\
& =-0.5+\frac{\gamma}{1-\gamma} .
\end{aligned}</script><p>Furthermore, if we set $\gamma=0.9$, then</p>
<script type="math/tex; mode=display">
\begin{aligned}
& v_\pi\left(s_4\right)=10, \\
& v_\pi\left(s_3\right)=10, \\
& v_\pi\left(s_2\right)=10, \\
& v_\pi\left(s_1\right)=-0.5+9=8.5 .
\end{aligned}</script><p>If we compare the state values of the two policies in the above examples, it can be seen that</p>
<script type="math/tex; mode=display">
v_{\pi_1}\left(s_i\right) \geq v_{\pi_2}\left(s_i\right), i= 1,2,3,4</script><p>which indicates that the policy in Figure 2.4 is better because it has greater state values.<br>This mathematical conclusion is consistent with the intuition that the rst policy is better because it can avoid entering the forbidden area when the agent starts from s1. As a result, the above two examples demonstrate that state values can be used to evaluate policies.</p>
<h3 id="2-4-Bellman-equation-the-matrix-vector-form"><a href="#2-4-Bellman-equation-the-matrix-vector-form" class="headerlink" title="2.4 Bellman equation: the matrix-vector form"></a>2.4 Bellman equation: the matrix-vector form</h3><p>Consider the Bellman equation:</p>
<script type="math/tex; mode=display">
\begin{equation} 
v_\pi(s)=\sum_a \pi(a \mid s)\left[\sum_r p(r \mid s, a) r+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right) v_\pi\left(s^{\prime}\right)\right]
\end{equation}</script><p>It’s an elementwise form. If we put all the equations together, we have a set of linear equations, which can be concisely written in a matrix-vector form.</p>
<p>Rewrite as</p>
<script type="math/tex; mode=display">
v_\pi(s)=r_\pi(s)+\gamma \sum_{s^{\prime}} p_\pi\left(s^{\prime} \mid s\right) v_\pi\left(s^{\prime}\right)</script><p>where</p>
<script type="math/tex; mode=display">
\begin{aligned}
& r_\pi(s) \triangleq \sum_a \pi(a \mid s) \sum_r p(r \mid s, a) r, \\
& p_\pi\left(s^{\prime} \mid s\right) \triangleq \sum_a \pi(a \mid s) p\left(s^{\prime} \mid s, a\right) .
\end{aligned}</script><p>Suppose the states could be indexed as $s_i(i=1, \ldots, n)$. For state $s_i$, the Bellman equation is</p>
<script type="math/tex; mode=display">
v_\pi\left(s_i\right)=r_\pi\left(s_i\right)+\gamma \sum_{s_j} p_\pi\left(s_j \mid s_i\right) v_\pi\left(s_j\right)</script><p>Put all these equations for all the states together and rewrite to a matrix-vector form</p>
<script type="math/tex; mode=display">
\begin{equation} 
v_\pi=r_\pi+\gamma P_\pi v_\pi
\end{equation}</script><p>where</p>
<p>$v_\pi=\left[v_\pi\left(s_1\right), \ldots, v_\pi\left(s_n\right)\right]^T \in \mathbb{R}^n$,</p>
<p>$r_\pi=\left[r_\pi\left(s_1\right), \ldots, r_\pi\left(s_n\right)\right]^T \in \mathbb{R}^n$,</p>
<p>$P_\pi \in \mathbb{R}^{n \times n}$, where $\left[P_\pi\right]_{i j}=p_\pi\left(s_j \mid s_i\right)$, is the state transition matrix.</p>
<p><strong>Examples: </strong>If there are four states, $v_\pi=r_\pi+\gamma P_\pi v_\pi$ can be written out as</p>
<script type="math/tex; mode=display">
\underbrace{\left[\begin{array}{l}
v_\pi\left(s_1\right) \\
v_\pi\left(s_2\right) \\
v_\pi\left(s_3\right) \\
v_\pi\left(s_4\right)
\end{array}\right]}_{v_\pi}=\underbrace{\left[\begin{array}{l}
r_\pi\left(s_1\right) \\
r_\pi\left(s_2\right) \\
r_\pi\left(s_3\right) \\
r_\pi\left(s_4\right)
\end{array}\right]}_{r_\pi}+\gamma \underbrace{\left[\begin{array}{llll}
p_\pi\left(s_1 \mid s_1\right) & p_\pi\left(s_2 \mid s_1\right) & p_\pi\left(s_3 \mid s_1\right) & p_\pi\left(s_4 \mid s_1\right) \\
p_\pi\left(s_1 \mid s_2\right) & p_\pi\left(s_2 \mid s_2\right) & p_\pi\left(s_3 \mid s_2\right) & p_\pi\left(s_4 \mid s_2\right) \\
p_\pi\left(s_1 \mid s_3\right) & p_\pi\left(s_2 \mid s_3\right) & p_\pi\left(s_3 \mid s_3\right) & p_\pi\left(s_4 \mid s_3\right) \\
p_\pi\left(s_1 \mid s_4\right) & p_\pi\left(s_2 \mid s_4\right) & p_\pi\left(s_3 \mid s_4\right) & p_\pi\left(s_4 \mid s_4\right)
\end{array}\right]}_{P_\pi} \underbrace{\left[\begin{array}{l}
v_\pi\left(s_1\right) \\
v_\pi\left(s_2\right) \\
v_\pi\left(s_3\right) \\
v_\pi\left(s_4\right)
\end{array}\right]}_{v_\pi} .</script><p>For the above deterministic policy</p>
<script type="math/tex; mode=display">
\left[\begin{array}{l}
v_\pi\left(s_1\right) \\
v_\pi\left(s_2\right) \\
v_\pi\left(s_3\right) \\
v_\pi\left(s_4\right)
\end{array}\right]=\left[\begin{array}{l}
0 \\
1 \\
1 \\
1
\end{array}\right]+\gamma\left[\begin{array}{llll}
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 0 & 1 \\
0 & 0 & 0 & 1
\end{array}\right]\left[\begin{array}{l}
v_\pi\left(s_1\right) \\
v_\pi\left(s_2\right) \\
v_\pi\left(s_3\right) \\
v_\pi\left(s_4\right)
\end{array}\right]</script><p>For the above stochastic policy:</p>
<script type="math/tex; mode=display">
\left[\begin{array}{c}
v_\pi\left(s_1\right) \\
v_\pi\left(s_2\right) \\
v_\pi\left(s_3\right) \\
v_\pi\left(s_4\right)
\end{array}\right]=\left[\begin{array}{c}
0.5(0)+0.5(-1) \\
1 \\
1 \\
1
\end{array}\right]+\gamma\left[\begin{array}{cccc}
0 & 0.5 & 0.5 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 0 & 1 \\
0 & 0 & 0 & 1
\end{array}\right]\left[\begin{array}{l}
v_\pi\left(s_1\right) \\
v_\pi\left(s_2\right) \\
v_\pi\left(s_3\right) \\
v_\pi\left(s_4\right)
\end{array}\right]</script><h3 id="2-5-Solving-state-values-from-the-Bellman-equation"><a href="#2-5-Solving-state-values-from-the-Bellman-equation" class="headerlink" title="2.5 Solving state values from the Bellman equation"></a>2.5 Solving state values from the Bellman equation</h3><p>Calculating the state values of a given policy is a fundamental problem in reinforcement learning. This problem is often referred to as policy evaluation. In this section, two methods are presented for calculating state values from the Bellman equation.</p>
<ul>
<li><p>The closed-form solution is:</p>
<script type="math/tex; mode=display">
v_\pi=\left(I-\gamma P_\pi\right)^{-1} r_\pi</script><p>The drawback of closed-form solution is that it involves a matrix inverse operation, which is computationally expensive. Thus, in practice, we’ll use an <strong>iterative algorithm</strong>.</p>
</li>
<li><p>An iterative solution is:</p>
<script type="math/tex; mode=display">
v_{k+1}=r_\pi+\gamma P_\pi v_k  \quad  k=0,1,2,\dots ,</script><p>where $I$ is the identity matrix. We can just <u>randomly select</u> a matrix $v_0$, then calculate $v_1, v_2, \cdots$. This leads to a sequence $\left\{v_0, v_1, v_2, \ldots\right\}$. We can show that</p>
<script type="math/tex; mode=display">
v_k \rightarrow v_\pi=\left(I-\gamma P_\pi\right)^{-1} r_\pi, \quad k \rightarrow \infty</script></li>
</ul>
<blockquote>
<p><strong>Proof: the iterative solution</strong>（Contraction mapping theorem）<br>Define the error as $\delta_k=v_k-v_\pi$. We only need to show $\delta_k \rightarrow 0$. Substituting: $v_{k+1}=\delta_{k+1}+v_\pi$,and $v_k=\delta_k+v_\pi$ into $v_{k+1}=r_\pi+\gamma P_\pi v_k$ gives</p>
<script type="math/tex; mode=display">
\delta_{k+1}+v_\pi=r_\pi+\gamma P_\pi\left(\delta_k+v_\pi\right)</script><p>which can be rewritten as</p>
<script type="math/tex; mode=display">
\delta_{k+1}=-v_\pi+r_\pi+\gamma P_\pi \delta_k+\gamma P_\pi v_\pi=\gamma P_\pi \delta_k</script><p>As a result,</p>
<script type="math/tex; mode=display">
\delta_{k+1}=\gamma P_\pi \delta_k=\gamma^2 P_\pi^2 \delta_{k-1}=\cdots=\gamma^{k+1} P_\pi^{k+1} \delta_0</script><p>Note that $0 \leq P_\pi^k \leq 1$, which means every entry of $P_\pi^k$ is no greater than 1 for any $k=0,1,2, \ldots$. That is because $P_\pi^k =\mathbf{1}$, where $\mathbf{1}=[1, \ldots, 1]^T$. On the other hand, since $\gamma&lt;1$, we know $\gamma^k \rightarrow 0$ and hence $\delta_{k+1} \rightarrow 0$ as $k \rightarrow \infty$.</p>
</blockquote>
<h3 id="2-6-Action-Value"><a href="#2-6-Action-Value" class="headerlink" title="2.6 Action Value"></a>2.6 Action Value</h3><p>From state value to action value:</p>
<ul>
<li>State value: the average return the agent can <u>get starting from a state</u>.</li>
<li>Action value: the average return the agent can <u>get starting from a state and taking an action</u>.</li>
</ul>
<p>Definition of action value (or action value function):</p>
<script type="math/tex; mode=display">
\begin{equation} 
q_\pi(s, a) \triangleq \mathbb{E}\left[G_t \mid S_t=s, A_t=a\right]
\end{equation}</script><p>Note:</p>
<ol>
<li>The $q_\pi(s, a)$ is a function of the state-action pair $(s, a)$.</li>
<li>The $q_\pi(s, a)$ depends on $\pi$.</li>
</ol>
<p><strong>Relation to the state value function</strong></p>
<p>It follows from the properties of <em>conditional expectation</em> that</p>
<script type="math/tex; mode=display">
\underbrace{\mathbb{E}\left[G_t \mid S_t=s\right]}_{v_\pi(s)}=\sum_a \underbrace{\mathbb{E}\left[G_t \mid S_t=s, A_t=a\right]}_{q_\pi(s, a)} \pi(a \mid s)</script><p>Hence,</p>
<script type="math/tex; mode=display">
\begin{equation} 
v_\pi(s)=\sum_a \pi(a \mid s) q_\pi(s, a)
\end{equation}</script><p>Recall the <a href="">Bellman equation</a>, that the state value is given by</p>
<script type="math/tex; mode=display">
v_\pi(s)=\sum_a \pi(a \mid s)[\underbrace{\sum_r p(r \mid s, a) r+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right) v_\pi\left(s^{\prime}\right)}_{q_\pi(s, a)}]</script><p>By comparing  and , we have the action-value function as</p>
<script type="math/tex; mode=display">
\begin{equation} 
q_\pi(s, a)=\sum_r p(r \mid s, a) r+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right) 
v_\pi\left(s^{\prime}\right)
\end{equation}</script><p>equation (5) and equation (6) are the two sides of the same coin:</p>
<ul>
<li>equation (5) shows how to obtain state values from action values.</li>
<li>equation (6) shows how to obtain action values from state values.</li>
</ul>
<p><strong>Example</strong></p>
<p></p><div align="center"><img src="/2024/08/06/Mathmatical-Foundation-of-Reinforcement-Learning-Concept-Notes/2024-04-12-22-00-41.png" width="50%" alt="Figure 2.8: An example for demonstrating the process of calculating action values."></div><br>Consider the stochastic policy shown in Figure 2.8. <p></p>
<script type="math/tex; mode=display">
q_\pi\left(s_1, a_2\right)=-1+\gamma v_\pi\left(s_2\right)</script><p>Note that even if an action would not be selected by a policy, it still has an action value. Therefor, for the other actions:</p>
<script type="math/tex; mode=display">
\begin{aligned}
& q_\pi\left(s_1, a_1\right)=-1+\gamma v_\pi\left(s_1\right), \\
& q_\pi\left(s_1, a_3\right)=0+\gamma v_\pi\left(s_3\right), \\
& q_\pi\left(s_1, a_4\right)=-1+\gamma v_\pi\left(s_1\right), \\
& q_\pi\left(s_1, a_5\right)=0+\gamma v_\pi\left(s_1\right) .
\end{aligned}</script><p><strong>NOTE:</strong><br>Although some actions cannot be possibly selected by a given policy, this does not mean that these actions are not good. It is possible that the given policy is not good, so it cannot select the best action. The purpose of reinforcement learning is to find optimal policies. To that end, we must keep exploring all actions to determine better actions for each state.</p>
<h3 id="2-7-Summary"><a href="#2-7-Summary" class="headerlink" title="2.7 Summary"></a>2.7 Summary</h3><p>The most important concept introduced in this chapter is the state value. Mathematically, a state value is the expected return that the agent can obtain by starting from a state. The values of different states are related to each other. That is, <strong>the value of state $s$ relies on the values of some other states</strong>, which may further rely on the value of state $s$ itself. This phenomenon might be the most confusing part of this chapter for beginners. It is related to an important concept called <strong>bootstrapping, which involves calculating something from itself.</strong> Although bootstrapping may be intuitively confusing, it is clear if we examine the matrix-vector form of the Bellman equation. In particular, the Bellman equation is a set of linear equations that describe the relationships between the values of all states.</p>
<p>Since state values can be used to evaluate whether a policy is good or not, <strong>the process of solving the state values of a policy from the Bellman equation is called policy evaluation.</strong> As we will see later in this book, policy evaluation is an important step in many reinforcement learning algorithms.</p>
<p>Another important concept, action value, was introduced to describe the value of taking one action at a state. As we will see later in this book, <strong>action values play a more direct role than state values when we attempt to find optimal policies.</strong> Finally, the Bellman equation is not restricted to the reinforcement learning field. Instead, it widely exists in many elds such as control theories and operation research. In different elds, the Bellman equation may have different expressions. In this book, the Bellman equation is studied under discrete Markov decision processes.</p>
<p><strong> How to calculate action value</strong></p>
<ul>
<li><p>We can first calculate all the state values and then calculate the action values.</p>
</li>
<li><p>We can also directly calculate the action values with or without models (discussed later).</p>
</li>
</ul>
<hr>
<h2 id="3-Bellman-Optimality-Equation-BOE"><a href="#3-Bellman-Optimality-Equation-BOE" class="headerlink" title="3. Bellman Optimality Equation(BOE)"></a>3. Bellman Optimality Equation(BOE)</h2><h3 id="3-1-Optimal-State-values-and-Policy"><a href="#3-1-Optimal-State-values-and-Policy" class="headerlink" title="3.1 Optimal State values and Policy"></a>3.1 Optimal State values and Policy</h3><p>The state value could be used to evaluate policy : if</p>
<script type="math/tex; mode=display">
v_{\pi_1}(s) \geq v_{\pi_2}(s) \quad \text { for all } s \in \mathcal{S}</script><p>then $\pi_1$ is said to be <strong>better</strong> than $\pi_2$.</p>
<p><strong>Definition 1</strong>(Optimal policy and optimal state value). A policy $\pi^\ast$<br>is optimal if $v_{\pi^\ast}(s) \geq v_\pi(s)$ for all $s\in S$ and for any other policy $\pi$. The state values $\pi^\ast$ of are the optimal state values.</p>
<p>The definition leads to many questions:</p>
<ul>
<li>Does the optimal policy <strong>exist</strong>?</li>
<li>Is the optimal policy <strong>unique</strong>?</li>
<li>Is the optimal policy <strong>stochastic or deterministic</strong>?</li>
<li><strong>How to obtain</strong> the optimal policy?</li>
</ul>
<p>These fundamental questions must be clearly answered to thoroughly understand optimal policies. For example, regarding the existence of optimal policies, if optimal policies do not exist, then we do not need to bother to design algorithms to find them. To answer these questions, we study the <em>Bellman optimality equation</em>.</p>
<h3 id="3-2-Bellman-optimality-equation-elementwise-form"><a href="#3-2-Bellman-optimality-equation-elementwise-form" class="headerlink" title="3.2 Bellman optimality equation  (elementwise form)"></a>3.2 Bellman optimality equation  (elementwise form)</h3><p><strong>Bellman optimality equation (BOE) is a tool for analyzing optimal policies and optimal state values.</strong> By solving this equation, we can obtain optimal policies and optimal state values.</p>
<p>For every $s \in \mathcal{S}$, the elementwise expression of the BOE is</p>
<script type="math/tex; mode=display">
\begin{aligned}
v(s) & =\max _{\pi(s) \in \Pi(s)} \sum_{a \in \mathcal{A}} \pi(a \mid s)\left(\sum_{r \in \mathcal{R}} p(r \mid s, a) r+\gamma \sum_{s^{\prime} \in \mathcal{S}} p\left(s^{\prime} \mid s, a\right) v\left(s^{\prime}\right)\right) \\
& =\max _{\pi(s) \in \Pi(s)} \sum_{a \in \mathcal{A}} \pi(a \mid s) q(s, a),
\end{aligned}</script><p>where $v(s), v\left(s^{\prime}\right)$ are unknown variables to be solved and</p>
<script type="math/tex; mode=display">
q(s, a) \doteq \sum_{r \in \mathcal{R}} p(r \mid s, a) r+\gamma \sum_{s^{\prime} \in \mathcal{S}} p\left(s^{\prime} \mid s, a\right) v\left(s^{\prime}\right)</script><p>Here, $\pi(s)$ denotes a policy for state $s$, and $\Pi(s)$ is the set of all possible policies for $s$.</p>
<p>Notes:</p>
<p>$p(r \mid s, a), p\left(s^{\prime} \mid s, a\right)$ are known and $r$,$\gamma$ are designed. this equation has two unknown variables $v(s)$ and $\pi(a \mid s)$. It may be confusing to beginners how to solve two unknown variables from one equation. The reason is this equation satisfy a special quality.(contraction mapping theorem)</p>
<h4 id="3-2-1-Maximize-on-the-right-hand-side-of-BOE"><a href="#3-2-1-Maximize-on-the-right-hand-side-of-BOE" class="headerlink" title="3.2.1 Maximize on the right-hand side of BOE"></a>3.2.1 Maximize on the right-hand side of BOE</h4><p>In practice we need to deal with the matrix-vector form since that is what we’re faced with. But since each row in the matrix is actually a vector of the elementwise form, we start with the element form.</p>
<p>In fact, we can turn the problem into “solve the optimal $\pi$ on the right-hand side, next to get the optimal state values “. Let’s look at one example first:</p>
<hr>
<p><strong>Example</strong> 1. Consider two unknown variables $x, y \in \mathbb{R}$ that satisfy</p>
<script type="math/tex; mode=display">
x=\max _{y \in \mathbb{R}}\left(2 x-1-y^2\right) .</script><p>The first step is to solve $y$ on the right-hand side of the equation. Regardless of the value of $x$, we always have $\max _y\left(2 x-1-y^2\right)=2 x-1$, where the maximum is achieved when $y=0$. The second step is to solve $x$. When $y=0$, the equation becomes $x=2 x-1$, which leads to $x=1$. Therefore, $y=0$ and $x=1$ are the solutions of the equation.</p>
<p><strong>Example</strong> 2. Given $q_1, q_2, q_3 \in \mathbb{R}$, we would like to find the optimal values of $c_1, c_2, c_3$ to maximize</p>
<script type="math/tex; mode=display">
\sum_{i=1}^3 c_i q_i=c_1 q_1+c_2 q_2+c_3 q_3,</script><p>where $c_1+c_2+c_3=1$ and $c_1, c_2, c_3 \geq 0$.<br>Without loss of generality, suppose that $q_3 \geq q_1, q_2$. Then, the optimal solution is $c_3^\ast=1$ and $c_1^\ast=c_2^\ast=0$. This is because</p>
<script type="math/tex; mode=display">
q_3=\left(c_1+c_2+c_3\right) q_3=c_1 q_3+c_2 q_3+c_3 q_3 \geq c_1 q_1+c_2 q_2+c_3 q_3</script><p>for any $c_1, c_2, c_3$.</p>
<hr>
<p>Inspired by the above example, since $\sum_a(\pi(a \mid s))= 1$, the (elementwise) BOE can be written as</p>
<script type="math/tex; mode=display">
v(s)=\max _{\pi(s) \in \Pi(s)} \sum_{a \in \mathcal{A}} \pi(a \mid s) q(s, a)\quad =\max _{a \in \mathcal{A}(s)} q(s, a) , s \in \mathcal{S}.</script><p>where the optimality is achieved when</p>
<script type="math/tex; mode=display">
\pi(a \mid s)= \begin{cases}1 & a=a^* \\ 0 & a \neq a^*\end{cases}</script><p>where $a^*=\arg \max _a q(s, a)$(arg is get the variant value).</p>
<p><strong>In summary, the optimal policy $\pi(s)$ is the one that selects the action that has the greatest value of $q(s,a)$.</strong></p>
<p>Now that we know the solution of BOE is to maximize the right-hand side, - just select the greatest action value $q(s,a)$. But <strong>we don’t know action value or state value at this time</strong>, next , we need study how to obtain state value from the <em>contraction mapping theorem</em>  on the matrix-vector form. </p>
<h4 id="3-2-2-Matrix-vector-form-of-the-BOE"><a href="#3-2-2-Matrix-vector-form-of-the-BOE" class="headerlink" title="3.2.2 Matrix-vector form of the BOE"></a>3.2.2 Matrix-vector form of the BOE</h4><p>To leverage the <em>contraction mapping theorem</em>, we’ll express the matrix-vector form as </p>
<script type="math/tex; mode=display">
\begin{equation} 
v =\max _{\pi \in \mathcal{\pi}}\left(r_\pi+\gamma P_\pi v\right) .
\end{equation}</script><p>where $v \in \mathbb{R}^{|\mathcal{S}|}$ and $\max _\pi$ is <strong>performed in an elementwise manner</strong>. The structures of $r_\pi$ and $P_\pi$ are the same as those in the matrix-vector form of the normal Bellman equation:</p>
<script type="math/tex; mode=display">
\left[r_\pi\right]_s \doteq \sum_{a \in \mathcal{A}} \pi(a \mid s) \sum_{r \in \mathcal{R}} p(r \mid s, a) r, \quad\left[P_\pi\right]_{s, s^{\prime}}=p\left(s^{\prime} \mid s\right) \doteq \sum_{a \in \mathcal{A}} \pi(a \mid s) p\left(s^{\prime} \mid s, a\right) .</script><p>Since the optimal value of $\pi$ is determined by $v$, the right-hand side of BOE (matrix-vector form) is a function of $v$, denoted as</p>
<script type="math/tex; mode=display">
\begin{equation} 
f(v) \triangleq \max _{\pi}\left(r_\pi+\gamma P_\pi v\right) .
\end{equation}</script><p>Then, the BOE can be expressed in a concise form as</p>
<script type="math/tex; mode=display">
v=f(v)</script><p>Every row $[f(v)]_s$ is the elementwise form of $s$.</p>
<h4 id="3-2-3-Contraction-mapping-theorem"><a href="#3-2-3-Contraction-mapping-theorem" class="headerlink" title="3.2.3 Contraction mapping theorem"></a>3.2.3 Contraction mapping theorem</h4><p>Now that the matrix-vector form is expressed as a nonlinear equation $v = f(v)$, we next introduce the <em>contraction mapping theorem</em> to analyze it. The contraction mapping theorem is a powerful tool for analyzing general nonlinear equations. </p>
<p> Concepts: Fixed point and Contraction mapping</p>
<p><strong>Fixed point</strong>: $x^\ast \in X$ is a fixed point of $f: X \rightarrow X$ if</p>
<script type="math/tex; mode=display">
f(x^*)=x^*</script><p><strong>Contraction mapping</strong> (or contractive function): $f$ is a contraction mapping if</p>
<script type="math/tex; mode=display">
\left\|f\left(x_1\right)-f\left(x_2\right)\right\| \leq \gamma\left\|x_1-x_2\right\|</script><p>where $\gamma \in(0,1)$. $|\cdot|$ denotes a vector or matrix norm.</p>
<p><strong>Example1</strong><br>Given $x=f(x)=A x$, where $x \in \mathbb{R}^n, A \in \mathbb{R}^{n \times n}$ and $|A| \leq \gamma&lt;1$.<br>It is easy to verify that $x=0$ is a fixed point since $0=A 0$. To see the contraction property,</p>
<script type="math/tex; mode=display">
\left\|A x_1-A x_2\right\|=\left\|A\left(x_1-x_2\right)\right\| \leq\|A\|\left\|x_1-x_2\right\| \leq \gamma\left\|x_1-x_2\right\| .</script><p>Therefore, $f(x)=A x$ is a contraction mapping.</p>
<p><strong>Theorem: Contraction Mapping Theorem</strong><br>For any equation that has the form of $x=f(x)$,where $x$ and $f(x)$ are real vectors if $f$ is a contraction mapping, then the following peoperties hold.</p>
<ul>
<li>Existence: there exists a fixed point $x^\ast$ satisfying $f\left(x^\ast\right)=x^\ast$.</li>
<li>Uniqueness: The fixed point $x^*$ is unique.</li>
<li>Algorithm: Consider a sequence $\left\{x_k\right\}$ where $x_{k+1}=f\left(x_k\right)$, then $x_k \rightarrow x^*$( fixed point) as $k \rightarrow \infty$ for any initial guess $x_0$. Moreover, the convergence rate is exponentially fast.</li>
</ul>
<p><a href="">-&gt; See the proof</a></p>
<h3 id="3-3-Contraction-property-of-the-BOE"><a href="#3-3-Contraction-property-of-the-BOE" class="headerlink" title="3.3 Contraction property of the BOE"></a>3.3 Contraction property of the BOE</h3><p><strong>Theorem (Contraction Property)</strong>:</p>
<p>$f(v)$  is a contraction mapping satisfying</p>
<script type="math/tex; mode=display">
\left\|f\left(v_1\right)-f\left(v_2\right)\right\|_{\infty} \leq \gamma\left\|v_1-v_2\right\|_{\infty}</script><p>where $\gamma \in(0,1)$ is the discount rate, and $|\cdot|_{\infty}$ is the maximum norm, which is the maximum absolute value of the elements of a vector.</p>
<p><a href="证明链接放置">-&gt; See the proof</a></p>
<h3 id="3-4-Solution-of-the-BOE"><a href="#3-4-Solution-of-the-BOE" class="headerlink" title="3.4 Solution of the BOE"></a>3.4 Solution of the BOE</h3><p>Due to the contraction property of BOE, the matrix-vector form can be solved by computing following equation iteratively</p>
<script type="math/tex; mode=display">
\begin{equation} 
v_{k+1}=f\left(v_k\right)=\max _\pi\left(r_\pi+\gamma P_\pi v_k\right) .
\end{equation}</script><p>At every iteration, for each state, <strong>what we face is actually the elementwise form</strong>:</p>
<script type="math/tex; mode=display">
\begin{aligned}
v_{k+1}(s) & =\max _\pi \sum_a \pi(a \mid s)\left(\sum_r p(r \mid s, a) r+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right) v_k\left(s^{\prime}\right)\right) \\
& =\max _\pi \sum_a \pi(a \mid s) q_k(s, a) \\
& =\max _a q_k(s, a) .
\end{aligned}</script><blockquote>
<p><strong>Procedure summary</strong> (value iteration algorithm):<br>For every $s$, estimate(randomly select) current state value as $v_k(s)$<br>For any $a \in \mathcal{A}(s)$, calculate</p>
<script type="math/tex; mode=display">
q_k(s, a)=\sum_r p(r \mid s, a) r+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right) v_k\left(s^{\prime}\right)</script><p>Calculate the greedy policy $\pi_{k+1}$ for every $s$ as</p>
<script type="math/tex; mode=display">
\pi_{k+1}(a \mid s)=\left\{\begin{array}{cc}
1 & a=a_k^*(s) \\
0 & a \neq a_k^*(s)
\end{array}\right.</script><p>where $a_k^*(s)=\arg \max _a q_k(s, a)$.<br>Calculate $v_{k+1}(s)=\max _a q_k(s, a)$</p>
</blockquote>
<h3 id="3-5-BOE-Optimality"><a href="#3-5-BOE-Optimality" class="headerlink" title="3.5 BOE: Optimality"></a>3.5 BOE: Optimality</h3><p>Suppose $v^*$ is the solution to the Bellman optimality equation. It satisfies</p>
<script type="math/tex; mode=display">
v^*=\max _\pi\left(r_\pi+\gamma P_\pi v^*\right)</script><p>Suppose</p>
<script type="math/tex; mode=display">
\pi^*=\arg \max _\pi\left(r_\pi+\gamma P_\pi v^*\right)</script><p>Then</p>
<script type="math/tex; mode=display">
v^*=r_{\pi^*}+\gamma P_{\pi^*} v^*</script><p><strong>Therefore, $ \pi^\ast $ is a policy and $ v^\ast=v_{\pi^\ast} $,and the BOE is a special Bellman equation whose corresponding policy is $ \pi^\ast $.</strong></p>
<h3 id="3-6-What-does-pi-ast-look-like"><a href="#3-6-What-does-pi-ast-look-like" class="headerlink" title="3.6 What does $\pi^\ast$ look like?"></a>3.6 What does $\pi^\ast$ look like?</h3><p>For any $s \in \mathcal{S}$, the deterministic <strong>greedy</strong> policy</p>
<script type="math/tex; mode=display">
\pi^*(a \mid s)= \begin{cases}1 & a=a^*(s) \\ 0 & a \neq a^*(s)\end{cases}</script><p>is an <strong>optimal policy</strong> solving the BOE. Here,</p>
<script type="math/tex; mode=display">
a^*(s)=\arg \max _a q^*(a, s)</script><p>where $ q^\ast(s, a):=\sum_r p(r \mid s, a) r+\gamma \sum_{s^{\prime}} p\left(s^{\prime} \mid s, a\right) v^\ast\left(s^{\prime}\right) $.</p>
<p>Proof: Due to</p>
<script type="math/tex; mode=display">
\pi^*(s)=\arg \max _{\pi \in \Pi} \sum_{a \in \mathcal{A}} \pi(a \mid s) \underbrace{\left(\sum_{r \in \mathcal{R}} p(r \mid s, a) r+\gamma \sum_{s^{\prime} \in \mathcal{S}} p\left(s^{\prime} \mid s, a\right) v^*\left(s^{\prime}\right)\right)}_{q^*(s, a)}, \quad s \in \mathcal{S} .</script><p>It is clear that $ \sum_{a \in \mathcal{A}} \pi(a \mid s) q^\ast(s, a) $ is maximized if $ \pi(s) $ selects the action with the greatest $ q^\ast(s, a) $.</p>
<p>Uniqueness of optimal policies: Although the value of $ v^\ast $ is unique, the optimal policy that corresponds to $ v^\ast $ may not be unique. This can be easily verified by counterexamples. For example, the two policies shown in below Figure are both optimal.<br>Stochasticity of optimal policies: An optimal policy can be either stochastic or deterministic, as demonstrated in below Figure. However, it is certain that there always exists a deterministic optimal policy.</p>
<p><img src="/2024/08/06/Mathmatical-Foundation-of-Reinforcement-Learning-Concept-Notes/2024-04-18-22-25-49.png" alt=""></p>
<h3 id="3-7-Factors-that-influence-optimal-policies"><a href="#3-7-Factors-that-influence-optimal-policies" class="headerlink" title="3.7 Factors that influence optimal policies"></a>3.7 Factors that influence optimal policies</h3><script type="math/tex; mode=display">
\pi^*(s)=\arg \max _{\pi \in \Pi(s)} \sum_{a \in \mathcal{A}} \pi(a \mid s) \left(\sum_{r \in \mathcal{R}} p(r \mid s, a) r+\gamma \sum_{s^{\prime} \in \mathcal{S}} p\left(s^{\prime} \mid s, a\right) v^*\left(s^{\prime}\right)\right), \quad s \in \mathcal{S} .</script><p>The optimal state value and optimal policy are determined by the following parameters: 1) the immediate reward $r$, 2) the discount rate $\gamma$, and 3) the system model<br> $p(s^{\prime} \mid s,a), p(r \mid s,a)$. While the system model is fixed, we next discuss how the optimal policy varies when we change the values of $r$ and $\gamma$.  </p>
<p><strong>Impact of the discount rate and the reward values</strong></p>
<ul>
<li><p>$\gamma$ from 0 to 1 , each state from extremely short-sighted to   far-sighted </p>
</li>
<li><p>if a state must travel along a longer trajectory to reach the target, its state value is smaller due to the discount rate.Therefore, although the immediate reward of every step does not encourage the agent to approach the target as quickly as possible, the discount rate does encourage it to do so.</p>
</li>
<li><p>If we want to strictly prohibit the agent from entering any forbidden area, we can increase the punishment received for doing so.</p>
</li>
</ul>
<p>Theorem (Optimal policy invariance)(Linear variation):</p>
<p>Consider a Markov decision process with $v^\ast \in \mathbb{R}^{|\mathcal{S}|}$ as the optimal state value satisfying $v^\ast=\max _{\pi \in \Pi}\left(r_\pi+\gamma P_\pi v^\ast\right)$. If every reward $r \in \mathcal{R}$ is changed by an affine transformation to $\alpha r+\beta$, where $\alpha, \beta \in \mathbb{R}$ and $\alpha&gt;0$, then the corresponding optimal state value $v^{\prime}$ is also an affine transformation of $v^\ast$ :</p>
<script type="math/tex; mode=display">
v^{\prime}=\alpha v^*+\frac{\beta}{1-\gamma} \mathbf{1}</script><p>where $\gamma \in(0,1)$ is the discount rate and $\mathbf{1}=[1, \ldots, 1]^T$.</p>
<p>Consequently, the optimal policy derived from $v^{\prime}$ is <strong>invariant</strong> to the affine transformation of the reward values.</p>
<h3 id="3-8-Summary"><a href="#3-8-Summary" class="headerlink" title="3.8  Summary"></a>3.8  Summary</h3><p>The core concepts in this chapter include <strong>optimal policies and optimal state values</strong>. In particular, a policy is optimal if its state values are greater than or equal to those of any other policy. The state values of an optimal policy are the optimal state values. The BOE is the core tool for analyzing optimal policies and optimal state values. This equation is a nonlinear equation <strong>with a nice contraction property</strong>. We can apply the contraction mapping theorem to analyze this equation. It was shown that the solutions of the BOE correspond to the optimal state value and optimal policy. This is the reason why we need to study the BOE.</p>
<p>Q: What is the definition of optimal policies?<br>A: A policy is optimal if its corresponding state values are greater than or equal to any other policy. It should be noted that this specific definition of optimality is valid only for tabular reinforcement learning algorithms. <strong>When the values or policies are approximated by functions, different metrics must be used to define optimal policies.</strong> This will become clearer in Chapters 8 and 9.</p>
<p>Q: Are optimal policies stochastic or deterministic?<br>A: An optimal policy can be either deterministic or stochastic. A nice fact is that there always exist deterministic greedy optimal policies.</p>
<p>Q: If we increase all the rewards by the same amount, will the optimal state value change? Will the optimal policy change?<br>A: Increasing all the rewards by the same amount is an affine transformation of the rewards, which would not affect the optimal policies. However, the optimal state value would increase, as shown in (Optimal policy invariance).</p>

    </div>

    
    
    

<div> 
<div class="my_post_copyright">
  <script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

  <!-- JS库 sweetalert 可修改路径 -->
  <script type="text/javascript" src="http://jslibs.wuxubj.cn/sweetalert_mini/jquery-1.7.1.min.js"></script>
  <script src="http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.min.js"></script>
  <link rel="stylesheet" type="text/css" href="http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.mini.css">

  <p><span>Article:</span>Mathematical Foundation of Reinforcement Learning-Concept Notes</a></p>
  <p><span>Author:</span>Ming Huang</a></p>
  <p><span>Release time：</span>2024-08-06   11:30:06</p>
  <p><span>Updat time:</span>2024-10-23   11:55:12</p>
  <p><span>Original link:</span><a href="/2024/08/06/Mathmatical-Foundation-of-Reinforcement-Learning-Concept-Notes/" title="Mathematical Foundation of Reinforcement Learning-Concept Notes">https://www.huangm.cn/2024/08/06/Mathmatical-Foundation-of-Reinforcement-Learning-Concept-Notes/</a>
    <span class="copy-path"  title="Click to copy the article link"><i class="fa fa-clipboard" data-clipboard-text="https://www.huangm.cn/2024/08/06/Mathmatical-Foundation-of-Reinforcement-Learning-Concept-Notes/"  aria-label="Copy successful!"></i></span>
  </p>
  <p><span>license agreement:</span><i class="fa fa-creative-commons" /></i> <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)"> (CC BY-NC-ND 4.0)</a> Please keep the original link and author when reprinting.</p>
</div>
<script>
    var clipboard = new Clipboard('.fa-clipboard');
    clipboard.on('success', $(function(){
      $(".fa-clipboard").click(function(){
        swal({
          title: "",
          text: 'Copy successful',
          html: false,
          timer: 500,
          showConfirmButton: false
        });
      });
    }));
</script>
 </div>

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/07/13/Python-Basic-Notes/" rel="prev" title="Python Basic Notes">
      <i class="fa fa-chevron-left"></i> Python Basic Notes
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/08/08/Mathmatical-Foundation-of-Reinforcement-Learning-Algorithms-model-based-non-incremental/" rel="next" title="Mathematical Foundation of Reinforcement Learning - Algorithms (model-based & non-incremental)">
      Mathematical Foundation of Reinforcement Learning - Algorithms (model-based & non-incremental) <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Reference-Sources"><span class="nav-text">Reference Sources:</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Basic-Concepts"><span class="nav-text">1. Basic Concepts</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-A-grid-world-example-Consistent-example"><span class="nav-text">1.1 A grid world example (Consistent example)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-State-and-action-s-n-a-m"><span class="nav-text">1.2 State and action ( $ s_n, a_m $ )</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-3-States-and-Observations"><span class="nav-text">1.3 States and Observations</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-4-State-transition-p-s-n-mid-s-1-a-2-x"><span class="nav-text">1.4 State transition ( $p(s_{n} \mid s_1, a_{2})&#x3D;x$ );</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-5-Policy-pi-a-mid-s"><span class="nav-text">1.5 Policy ($\pi(a \mid s)$)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-6-Rewards-r-s-a-p-r-mid-s-a"><span class="nav-text">1.6 Rewards ($r(s, a)$, $p(r \mid s,a)$)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-7-Trajectories-and-Return"><span class="nav-text">1.7 Trajectories and Return</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-8-Episodes"><span class="nav-text">1.8 Episodes</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-9-Markov-decision-processes"><span class="nav-text">1.9 Markov decision processes</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-10-Markov-processes"><span class="nav-text">1.10 Markov processes</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-11-What-is-reinforcement-learning"><span class="nav-text">1.11 What is reinforcement learning</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-State-Values-Bellman-Equation-and-Action-Values"><span class="nav-text">2. State Values, Bellman Equation and Action Values</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-State-Values"><span class="nav-text">2.1 State Values</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-Bellman-Equation"><span class="nav-text">2.2 Bellman Equation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-1-First-term-the-mean-of-immediate-rewards"><span class="nav-text">2.2.1 First term: the mean of immediate rewards</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-2-Second-term-the-mean-of-future-rewards"><span class="nav-text">2.2.2 Second term: the mean of future rewards</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-2-3-Formula-simplification"><span class="nav-text">2.2.3 Formula simplification</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-Examples"><span class="nav-text">2.3 Examples</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-1-For-deterministic-policy"><span class="nav-text">2.3.1 For deterministic policy</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-3-2-For-stochastic-policy"><span class="nav-text">2.3.2 For stochastic policy</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-Bellman-equation-the-matrix-vector-form"><span class="nav-text">2.4 Bellman equation: the matrix-vector form</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-5-Solving-state-values-from-the-Bellman-equation"><span class="nav-text">2.5 Solving state values from the Bellman equation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-6-Action-Value"><span class="nav-text">2.6 Action Value</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-7-Summary"><span class="nav-text">2.7 Summary</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-Bellman-Optimality-Equation-BOE"><span class="nav-text">3. Bellman Optimality Equation(BOE)</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#3-1-Optimal-State-values-and-Policy"><span class="nav-text">3.1 Optimal State values and Policy</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-2-Bellman-optimality-equation-elementwise-form"><span class="nav-text">3.2 Bellman optimality equation  (elementwise form)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-1-Maximize-on-the-right-hand-side-of-BOE"><span class="nav-text">3.2.1 Maximize on the right-hand side of BOE</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-2-Matrix-vector-form-of-the-BOE"><span class="nav-text">3.2.2 Matrix-vector form of the BOE</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-2-3-Contraction-mapping-theorem"><span class="nav-text">3.2.3 Contraction mapping theorem</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-3-Contraction-property-of-the-BOE"><span class="nav-text">3.3 Contraction property of the BOE</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-4-Solution-of-the-BOE"><span class="nav-text">3.4 Solution of the BOE</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-5-BOE-Optimality"><span class="nav-text">3.5 BOE: Optimality</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-6-What-does-pi-ast-look-like"><span class="nav-text">3.6 What does $\pi^\ast$ look like?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-7-Factors-that-influence-optimal-policies"><span class="nav-text">3.7 Factors that influence optimal policies</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-8-Summary"><span class="nav-text">3.8  Summary</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Ming Huang"
      src="/images/pic.png">
  <p class="site-author-name" itemprop="name">Ming Huang</p>
  <div class="site-description" itemprop="description">Let’s do something extraordinary together.</div>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://bit.edu.cn/" title="Beijing Institute of Technology → https:&#x2F;&#x2F;bit.edu.cn&#x2F;" rel="noopener" target="_blank"><i class="fa fa-university fa-fw"></i>Beijing Institute of Technology</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://scholar.google.com.hk/citations?user=nDvJ6BUAAAAJ&hl=en/" title="Google Scholar → https:&#x2F;&#x2F;scholar.google.com.hk&#x2F;citations?user&#x3D;nDvJ6BUAAAAJ&amp;hl&#x3D;en&#x2F;" rel="noopener" target="_blank"><i class="fa fa-graduation-cap fa-fw"></i>Google Scholar</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:huangming98@163.com" title="E-Mail → mailto:huangming98@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://orcid.org/0000-0001-9146-4721" title="ORCID → https:&#x2F;&#x2F;orcid.org&#x2F;0000-0001-9146-4721" rel="noopener" target="_blank"><i class="fa fa-id-card fa-fw"></i>ORCID</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://github.com/huangming98" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;huangming98" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>




<script src="/js/switch_language.js"></script>
      </div>

<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="https://music.163.com/outchain/player?type=2&id=1941963749&auto=1&height=66"></iframe>

    </div>
  </aside>

  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2022.09— 
  <span itemprop="copyrightYear">2026.01</span>
  
   
</div>
  <div class="powered-by"><p style="color:rgb(0,0,255,0.8);font-size:1.2em;font-weight:bold;font-family:Arial">Do not be afraid of making mistakes, only of missing opportunities!</p>
  </div>

<!-- Insert clustrmaps.com -->
<!-- <a target="_blank" rel="noopener" href='https://clustrmaps.com/site/1bqbj' ><img src='//clustrmaps.com/map_v2.png?cl=555555&w=350&t=m&d=b6-cOzsyxmN07VPEXlJO1PaWKRLFhE6feh1y5oiN1Hw&co=f5f5f5&ct=555555'/></a> -->


<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "<br />This website is as worthless as hundreds of daily chores, but I still hope it will be helpful to you! <br /> It's been running quietly for  "+dnum+" days ";
        document.getElementById("times").innerHTML = hnum + " hours " + mnum + " minutes " + snum + " seconds.";
    } 
setInterval("createtime()",250);
</script>



        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
