<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic|Roboto:300,300italic,400,400italic,700,700italic|JetBrains Mono:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.huangm.cn","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","Muse | Mist":180,"width":250,"display":"always","padding":20,"offset":12,"onmobile":true},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":"ture","lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Learning Notes for the Course of &quot;Mathematical Foundation of Reinforcement Learning&quot;（6-7）">
<meta property="og:type" content="article">
<meta property="og:title" content="Mathematical Foundation of Reinforcement Learning-TD algorithms">
<meta property="og:url" content="https://www.huangm.cn/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/index.html">
<meta property="og:site_name" content="Ming Huang&#39;s Homepage">
<meta property="og:description" content="Learning Notes for the Course of &quot;Mathematical Foundation of Reinforcement Learning&quot;（6-7）">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://www.huangm.cn/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-04-24-21-31-33.png">
<meta property="og:image" content="https://www.huangm.cn/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-04-24-21-40-10.png">
<meta property="og:image" content="https://www.huangm.cn/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-04-25-21-20-47.png">
<meta property="og:image" content="https://www.huangm.cn/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-05-22-16-27-35.png">
<meta property="og:image" content="https://www.huangm.cn/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-05-22-16-36-18.png">
<meta property="og:image" content="https://www.huangm.cn/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-05-22-16-53-59.png">
<meta property="og:image" content="https://www.huangm.cn/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-05-22-19-40-40.png">
<meta property="og:image" content="https://www.huangm.cn/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-05-22-19-41-00.png">
<meta property="og:image" content="https://www.huangm.cn/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-05-22-19-49-51.png">
<meta property="og:image" content="https://www.huangm.cn/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-05-22-19-50-11.png">
<meta property="og:image" content="https://www.huangm.cn/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-05-22-19-55-35.png">
<meta property="og:image" content="https://www.huangm.cn/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-05-22-19-55-45.png">
<meta property="og:image" content="https://www.huangm.cn/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-05-22-20-00-27.png">
<meta property="article:published_time" content="2024-08-09T03:39:09.000Z">
<meta property="article:modified_time" content="2024-10-23T03:56:44.000Z">
<meta property="article:author" content="Ming Huang">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.huangm.cn/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-04-24-21-31-33.png">

<link rel="canonical" href="https://www.huangm.cn/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Mathematical Foundation of Reinforcement Learning-TD algorithms | Ming Huang's Homepage</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>


<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Ming Huang's Homepage</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/home" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-publications">

    <a href="/publications/" rel="section"><i class="fa fa-book fa-fw"></i>Publications</a>

  </li>
        <li class="menu-item menu-item-resources">

    <a href="/resources/" rel="section"><i class="fa fa-cloud fa-fw"></i>Resources</a>

  </li>
        <li class="menu-item menu-item-link">

    <a href="/link/" rel="section"><i class="fa fa-link fa-fw"></i>Scholars Link</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Blog</a>

  </li>
        <li class="menu-item menu-item-switch-to-chinese">

    <a href="https://www.huangm.cn/cn/" rel="section"><i class="fa fa-language fa-fw"></i>Switch to Chinese</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://www.huangm.cn/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/pic.png">
      <meta itemprop="name" content="Ming Huang">
      <meta itemprop="description" content="Let’s do something extraordinary together.">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Ming Huang's Homepage">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Mathematical Foundation of Reinforcement Learning-TD algorithms
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2024-08-09 11:39:09" itemprop="dateCreated datePublished" datetime="2024-08-09T11:39:09+08:00">2024-08-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2024-10-23 11:56:44" itemprop="dateModified" datetime="2024-10-23T11:56:44+08:00">2024-10-23</time>
              </span>

          
            <span class="post-meta-item" title="Views" id="busuanzi_container_page_pv" style="display: none;">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span id="busuanzi_value_page_pv"></span>
            </span><br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>5k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>9 mins.</span>
            </span>
            <div class="post-description">Learning Notes for the Course of "Mathematical Foundation of Reinforcement Learning"（6-7）</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>[toc] </p>
<p>Sources:</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://github.com/MathFoundationRL/Book-Mathmatical-Foundation-of-Reinforcement-Learning"><em>Shiyu Zhao</em>. 《Mathematical Foundation of Reinforcement Learning》Chapter 6-7</a>.</li>
<li><a target="_blank" rel="noopener" href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#">OpenAI Spinning Up</a></li>
</ol>
<h2 id="6-Stochastic-Approximation"><a href="#6-Stochastic-Approximation" class="headerlink" title="6 Stochastic Approximation"></a>6 Stochastic Approximation</h2><p>We use the present chapter to fill this knowledge gap by introducing the basics of stochastic approximation. Although this chapter does not introduce any specific reinforcement learning algorithms, it lays the necessary foundations for studying subsequent chapters. We will see in Chapter 7 that <strong>the temporal-difference algorithms can be viewed as special stochastic approximation algorithms.</strong> The well-known stochastic gradient descent <strong>(SGD)</strong> algorithms widely used in machine learning are also introduced in the present chapter.</p>
<h3 id="6-1-Mean-estimation"><a href="#6-1-Mean-estimation" class="headerlink" title="6.1 Mean estimation"></a>6.1 Mean estimation</h3><p>The basic idea of <strong>Monte Carlo estimation</strong> is:</p>
<script type="math/tex; mode=display">
\mathbb{E}[X] \approx \bar{x}=\frac{1}{n} \sum_{j=1}^n x_j .</script><p>as $n$ increases, the approximation becomes increasingly accurate. When $n \rightarrow \infty$, we have $\bar{x} \rightarrow \mathbb{E}[X]$.</p>
<p>We next show that two methods can be used to calculate $\bar{x}$ . The first <strong>non-incremental method</strong> collects all the samples first and then calculates the average. The drawback of such a method is that, if the number of samples is large, we may have to wait for a long time until all of the samples are collected. </p>
<p>The second method can avoid this drawback because it calculates the average in an <strong>incremental manner</strong> Specifically, suppose that:</p>
<script type="math/tex; mode=display">
w_{k+1} =\frac{1}{k} \sum_{i=1}^k x_i ,  k=1,2,\dots .</script><script type="math/tex; mode=display">
w_{k} =\frac{1}{k-1} \sum_{i=1}^{k-1} x_i ,  k=2,3,\dots .</script><p>Therefore, we obtain the following <strong>incremental algorithm</strong>:</p>
<script type="math/tex; mode=display">
\begin{equation}
  w_{k+1} =w_{k}-\frac{1}{k}(w_k-x_k) .
\end{equation}</script><p>The advantage of the incremental algorithm is that the average can be immediately calculated every time we receive a sample. This average can be used to approximate $\bar{x}$ and hence $\mathbb{E}[X]$. Notably, the approximation may not be accurate at the beginning due to insufficient samples. However, it is better than nothing.</p>
<p>Furthermore, consider an algorithm with a more general expression:</p>
<script type="math/tex; mode=display">
\begin{aligned}
  \textcolor{red}{w_{k+1} =w_{k}-a_k(w_k-x_k)} .
\end{aligned}</script><p>It is the same as the incremental algorithm except that the coefficient $1/k$ is replaced by $a_k &gt; 0$ However, we will show in the next section that, if $k$ satisfies some mild conditions, $w_k \rightarrow \mathbb{E}[X]$ as $k \rightarrow \infty$. In Chapter 7, we will see that temporal-difference algorithms have similar (but more complex) expressions.</p>
<h3 id="6-2-Robbins-Monro-RM-algorithm"><a href="#6-2-Robbins-Monro-RM-algorithm" class="headerlink" title="6.2 Robbins-Monro (RM) algorithm"></a>6.2 Robbins-Monro (RM) algorithm</h3><p><strong>Robbins-Monro (RM) algorithm</strong>: solve $g(w)=0$ using $\left\{\tilde{g}\left(w_k, \eta_k\right)\right\}$ </p>
<script type="math/tex; mode=display">
 w_{k+1}=w_k-a_k \tilde{g}\left(w_k, \eta_k\right) ,k=1,2,3,\dots</script><p><strong>Stochastic approximation</strong> refers to a broad class of stochastic iterative algorithms solving root finding or optimization problems. Compared to many other root-finding algorithms such as gradient-based methods, Stochastic approximation is powerful in the sense that it does not require the expression of the objective function nor its derivative.</p>
<p><strong>Robbins-Monro (RM) algorithm</strong> is a pioneering work in the field of stochastic approximation. The famous stochastic gradient descent (SGD) algorithm is a special form of the RM algorithm. </p>
<h4 id="6-2-1-Problem-statement"><a href="#6-2-1-Problem-statement" class="headerlink" title="6.2.1 Problem statement"></a>6.2.1 Problem statement</h4><p>Problem statement: Suppose we would like to find the root of the equation</p>
<script type="math/tex; mode=display">
 g(w)=0</script><p>where $w \in \mathbb{R}$ is the variable to be solved and $g: \mathbb{R} \rightarrow \mathbb{R}$ is a function. Many problems can be formulated as root- finding problems. For example, if $J(w)$ is an objective function to be optimized, this optimization problem can be converted to solving $g(w) = \nabla_w J(w) = 0$.</p>
<p>If the expression of $g$ or its derivative is known, there are many numerical algorithms that can solve this problem.However, <strong>the problem we are facing is that the expression of the function g is unknown.</strong> For example, the function may be represented by an artificial neural network whose structure and parameters are unknown. Moreover, we can only obtain a noisy observation of $g(w)$:</p>
<script type="math/tex; mode=display">
\begin{equation}
 \textcolor{purple} {\tilde{g}(w, \eta)=g(w)+\eta}
\end{equation}</script><p>where $\eta \in \mathbb{R}$ is the <em>observation error</em>, which may or may not be Gaussian. In summary, it is a black-box system where only the input $w$ and the noisy output $\tilde{g}(w, \eta)$ are known. Our aim is to solve $g(w)=0$ using $w$ and $\tilde{g}$.<br><img src="/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-04-24-21-31-33.png" alt="Figure 6.2: An illustration of the problem of solving g(w) = 0 from w and g."></p>
<p>The Robbins-Monro (RM) algorithm can solve $g(w)=0$ is </p>
<script type="math/tex; mode=display">
\begin{equation}
 \textcolor{red} {w_{k+1}=w_k-a_k \tilde{g}\left(w_k, \eta_k\right) ,k=1,2,3,\dots}
\end{equation}</script><p>where $w_k$ is the $k$ th estimate of the root, $\tilde{g}\left(w_k, \eta_k\right)=g\left(w_k\right)+\eta_k$ is the $k$ th noisy observation $a_k$ is a positive coefficient. As can been seen, RM algorithm only requires the input ($\left\{w_k\right\}$) and output data ($\left\{\tilde{g}\left(w_k, \eta_k\right)\right\}$). It does not need to know the expression of $g(w)$. Next we introduce an example to illustrate RM algorithm.</p>
<h4 id="6-2-2-An-illustrative-example"><a href="#6-2-2-An-illustrative-example" class="headerlink" title="6.2.2 An illustrative example"></a>6.2.2 An illustrative example</h4><p><img src="/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-04-24-21-40-10.png" alt=""><br>Consider the example shown in Figure. In this example, $g(w)=\tanh (w-1)$, the true root of $g(w)=0$ is $w^\ast=1$. Parameters: $w_1=3, a_k=1 / k, \eta_k \equiv 0$ (no noise for the sake of simplicity) The RM algorithm in this case is $\textcolor{blue} {w_{k+1}=w_k-a_k g\left(w_k\right)} $ since $\tilde{g}\left(w_k, \eta_k\right)=g\left(w_k\right)$ when $\eta_k=0$. The resulting $\left\{w_k\right\}$ generated by the RM algorithm is shown in Figure . It can be seen that $w_k$ converges to the true root $w^\ast=1$.</p>
<p>This simple example can illustrate why the RM algorithm converges.</p>
<ul>
<li>When $w_k&gt;w^\ast$, we have $g\left(w_k\right)&gt;0$. Then, $w_{k+1}=w_k-a_k g\left(w_k\right)&lt;w_k$. If $a_k g\left(w_k\right)$ is sufficiently small, we have $w^\ast&lt;w_{k+1}&lt;w_k$. As a result, $w_{k+1}$ is closer to $w^\ast$ than $w_k$.</li>
<li>When $w_k<w^\ast$, we="" have="" $g\left(w_k\right)<0$.="" then,="" $w_{k+1}="w_k-a_k" g\left(w_k\right)="">w_k$. If $\left|a_k g\left(w_k\right)\right|$ is sufficiently small, we have $w^\ast&gt;w_{k+1}&gt;w_k$. As a result, $w_{k+1}$ is closer to $w^\ast$ than $w_k$.</w^\ast$,></li>
</ul>
<p>In either case,$w_{k+1}$ is closer to $w^\ast$ than $w_k$. Therefore, it is intuitive that $w_k$ converges to $w^\ast$.</p>
<h4 id="6-2-3-RM-algorithm’s-proof"><a href="#6-2-3-RM-algorithm’s-proof" class="headerlink" title="6.2.3 RM algorithm’s proof"></a>6.2.3 RM algorithm’s proof</h4><p>Robbins-Monro theorem: In the Robbins-Monro algorithm $\textcolor{red} {w_{k+1}=w_k-a_k \tilde{g}\left(w_k, \eta_k\right) ,k=1,2,3,\dots}$, if：<br><em>(a)</em> $0&lt;c_1 \leq \nabla_w g(w) \leq c_2$ for all $w$;<br><em>(b)</em> $\sum_{k=1}^{\infty} a_k=\infty$ and $\sum_{k=1}^{\infty} a_k^2&lt;\infty$;<br><em>(c)</em> $\mathbb{E}\left[\eta_k \mid \mathcal{H}_k\right]=0$ and $\mathbb{E}\left[\eta_k^2 \mid \mathcal{H}_k\right]&lt;\infty$;</p>
<p>where $\mathcal{H}_k=\left\{w_k, w_{k-1}, \ldots\right\}$, then $w_k$ almost surely converges to the root $w^\ast$ satisfying $g\left(w^\ast\right)=0$.</p>
<p>The three conditions in previous are explained as follows.</p>
<p><strong><em>Condition (a)</em></strong><br>$0&lt;c_1 \leq \nabla_w g(w)$ indicates that $g(w)$ is a monotonically increasing function. This condition ensures that the root of $g(w)=0$ exists and is unique. If $g(w)$ is monotonically decreasing, we can simply treat $-g(w)$ as a new function that is monotonically increasing.</p>
<p>As an application, we can formulate an optimization problem in which the objective function is $J(w)$ as a root- finding problem: $g(w) = \nabla_wJ(w) = 0$. In this case, the condition that $g(w)$ is monotonically increasing <strong>indicates that $J(w)$ is convex, which is a commonly adopted assumption in optimization problems.</strong></p>
<p>The inequality $\nabla_w g(w) \leq c_2$ indicates that the gradient of $g(w)$ is bounded from above. For example, $g(w)=\tanh (w-1)$ satisfies this condition, but $g(w)=w^3-5$ does not.</p>
<p><strong><em>Condition (b)</em></strong><br>The second condition about $\left\{a_k\right\}$ is interesting. We often see conditions like this in reinforcement learning algorithms.</p>
<p>In particular, the condition $\sum_{k=1}^{\infty} a_k^2&lt;\infty$ means that $\lim _{n \rightarrow \infty} \sum_{k=1}^n a_k^2$ is bounded from above. It requires that $a_k$ converges to zero as $k \rightarrow \infty$.</p>
<p>The condition $\sum_{k=1}^{\infty} a_k=\infty$ means that $\lim _{n \rightarrow \infty} \sum_{k=1}^n a_k$ is infinitely large. It requires that $a_k$ should not converge to zero too fast. These conditions have interesting properties, which will be analyzed in detail shortly.</p>
<p><strong><em>Condition (c)</em></strong><br>The condition is mild. It does not require the observation error $\eta_k$ to be Gaussian. An important special case is that $\left\{\eta_k\right\}$ is an i.i.d. stochastic sequence satisfying $\mathbb{E}\left[\eta_k\right]=0$ and $\mathbb{E}\left[\eta_k^2\right]&lt;\infty$. In this case, the third condition is valid because $\eta_k$ is independent of $\mathcal{H}_k$ and hence we have $\mathbb{E}\left[\eta_k \mid \mathcal{H}_k\right]=\mathbb{E}\left[\eta_k\right]=0$ and $\mathbb{E}\left[\eta_k^2 \mid \mathcal{H}_k\right]=\mathbb{E}\left[\eta_k^2\right]$.</p>
<p><strong><u>Why is the second condition important for the convergence of the RM algorithm?</u></strong></p>
<p>This question can naturally be answered when we present a rigorous proof of the above theorem later. Here, we would like to provide some insightful intuition.</p>
<p>First, $\sum_{k=1}^{\infty} a_k^2&lt;\infty$ indicates that $a_k \rightarrow 0$ as $k \rightarrow \infty$. Why is this condition important? Suppose that the observation $\tilde{g}\left(w_k, \eta_k\right)$ is always bounded. Since </p>
<script type="math/tex; mode=display">
w_{k+1}-w_k=-a_k \tilde{g}\left(w_k, \eta_k\right)</script><p>if $a_k \rightarrow 0$, then $a_k \tilde{g}\left(w_k, \eta_k\right) \rightarrow 0$ and hence $w_{k+1}-w_k \rightarrow 0$, indicating that $w_{k+1}$ and $w_k$ approach each other when $k \rightarrow \infty$.Otherwise, if $a_k$ does not converge, then $w_k$ may still fluctuate when $k \rightarrow \infty$.</p>
<p>Second, $\sum_{k=1}^{\infty} a_k=\infty$ indicates that $a_k$ should not converge to zero too fast. Why is this condition important?</p>
<p>Summarizing both sides of the equations of $ w_2-w_1 = -a_1 \tilde{g}\left(w_1, \eta_1\right), w_3-w_2=-a_2 \tilde{g}\left(w_2, \eta_2\right), w_4-w_3=-a_3 \tilde{g}\left(w_3, \eta_3\right), \ldots $ gives </p>
<script type="math/tex; mode=display">
w_1-w_{\infty}=\sum_{k=1}^{\infty} a_k \tilde{g}\left(w_k, \eta_k\right) .</script><p>If $\sum_{k=1}^{\infty} a_k&lt;\infty$, then $\left|\sum_{k=1}^{\infty} a_k \tilde{g}\left(w_k, \eta_k\right)\right|$ is also bounded. But we will show that this can not guarantee the convergence. Let $b$ denote the finite upper bound such that </p>
<script type="math/tex; mode=display">
\begin{equation}
\left|w_1-w_{\infty}\right|=\left|\sum_{k=1}^{\infty} a_k \tilde{g}\left(w_k, \eta_k\right)\right| \leq b . 
\end{equation}</script><p>If the initial guess $w_1$ is selected far away from $w^\ast$ so that $\left|w_1-w^\ast\right|&gt;b$, then it is impossible to have $w_{\infty}=w^\ast$. This suggests that the RM algorithm cannot find the true solution $w^\ast$ in this case. Therefore, the condition $\sum_{k=1}^{\infty} a_k=\infty$ is necessary to ensure convergency given an arbitrary initial guess.</p>
<p><strong><u>An example of $a_k$</u></strong><br>What kinds of sequences satisfy $\sum_{k=1}^{\infty} a_k=\infty$ and $\sum_{k=1}^{\infty} a_k^2&lt;\infty$ ?</p>
<p>One typical sequence is </p>
<script type="math/tex; mode=display">
a_k=\frac{1}{k} .</script><p>On the one hand, it holds that </p>
<script type="math/tex; mode=display">
\lim _{n \rightarrow \infty}\left(\sum_{k=1}^n \frac{1}{k}-\ln n\right)=\kappa,</script><p>where $\kappa \approx 0.577$ is called the Euler-Mascheroni constant (or Euler’s constant) . Since $\ln n \rightarrow \infty$ as $n \rightarrow \infty$, we have </p>
<script type="math/tex; mode=display">
  \sum_{k=1}^{\infty} \frac{1}{k}=\infty</script><p>In fact, $H_n=\sum_{k=1}^n \frac{1}{k}$ is called the harmonic number in number theory [29]. On the other hand, it holds that </p>
<script type="math/tex; mode=display">
\sum_{k=1}^{\infty} \frac{1}{k^2}=\frac{\pi^2}{6}<\infty .</script><p>Finding the value of $\sum_{k=1}^{\infty} \frac{1}{k^2}$ is known as the Basel problem [30].</p>
<p>In summary, the sequence $\left\{a_k=1 / k\right\}$ satisfies the second condition in Theorem 6.1. Notably, a slight modification, such as $a_k=1 /(k+1)$ or $a_k=c_k / k$ where $c_k$ is bounded, also preserves this condition.</p>
<p><strong>Note:</strong><br>In the RM algorithm, <strong>$a_k$ is often selected as a sufficiently small constant in many applications.</strong> Although the second condition is not satisfied anymore in this case because $\sum_{k=1}^{\infty} a_k^2=\infty$ rather than $\sum_{k=1}^{\infty} a_k^2&lt;\infty$, the algorithm can still converge in a certain sense.</p>
<h4 id="6-2-4-Mean-estimation-is-a-special-example-of-RM-algorithm"><a href="#6-2-4-Mean-estimation-is-a-special-example-of-RM-algorithm" class="headerlink" title="6.2.4 Mean estimation is a special example of RM algorithm"></a>6.2.4 Mean estimation is a special example of RM algorithm</h4><p>In particular, define a function as</p>
<script type="math/tex; mode=display">
g(w) = w- \mathbb{E}[X]</script><p>The original problem is to obtain the value of $\mathbb{E}[X]$. This problem is formulated as a root- finding problem to solve $g(w) = 0$. Given a value of $w$, the noisy observation that we can obtain is $\tilde{g} = w -x$, where $x$ is a sample of $X$. Note that $\tilde{g}$ can be written as</p>
<script type="math/tex; mode=display">
\begin{aligned} 
\tilde{g}(w, \eta) & = w-x \\ 
& =w-x+\mathbb{E}[X] -\mathbb{E}[X] \\
& =(w-\mathbb{E}[X]) + (\mathbb{E}[X]-x)\\
& =g(w)+\eta
\end{aligned}</script><p>The RM algorithm for solving this problem is</p>
<script type="math/tex; mode=display">
w_{k+1}=w_k- a_k \tilde{g}\left(w_k, \eta_k\right)=w_k-a_k(w_k-x_k).</script><p>So, solving $\mathbb{E}(X)$ by mean estimation can be convert to solving <strong>a RM algorithm with $g(w) = w-\mathbb{E}(x)$ and $a_k=\frac{1}{k}$.</strong></p>
<h3 id="6-3-Stochastic-gradient-descent"><a href="#6-3-Stochastic-gradient-descent" class="headerlink" title="6.3 Stochastic gradient descent"></a>6.3 Stochastic gradient descent</h3><p>Stochastic Gradient Descent (SGD) algorithm: minimize $J(w)=\mathbb{E}[f(w, X)]$ using $\left\{\nabla_w f\left(w_k, x_k\right)\right\}$ </p>
<script type="math/tex; mode=display">
w_{k+1}=w_k-\alpha_k \nabla_w f\left(w_k, x_k\right)</script><p>The iterative mean estimation algorithm is a special form of the Stochastic Gradient Descent (SGD) algorithm, and  the SGD algorithm is a special form of the RM algorithm. (<strong>IME → SGD → RM</strong>)</p>
<p>Consider the following optimization problem:</p>
<script type="math/tex; mode=display">
\min_w J(w)=\mathbb{E}[f(w, X)]</script><p>where $w$ is the parameter to be optimized. $X$ is a random variable. The expectation is with respect to $X$. $w$ and $X$ can be either scalars or vectors. The function $f(\cdot)$ is a scalar.</p>
<p><strong>The gradient descent algorithm (GD)</strong></p>
<script type="math/tex; mode=display">
\begin{aligned} 
w_{k+1} & = w_k-\alpha_k \nabla_w J(w) = w_k-\alpha_k  \nabla_w \mathbb{E}\left[f\left(w_k, X\right)\right]\\
& = w_k-\alpha_k \textcolor{blue} {\mathbb{E}\left[\nabla_w f\left(w_k, X\right)\right]}
\end{aligned}</script><p><em>we must know the probability distribution of $X$, if you want use this algorithm.</em></p>
<p><strong>The batch gradient descent algorithm (BGD)</strong><br>We can use Monte Carlo estimation to compute the estimate $\mathbb{E}\left[\nabla_w f\left(w_k, X\right)\right]$. </p>
<script type="math/tex; mode=display">
\begin{aligned} 
w_{k+1} & = w_k-\alpha_k \mathbb{E}\left[\nabla_w f\left(w_k, X\right)\right]\\
&\approx w_k-\alpha_k \textcolor{blue} {\frac{1}{n} \sum_{i=1}^n \nabla_w f(w_k , x_i)} \\
&(\text{ if no model need have data/samples})
\end{aligned}</script><p><em>One problem of the BGD algorithm is that it requires all the samples in each iterationfor each $w_k$.</em></p>
<p><strong>The stochastic gradient descent algorithm (SGD)</strong></p>
<script type="math/tex; mode=display">
\begin{equation}
w_{k+1} = w_k-\alpha_k \textcolor{blue} {\nabla_w f\left(w_k, x_k\right)}, \end{equation}</script><p>where $x_k$ is the sample collected at time step $k$. It relies on stochastic samples ${x_k}$</p>
<ol>
<li><p>Compared to the gradient descent algorithm: Replace the true gradient $\mathbb{E}\left[\nabla_w f\left(w_k, X\right)\right]$ by the stochastic gradient $\nabla_w f\left(w_k, x_k\right)$. </p>
</li>
<li><p>Compared to the batch gradient descent method: let $n=1$.</p>
</li>
<li><p><strong>From GD to SGD</strong>: The stochastic gradient $\nabla_w f\left(w_k, x_k\right)$ can be viewed as a noisy measurement of the true gradient $\mathbb{E}\left[\nabla_w f(w, X)\right]$: </p>
</li>
</ol>
<script type="math/tex; mode=display">
\nabla_w f\left(w_k, x_k\right)=\mathbb{E}\left[\nabla_w f(w, X)\right]+\underbrace{\nabla_w f\left(w_k, x_k\right)-\mathbb{E}\left[\nabla_w f(w, X)\right]}_\eta .</script><script type="math/tex; mode=display">
\begin{aligned} 
\mathbb{E}(\eta) & =\mathbb{E}(\nabla_w f\left(w_k, x_k\right)-\mathbb{E}\left[\nabla_w f(w, X)\right])\\
& =\mathbb{E}_{x_k}[\nabla_w f\left(w_k, x_k\right)]-\mathbb{E}_X\left[\nabla_w f(w, X)\right]=0
\end{aligned}</script><p>Therefore, the perturbation term $\eta_{k}$ has a zero mean, which intuitively suggests that it may not jeopardize the convergence property. </p>
<h4 id="6-3-1-Mean-estimation-is-a-special-example-of-SGD"><a href="#6-3-1-Mean-estimation-is-a-special-example-of-SGD" class="headerlink" title="6.3.1 Mean estimation is a special example of SGD"></a>6.3.1 Mean estimation is a special example of SGD</h4><p>we formulate the mean estimation problem as an optimization problem:</p>
<script type="math/tex; mode=display">
 \min _w J(w)=\mathbb{E}[f(w, X)]=\mathbb{E}\left[\frac{1}{2}\|w-X\|^2\right],</script><p>where $ f(w, X)=|w-X|^2 / 2 \quad \nabla_w f(w, X)=w-X $ . It can be verified that the optimal solution is by solving $\textcolor{green} {\nabla_w J(w)=0}$. Therefore, this optimization problem is equivalent to the iterative mean estimation problem.</p>
<script type="math/tex; mode=display">
\begin{aligned}
w_{k+1} & =w_k-\alpha_k \nabla_w J\left(w_k\right) \\ & =w_k-\alpha_k \mathbb{E}\left[\nabla_w f\left(w_k, X\right)\right] \\ & =w_k-\alpha_k \mathbb{E}\left[w_k-X\right] . 
\end{aligned}</script><p>The SGD algorithm for solving the above problem is </p>
<script type="math/tex; mode=display">
w_{k+1}=w_k-\alpha_k \nabla_w f\left(w_k, x_k\right)=w_k-\alpha_k\left(w_k-x_k\right)</script><p><strong>We can therefore see that the iterative mean estimation algorithm is a special form of the SGD algorithm for solving the mean estimation problem.</strong></p>
<h4 id="6-3-2-SGD-is-a-special-example-of-RM-algorithm-Convergence-analysis"><a href="#6-3-2-SGD-is-a-special-example-of-RM-algorithm-Convergence-analysis" class="headerlink" title="6.3.2 SGD is a special example of RM algorithm (Convergence analysis)"></a>6.3.2 SGD is a special example of RM algorithm (Convergence analysis)</h4><p>We next show that SGD is a special RM algorithm, Then, the convergence naturally follows.</p>
<p>The aim of SGD is to minimize </p>
<script type="math/tex; mode=display">
J(w)=\mathbb{E}[f(w, X)]</script><p>This problem can be converted to a root-finding problem: </p>
<script type="math/tex; mode=display">
\nabla_w J(w)=\mathbb{E}\left[\nabla_w f(w, X)\right]=0</script><p>Let </p>
<script type="math/tex; mode=display">
\textcolor{blue} {g(w)=\nabla_w J(w)=\mathbb{E}\left[\nabla_w f(w, X)\right]} .</script><p>Then, the aim of SGD is to find the root of $g(w)=0$.</p>
<p>As in RM algorithm, we don’t know $g(w)$. Knowing it needs GD (gradient descent), not SGD.</p>
<p>What we can measure is </p>
<script type="math/tex; mode=display">
\begin{aligned}
 \tilde{g}(w, \eta) & =\nabla_w f(w, x) \\ & =\underbrace{\mathbb{E}\left[\nabla_w f(w, X)\right]}_{g(w)}+\underbrace{\nabla_w f(w, x)-\mathbb{E}\left[\nabla_w f(w, X)\right]}_\eta .
\end{aligned}</script><p>Then, the RM algorithm for solving $g(w)=0$ is </p>
<script type="math/tex; mode=display">
w_{k+1}=w_k-a_k \tilde{g}\left(w_k, \eta_k\right)=w_k-a_k \nabla_w f\left(w_k, x_k\right) .</script><p><strong>It is exactly the SGD algorithm. Therefore, SGD is a special RM algorithm.Since SGD is a special RM algorithm, its convergence naturally follows.</strong></p>
<p><strong>Theorem</strong> (Convergence of SGD): In the SGD algorithm, if</p>
<p>$0&lt;c_1 \leq \nabla_w^2 f(w, X) \leq c_2$;<br>$\sum_{k=1}^{\infty} a_k=\infty$ and $\sum_{k=1}^{\infty} a_k^2&lt;\infty$;<br>$\left\{x_k\right\}_{k=1}^{\infty}$ is iid;<br>then $w_k$ converges to the root of $ \mathbb{E}[\nabla_wf(w, X)]=0$ with probability 1.</p>
<h4 id="6-3-3-Convergence-pattern"><a href="#6-3-3-Convergence-pattern" class="headerlink" title="6.3.3 Convergence pattern"></a>6.3.3 Convergence pattern</h4><p>Since the stochastic gradient is random and hence the approximation is inaccurate, whether the convergence of SGD is slow or random? To answer this question, we consider the relative error between the stochastic and batch gradients:</p>
<p>The relative error between the stochastic and true gradients is </p>
<script type="math/tex; mode=display">
\delta_k \triangleq \frac{\left|\nabla_w f\left(w_k, x_k\right)-\mathbb{E}\left[\nabla_w f\left(w_k, X\right)\right]\right|}{\left|\mathbb{E}\left[\nabla_w f\left(w_k, X\right)\right]\right|} .</script><p>For the sake of simplicity, we consider the case where $w$ and $\nabla_w f(w, x)$ are both scalars. Since $w^\ast$ is the optimal solution, it holds that $\mathbb{E}\left[\nabla_w f\left(w^\ast, X\right)\right]=0$. Then, the relative error can be rewritten as </p>
<script type="math/tex; mode=display">
\delta_k=\frac{\left|\nabla_w f\left(w_k, x_k\right)-\mathbb{E}\left[\nabla_w f\left(w_k, X\right)\right]\right|}{\left|\mathbb{E}\left[\nabla_w f\left(w_k, X\right)\right]-\textcolor{blue} {\mathbb{E} [\nabla_w f\left(w^\ast, X\right)] }\right|} = \frac{\left|\nabla_w f\left(w_k, x_k\right)-\mathbb{E}\left[\nabla_w f\left(w_k, X\right)\right]\right|}{| \textcolor{purple} {\mathbb{E} [\nabla_w^2 f\left(\tilde{w}_k, X\right)\left(w_k-w^\ast\right)] }|},</script><p>where the last equality is due to the <strong>mean value theorem</strong> and $\tilde{w}_k \in\left[w_k, w^\ast\right]$. Suppose that $f$ is strictly convex such that $\nabla_w^2 f \geq c&gt;0$ for all $w, X$. Then, the denominator becomes </p>
<script type="math/tex; mode=display">
 \begin{aligned} \left|\mathbb{E}\left[\nabla_w^2 f\left(\tilde{w}_k, X\right)\left(w_k-w^\ast\right)\right]\right| & =\left|\mathbb{E}\left[\nabla_w^2 f\left(\tilde{w}_k, X\right)\right]\right|\left|\left(w_k-w^\ast\right)\right| \\ & \geq c\left|w_k-w^\ast\right| . \end{aligned}</script><p>Substituting the above inequality into the above equation yields </p>
<script type="math/tex; mode=display">
\delta_k \leq \frac{|\overbrace{\nabla_w f\left(w_k, x_k\right)}^{\text {stochastic gradient }}-\overbrace{\mathbb{E}\left[\nabla_w f\left(w_k, X\right)\right]}^{\text {true gradient }}|}{\underbrace{c\left|w_k-w^\ast\right|}_{\text {distance to the optimal solution }}} .</script><p>The above inequality suggests an interesting convergence pattern of SGD: the relative error $\delta_k$ is inversely proportional to $\left|w_k-w^\ast\right|$. As a result, when $\left|w_k-w^\ast\right|$ is large, $\delta_k$ is small. In this case, the SGD algorithm behaves like the gradient descent algorithm and hence $w_k$ quickly converges to $w^\ast$. When $w_k$ is close to $w^\ast$, the relative error $\delta_k$ may be large, and the convergence exhibits more randomness.</p>
<p><img src="/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-04-25-21-20-47.png" alt=""></p>
<p>The simulation results are shown in Figure. Here, $X\in \mathbb{R^2}$ represents a random position in the plane. Its distribution is uniform in the square area centered at the origin and $\mathbb{E}[X] = 0$. The mean estimation is based on 100 i.i.d. samples. <strong>Although the initial guess of the mean is far away from the true value, it can be seen that the SGD estimate quickly approaches the neighborhood of the origin. When the estimate is close to the origin, the convergence process exhibits certain randomness.</strong></p>
<h4 id="6-3-4-BGD-SGD-and-mini-batch-GD"><a href="#6-3-4-BGD-SGD-and-mini-batch-GD" class="headerlink" title="6.3.4 BGD, SGD, and mini-batch GD"></a>6.3.4 BGD, SGD, and mini-batch GD</h4><div class="table-container">
<table>
<thead>
<tr>
<th style="text-align:center">algorithms</th>
<th style="text-align:center">sample used in every iteration</th>
<th style="text-align:center">randomness</th>
<th style="text-align:center">flexible and efficient</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">SGD</td>
<td style="text-align:center">a single sample ($1$)</td>
<td style="text-align:center">maximum</td>
<td style="text-align:center">maximum</td>
</tr>
<tr>
<td style="text-align:center">MBGD</td>
<td style="text-align:center">a few more samples ($1&lt;=m&lt;=n$)</td>
<td style="text-align:center">median</td>
<td style="text-align:center">median</td>
</tr>
<tr>
<td style="text-align:center">BGD</td>
<td style="text-align:center">all samples   ($n$)</td>
<td style="text-align:center">minimum</td>
<td style="text-align:center">minimum</td>
</tr>
</tbody>
</table>
</div>
<p><strong>notes</strong><br>If $m=n$, MBGD does NOT become BGD strictly speaking, because MBGD uses randomly fetched $n$ samples whereas BGD uses all $n$ numbers. In particular, MBGD may use a value in $\left\{x_i\right\}_{i=1}^n$ multiple times whereas BGD uses each number once.</p>
<p>Suppose we would like to minimize $J(w)=\mathbb{E}[f(w, X)]$, given a set of random samples $\left\{x_i\right\}_{i=1}^n$ of $X$. The BGD, SGD, MBGD algorithms solving this problem are, respectively, </p>
<script type="math/tex; mode=display">
\begin{aligned}
& w_{k+1}=w_k-\alpha_k \frac{1}{n} \sum_{i=1}^n \nabla_w f\left(w_k, x_i\right), \quad \text{(BGD)}\\ & w_{k+1}=w_k-\alpha_k \frac{1}{m} \sum_{j \in \mathcal{I}_k} \nabla_w f\left(w_k, x_j\right), \quad \text{(MBGD)}\\ & w_{k+1}=w_k-\alpha_k \nabla_w f\left(w_k, x_k\right) \quad \text{(SGD)} 
\end{aligned}</script><p>where $n$ is the size of the dataset, $m \leq n$.</p>
<ol>
<li><p>In the BGD algorithm, all the samples are used in every iteration. When $n$ is large, $(1 / n) \sum_{i=1}^n \nabla_w f\left(w_k, x_i\right)$ is close to the true gradient $\mathbb{E}\left[\nabla_w f\left(w_k, X\right)\right]$.</p>
</li>
<li><p>In the MBGD algorithm, $\mathcal{I}_k$ is a subset of $\{1, \ldots, n\}$ with the size as $\left|\mathcal{I}_k\right|=m$. The set $\mathcal{I}_k$ is obtained by $m$ times i.d.d. samplings.</p>
</li>
<li><p>In the SGD algorithm, $x_k$ is randomly sampled from $\left\{x_i\right\}_{i=1}^n$ at time $k$.</p>
</li>
</ol>
<p><strong>A good example</strong><br>Given some numbers $\left\{x_i\right\}_{i=1}^n$, our aim is to calculate the mean $\bar{x}=\sum_{i=1}^n x_i / n$. This problem can be equivalently stated as the following optimization problem: </p>
<script type="math/tex; mode=display">
\min _w J(w)=\frac{1}{2 n} \sum_{i=1}^n\left\|w-x_i\right\|^2</script><p>The three algorithms for solving this problem are, respectively, </p>
<script type="math/tex; mode=display">
\begin{aligned} & w_{k+1}=w_k-\alpha_k \frac{1}{n} \sum_{i=1}^n\left(w_k-x_i\right)=w_k-\alpha_k\left(w_k-\bar{x}\right), \quad \text{(BGD)} \\ & w_{k+1}=w_k-\alpha_k \frac{1}{m} \sum_{j \in \mathcal{I}_k}\left(w_k-x_j\right)=w_k-\alpha_k\left(w_k-\bar{x}_k^{(m)}\right), \quad \text{(MBGD)}\\ & w_{k+1}=w_k-\alpha_k\left(w_k-x_k\right), \quad \text { (SGD) } \end{aligned}</script><p>where $\bar{x}_k^{(m)}=\sum_{j \in \mathcal{I}_k} x_j / m$. Furthermore, if $\alpha_k=1 / k$, the above equation can be solved as </p>
<script type="math/tex; mode=display">
 \begin{aligned} & w_{k+1}=\frac{1}{k} \sum_{j=1}^k \bar{x}=\bar{x}, \quad \text{(BGD)} \\ & w_{k+1}=\frac{1}{k} \sum_{j=1}^k \bar{x}_j^{(m)}, \quad \text{(MBGD)}\\ & w_{k+1}=\frac{1}{k} \sum_{j=1}^k x_j . \quad \text{(SGD)} \end{aligned}</script><ol>
<li><p>The estimate of BGD at each step is exactly the optimal solution $w^\ast=\bar{x}$.</p>
</li>
<li><p>The estimate of MBGD converges the mean faster than SGD because $\bar{x}_k^{(m)}$ is already an average.</p>
</li>
<li><p>the convergence rate of SGD is still fast, especially when $w_k$ is far from $w^\ast$.</p>
</li>
</ol>
<h3 id="6-4-Summary"><a href="#6-4-Summary" class="headerlink" title="6.4 Summary"></a>6.4 Summary</h3><p>This chapter introduced the preliminaries of <strong>stochastic approximation</strong> such as the RM and SGD algorithms. Compared to many other root-finding algorithms, the RM algorithm does <strong>not require the expression of the objective function or its derivative.</strong> It has been shown that the SGD algorithm is a special RM algorithm. Moreover, an important problem frequently discussed throughout this chapter is mean estimation. The mean estimation algorithm is a special SGD algorithm. We will see in Chapter 7 that temporal-difference learning algorithms have similar expressions. Finally, the name <strong>“stochastic approximation”</strong> was first used by Robbins and Monro in 1951 <sup><a href="#fn_1" id="reffn_1">1</a></sup>. More information about stochastic approximation can be found in <sup><a href="#fn_2" id="reffn_2">2</a></sup>.</p>
<p><sup><a href="#fn_1" id="reffn_1">1</a></sup>:H. Robbins and S. Monro, A stochastic approximation method, The Annals of<br> Mathematical Statistics, pp. 400407, 1951.</p>
<blockquote id="fn_2">
<sup>2</sup>. H.-F. Chen, Stochastic approximation and its applications, vol. 64. Springer Science &amp; Business Media, 2006<a href="#reffn_2" title="Jump back to footnote [2] in the text."> ↩</a>
</blockquote>
<h2 id="7-Temporal-Difference-Methods"><a href="#7-Temporal-Difference-Methods" class="headerlink" title="7 Temporal-Difference Methods"></a>7 Temporal-Difference Methods</h2><p>Temporal-difference (TD) learning is one of the most well-known methods in reinforcement learning (RL).TD learning often refers to a broad class of reinforcement learning algorithms(Sarsa\ Expectded Sarsa\ n-step Sarsa\Q-learning eta).<br>Monte Carlo (MC) learning and TD learning are the model-free methods. but TD learning has some advantages due to its <strong>incremental form</strong>.<br>TD learning algorithms can be viewed as special stochastic algorithms for solving the Bellman or Bellman optimality equations.</p>
<h3 id="7-1-TD-learning-of-state-values"><a href="#7-1-TD-learning-of-state-values" class="headerlink" title="7.1 TD learning of state values"></a>7.1 TD learning of state values</h3><h4 id="7-1-1-Algorithm-description"><a href="#7-1-1-Algorithm-description" class="headerlink" title="7.1.1 Algorithm description"></a>7.1.1 Algorithm description</h4><p>TD algorithm can estimate the state values of a given policy using the following equations:<br>Suppose that we have some experience samples $(s_0, r_1, s_1,\dots, s_t, r_{t+1}, s_{t+1},\dots)$ generated following .</p>
<script type="math/tex; mode=display">
\begin{equation}
\textcolor{blue}{v_{t+1}(s_t) =v_{t}(s_t)-\alpha_t(s_t)[v_{t}(s_t)-(r_{t+1}+\gamma v_{t}(s_{t+1}))]}  , \end{equation}</script><script type="math/tex; mode=display">
\begin{equation}
\textcolor{blue}{v_{t+1}(s) =v_{t}(s) \quad \text{for  all} \quad s \neq s_t} \end{equation}</script><p>where $t = 0,1,2,\dots$ denotes the time step. Here, $v_t(s_t)$ is the estimate of $v_\pi(s_t)$ at time $t$; $\alpha_t(s_t)$ is the learning rate for $s_t$ at time $t$. Equation (7) is often omitted for simplicity, but it should be kept in mind because the algorithm would be <em>mathematically incomplete without this equation</em>.</p>
<p>TD learning algorithm can be viewed as a special stochastic approximation<br> algorithm(i.e. Robbins-Monro algorithm) for solving the Bellman equation.</p>
<script type="math/tex; mode=display">
v_{t+1}(s_t) =\textcolor{blue}{\mathbb{E}[R_{t+1}+\gamma G_{t+1}\mid S_{t}=s], s \in S} \longrightarrow \textcolor{green}{\mathbb{E}[R_{t+1}+\gamma v_{\pi}(S_{t+1})\mid S_{t}=s], s \in S}</script><p>It is another expression of the Bellman equation which sometimes called the <strong>Bellman expectation equation</strong>.</p>
<blockquote>
<p>Derivation of the TD algorithm</p>
<script type="math/tex; mode=display">
g(v_{\pi}(s_t))=v_{\pi}(s_t) - \mathbb{E}[R_{t+1}+\gamma v_{\pi}(S_{t+1})\mid S_{t}=s_t]</script><p>our goal is to solve the $g(v_{\pi}(s_t))=0$ to obtain $v_{\pi}(s_t)$ using the RM algorithm. $r_{t+1}$ and $s_{t+1}$ are the samples of $R_{t+1}$ and $S_{t+1}$,the noisy observation of $g(v_{\pi}(s_t))$ that we can obtain is</p>
<script type="math/tex; mode=display">
\tilde{g}(v_{\pi}(s_t))=v_{\pi}(s_t) - [r_{t+1}+\gamma v_{\pi}(s_{t+1})]=g(v_{\pi}(s_t))+\eta</script><p>Therefore, the Robbins-Monro algorithm (Section 6.2) for solving $g(v_{\pi}(s_t)) = 0$ is </p>
<script type="math/tex; mode=display">
\begin{aligned} 
v_{t+1}(s_t) &=v_{t}(s_t)-\alpha_t(s_t)\tilde{g}(v_{\pi}(s_t))\\
& =v_{t}(s_t)-\alpha_t(s_t)[v_{\textcolor{red}{\pi}}(s_t)-(r_{t+1}+\gamma v_{\textcolor{red}{\pi}}(s_{t+1}))]\\
\text{(convergence)}&\approx v_{t}(s_t)-\alpha_t(s_t)[v_{\textcolor{red}{t}}(s_t)-(r_{t+1}+\gamma v_{\textcolor{red}{t}}(s_{t+1}))]
\end{aligned}</script><p>$v_t(s)$ converges almost surely to $v_{\pi}(s)$ as $t \to \infty$ for all $s \in S$</p>
</blockquote>
<h4 id="7-1-2-Property-analysis-TD-target-and-TD-error"><a href="#7-1-2-Property-analysis-TD-target-and-TD-error" class="headerlink" title="7.1.2 Property analysis (TD target and TD error)"></a>7.1.2 Property analysis (TD target and TD error)</h4><p>TD algorithm can be described as</p>
<script type="math/tex; mode=display">
\begin{equation}
\textcolor{red}{\underbrace{v_{t+1}(s_t)}_{\text{new estimate}} =\underbrace{v_{t}(s_t)}_{\text{current estimate}}-\alpha_t(s_t)[\overbrace{v_{t}(s_t)-(\underbrace{r_{t+1}+\gamma v_{t}(s_{t+1})}_{\text{TD target} \quad{} \tilde{v}_t})}^{\text{TD error} \quad \delta_t}]} \end{equation}</script><p><strong>TD target</strong> : $\tilde{v}_t=r_{t+1}+\gamma v_{t}(s_{t+1})$<br>Key:The current estimated state value ($\tilde{v}_t$ or $\tilde{v}_t(s_t)$) is obtained  by using the next state value $v_t(s_{t+1})$  in time step $t+1$.</p>
<p><strong>TD error</strong> :$\delta_t=v_{t}(s_t)-\tilde{v}_t=v_{t}(s_t)-(r_{t+1}+\gamma v_{t}(s_{t+1}))$<br>Key:the error of the current state value $v_t(s_t)$ with the estimated state value ($\tilde{v}_t$ or $\tilde{v}_t(s_t)$) in time step $t$.</p>
<p>the TD error can be interpreted as the innovation, which indicates new information obtained from the experience sample $(s_t, r_{t+1}, s_{t+1})$. The fundamental idea of TD learning is to correct our current estimate of the state value based on the newly obtained information.<br><strong>Note</strong>: </p>
<ol>
<li>the new value $v_{t+1}(s_t)$ is closer to $\tilde{v}_t$ than the old value $v_t(s_t)$. Therefore, this algorithm mathematically drives $v_t(s_t)$ toward $\tilde{v}_t$. This is why $\tilde{v}_t$ is called the TD target.</li>
<li>TD error is called temporal-difference because $\delta_t$ reflects the discrepancy between two time steps $t$ and $t + 1$. In addition, the TD error is zero in the expectation sense when the state value estimate is accurate. So the TD error also reflects the discrepancy between the estimate $v_t$ and the true state value $v_{\pi}$.<h4 id="7-1-3-Compare-TD-Learning-with-MC-Learning"><a href="#7-1-3-Compare-TD-Learning-with-MC-Learning" class="headerlink" title="7.1.3 Compare TD Learning with MC Learning"></a>7.1.3 Compare TD Learning with MC Learning</h4>|                                                                                  TD learning                                                                                   |                                                                    MC learning                                                                    |<br>| :——————————————————————————————————————————————————————————————————————————————————————: | :———————————————————————————————————————————————————————————————————————-: |<br>|                                      <strong>Online</strong>: It can update the state/action values immediately after receiving an experience sample.                                       | <strong>Offline</strong>: It must wait until an episode has been completely collected. That is because it must calculate the discounted return of the episode. |<br>|                                                <strong>Continuing tasks</strong>: online, it can handle both episodic and continuing tasks.                                                 |           <strong>Episodic tasks</strong>:  offline, it can only handle episodic tasks where the episodes terminate after a finite number of steps.            |<br>| <strong>Bootstrapping</strong>: Because the update of a state/action value relies on the previous estimate of this value. As a result, TD learning requires an initial guess of the values. |                       <strong>Non-bootstrapping</strong>: Because it can directly estimate state/action values without initial guesses.                        |<br>|                                                     <strong>Low estimation variance</strong>: few Random variables but biased estimate                                                      |                                    <strong>High estimation variance</strong>: many Random variables but non-biased estimate                                    |</li>
</ol>
<h3 id="7-2-TD-learning-of-action-values-Sarsa"><a href="#7-2-TD-learning-of-action-values-Sarsa" class="headerlink" title="7.2 TD learning of action values : Sarsa"></a>7.2 TD learning of action values : <strong>Sarsa</strong></h3><p>The TD algorithm introduced in Section 7.1 can only estimate the state values of a given policy(merely used for policy evaluation). To find optimal policies, we still need to further calculate the action values and then conduct policy improvement.<strong> Sarsa</strong> can directly estimate action values. <em>Estimating action values is important because it can be combined with a policy improvement step to learn optimal policies.</em><br>Sarsa is an abbreviation for state-action-reward-state-action. because each iteration of the algorithm requires $(s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1})$.  The Sarsa algorithm was first proposed in <sup><a href="#fn_3" id="reffn_3">3</a></sup> and its name was coined by <sup><a href="#fn_4" id="reffn_4">4</a></sup>.</p>
<blockquote id="fn_3">
<sup>3</sup>. G. A. Rummery and M. Niranjan, On-line Q-learning using connectionist systems. Technical Report, Cambridge University, 1994.<a href="#reffn_3" title="Jump back to footnote [3] in the text."> ↩</a>
</blockquote>
<blockquote id="fn_4">
<sup>4</sup>. R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction (2nd Edition). MIT Press, 2018.<a href="#reffn_4" title="Jump back to footnote [4] in the text."> ↩</a>
</blockquote>
<h4 id="7-2-1-Algorithm-description"><a href="#7-2-1-Algorithm-description" class="headerlink" title="7.2.1 Algorithm description"></a>7.2.1 Algorithm description</h4><p>Sarsa can estimate the action values of a given policy using the following equations:<br>Suppose that we have some experience samples $(s_0,a_0, r_1, s_1,a_1,\dots, s_t,a_t, r_{t+1}, s_{t+1},a_{t+1},\dots)$ generated following .</p>
<script type="math/tex; mode=display">
\begin{equation}
\textcolor{blue}{q_{t+1}(s_t,a_t) =q_{t}(s_t,a_t)-\alpha_t(s_t,a_t)[q_{t}(s_t,a_t)-(r_{t+1}+\gamma q_{t}(s_{t+1},a_{t+1}))]}  , \end{equation}</script><script type="math/tex; mode=display">
\begin{equation}
\textcolor{blue}{q_{t+1}(s,a) =q_{t}(s,a) \quad \text{for  all} \quad (s,a) \neq (s_t,a_t)} \end{equation}</script><p>where $t = 0,1,2,\dots$ denotes the time step. Here, $q_t(s_t,a_t)$ is the estimate of q_\pi(s_t,a_t) at time $t$; $\alpha_t(s_t,a_t)$ is the learning rate for $(s_t,a_t)$ at time $t$.<br>Sarsa is a stochastic approximation algorithm for solving the Bellman equation of a given policy.</p>
<script type="math/tex; mode=display">
q_{\pi}(s,a) =\textcolor{blue}{\mathbb{E}[R+\gamma q_{\pi}(S',A')\mid s,a], \quad \text{for all} \quad (s,a)}</script><h4 id="7-2-2-Optimal-policy-learning-via-Sarsa"><a href="#7-2-2-Optimal-policy-learning-via-Sarsa" class="headerlink" title="7.2.2 Optimal policy learning via Sarsa"></a>7.2.2 Optimal policy learning via Sarsa</h4><p>In Algorithm 7.1,each iteration has two steps. The first step is to update the q-value of the visited state-action pair. The second step is to update the policy to an $\epsilon$-greedy one.we do not evaluate a given policy su fficiently well before updating the policy. This is based on the idea of <strong>generalized policy iteration</strong>.<br>Moreover, after the policy is updated, the policy is immediately used to generate the next experience sample. The policy here is $\epsilon$-greedy so that it is <strong>exploratory</strong>.<br>-1 The core idea is simple: that is to use an algorithm to solve the Bellman equation of a given policy.<br>-2 The complication emerges when we try to nd optimal policies and work efficiently.<br><img src="/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-05-22-16-27-35.png" alt="7.1"><br><img src="/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-05-22-16-36-18.png" alt="7.2"><br>the total reward of each episode increases gradually. That is because the initial policy is not good and hence negative rewards are frequently obtained. As the policy becomes better, the total reward increases.<br>Notably, the length of an episode may increase abruptly (e.g., the 460th episode) and the corresponding total reward also drops sharply. That is because the policy is-greedy, and there is a chance for it to take non-optimal actions. One way to resolve this problem is to use decaying $\epsilon$ whose value converges to zero gradually(Gradually reduce $\epsilon$ or deal with smooth).</p>
<h3 id="7-3-TD-learning-of-action-values-Expectded-Sarsa"><a href="#7-3-TD-learning-of-action-values-Expectded-Sarsa" class="headerlink" title="7.3 TD learning of action values : Expectded Sarsa"></a>7.3 TD learning of action values : Expectded Sarsa</h3><script type="math/tex; mode=display">
\begin{equation}
\textcolor{blue}{q_{t+1}(s_t,a_t) =q_{t}(s_t,a_t)-\alpha_t(s_t,a_t)[q_{t}(s_t,a_t)-(r_{t+1}+\gamma \mathbb{E}[q_{t}(s_{t+1},A)])]}  , \end{equation}</script><script type="math/tex; mode=display">
\begin{equation}
\textcolor{blue}{q_{t+1}(s,a) =q_{t}(s,a) \quad \text{for  all} \quad (s,a) \neq (s_t,a_t)} \end{equation}</script><p>where $\mathbb{E}[q_{t}(s_{t+1},A)]=\sum_a\pi_t(a \mid s_{t+1})q_t(s_{t+1},a)=v_t(s_{t+1})$.<br>Since the algorithm involves an expected value, it is called Expected Sarsa. Although calculating the expected value may increase the computational complexity slightly, it is beneficial in the sense that it reduces the estimation variances because it reduces the random variables in Sarsa from ${s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}}$ to ${s_t, a_t, r_{t+1}, s_{t+1}}$ .</p>
<h3 id="7-4-TD-learning-of-action-values-n-step-Sarsa"><a href="#7-4-TD-learning-of-action-values-n-step-Sarsa" class="headerlink" title="7.4 TD learning of action values : n-step Sarsa"></a>7.4 TD learning of action values : n-step Sarsa</h3><p>The n-step Sarsa algorithm is a generalization of the Sarsa algorithm. It will be shown that Sarsa and MC learning are two special cases of n-step Sarsa.<br><img src="/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-05-22-16-53-59.png" alt=""><br>In summary, n-step Sarsa is a more general algorithm because it becomes the (one step) Sarsa algorithm when $n = 1$ and the MC learning algorithm when $n = \infty$(by setting $\alpha_t = 1$).</p>
<script type="math/tex; mode=display">
\begin{equation}
\textcolor{blue}{q_{t+n}(s_t,a_t) =q_{t+n-1}(s_t,a_t)-\alpha_{t+n-1}(s_t,a_t)[q_{t+n-1}(s_t,a_t)-(r_{t+1}+\gamma r_{t+2}+\dots+ \gamma^n q_{t+n-1}(s_{t+n},a_{t+n})])}  , \end{equation}</script><p>where $q_{t+n}(s_t, a_t)$ is the estimate of $q_{\pi}(s_t, a_t)$ at time $t + n$.<br>In particular, if $n$ is selected as a large number, n-step Sarsa is close to MC learning: the estimate has a relatively high variance but a small bias. If $n$ is selected to be small, n-step Sarsa is close to Sarsa: the estimate has a relatively large bias but a low variance.</p>
<h3 id="7-5-TD-learning-of-optimal-action-values-Q-Learning"><a href="#7-5-TD-learning-of-optimal-action-values-Q-Learning" class="headerlink" title="7.5 TD learning of optimal action values : Q-Learning"></a>7.5 TD learning of optimal action values : <strong>Q-Learning</strong></h3><p>The other TD algorithms aim to solve the Bellman equation of a given policy, Q-learning aims to directly solve the Bellman optimality equation to obtain optimal policies. Recall that Sarsa can only estimate the action values of a given policy, and it must be combined with a policy improvement step to find optimal policies. By contrast, Q-learning can directly estimate optimal action values and find optimal policies.</p>
<h4 id="7-5-1-Algorithm-description"><a href="#7-5-1-Algorithm-description" class="headerlink" title="7.5.1 Algorithm description"></a>7.5.1 Algorithm description</h4><p>The Q-learning algorithm is:</p>
<script type="math/tex; mode=display">
\begin{equation}
\textcolor{blue}{q_{t+1}(s_t,a_t) =q_{t}(s_t,a_t)-\alpha_t(s_t,a_t)[q_{t}(s_t,a_t)-(r_{t+1}+\gamma \max_{a \in \mathbb{A}(s_{t+1})}q_{t}(s_{t+1},a_{t+1}))]}  , \end{equation}</script><script type="math/tex; mode=display">
\begin{equation}
\textcolor{blue}{q_{t+1}(s,a) =q_{t}(s,a) \quad \text{for  all} \quad (s,a) \neq (s_t,a_t)} \end{equation}</script><p>where $t = 0,1,2,\dots$ denotes the time step. Here, $q_t(s_t,a_t)$ is the estimate the optimal action value of (s_t,a_t) ; $\alpha_t(s_t,a_t)$ is the learning rate for $(s_t,a_t)$ at time $t$.</p>
<p>Q-learning is a stochastic approximation algorithm for solving the Bellman optimality equation expressed in terms of action values.</p>
<script type="math/tex; mode=display">
q_{\pi}(s,a) =\textcolor{blue}{\mathbb{E}[R_{t+1}+\gamma\max_{a} q(S_{t+1},a)\mid S_t=s,A_t=a], \quad \text{for all} \quad (s,a)}</script><p> Sarsa requires $(r_{t+1}, s_{t+1}, a_{t+1})$ in every iteration, whereas Q-learning merely requires $(r_{t+1}, s_{t+1})$.</p>
<h4 id="7-5-2-Off-policy-and-on-policy"><a href="#7-5-2-Off-policy-and-on-policy" class="headerlink" title="7.5.2 Off-policy and on-policy"></a>7.5.2 Off-policy and on-policy</h4><p>Q-learning slightly special compared to the other TD algorithms is that Q-learning is off-policy while the others are on-policy.<br>Two policies exist in any reinforcement learning task: a <em>behavior policy</em> and a <em>target policy</em>. </p>
<ul>
<li><strong>The behavior policy</strong> is the one used to generate experience samples.</li>
<li><strong>The target policy</strong> is the one that is constantly updated to converge to an optimal policy. </li>
</ul>
<p>When the behavior policy is the same as the target policy, such a learning process is called on-policy. Otherwise, when they are different, the learning process is called off-policy.</p>
<p>The advantage of off-policy learning is that it can learn optimal policies based on the experience samples generated by other policies, which may be, for example, a policy executed by a human operator. As an important case, <strong>the behavior policy can be selected to be exploratory.</strong> For example, if we would like to estimate the action values of all state action pairs, we must generate episodes visiting every state-action pair sufficiently many times. Although Sarsa uses $\epsilon$-greedy policies to maintain certain exploration abilities, the value of is usually small and hence <strong>the exploration ability is limited</strong>. By contrast, <u>if we can use a policy with a strong exploration ability to generate episodes and then use off-policy learning to learn optimal policies, the learning efficiency would be significantly increased.</u></p>
<p><strong>Sarsa is on-policy.</strong><br>we need samples generated by $\pi$. Therefore, $\pi$ is the behavior policy. The second step is to obtain an improved policy based on the estimated values of $\pi$. As a result, $\pi$ is the target policy that is constantly updated and eventually converges to an optimal policy. Therefore, the behavior policy and the target policy are the same.</p>
<p><strong>Monte Carlo learning is on-policy.</strong><br>A policy is used to generate samples, which is further used to estimate the action values of the policy. Based on the action values, we can improve the policy.</p>
<p><strong>Q-learning is off-policy.</strong><br>The fundamental reason is that Q-learning is an algorithm for solving the Bellman optimality equation that can directly generate the optimal values and optimal policies.we need not to evaluate the associated policy.<br>Q-learning experience sample requires $(s_t, a_t, r_{t+1},s_{t+1})$. If $(s_t, a_t)$ is given, then $r_{t+1}$ and $s_{t+1}$ do not depend on any policy!<br>The behavior policy to generate at from $s_t$ can be anything. The target policy will converge to the optimal policy.<br><strong>Note</strong>:  on-policy/off-policy is online/offline learning?<br>Online learning refers to the case where the value and the policy are updated once an experience sample is obtained. Offline learning refers to the case where the update can only be done after all experience samples have been collected(episodes).</p>
<h4 id="7-5-3-Implementation-and-Illustrative-examples"><a href="#7-5-3-Implementation-and-Illustrative-examples" class="headerlink" title="7.5.3 Implementation and Illustrative examples"></a>7.5.3 Implementation and Illustrative examples</h4><p><img src="/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-05-22-19-40-40.png" alt=""></p>
<p><img src="/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-05-22-19-41-00.png" alt=""><br>The on-policy version of Q-learning is shown in Algorithm 7.2. This implementation is similar to the Sarsa one in Algorithm 7.1. Here, the behavior policy is the same as the target policy, which is an-greedy policy.</p>
<p>The off-policy version is shown in Algorithm 7.3. The behavior policy $\pi_b$ can be any policy as long as it can generate sufficient experience samples. <strong>It is usually favorable when $\pi_b$ is exploratory</strong>. Here, the target policy $\pi_T$ is greedy rather than $\epsilon$-greedy <strong>since it is not used to generate samples and hence is not required to be exploratory.</strong> Moreover, the off-policy version of Q-learning presented here is implemented offline: all the experience samples are collected first and then processed. It can be modified to become online: the value and policy can be updated immediately once a sample is received.</p>
<p><img src="/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-05-22-19-49-51.png" alt=""><br><img src="/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-05-22-19-50-11.png" alt=""><br><strong>Ground truth</strong>: To verify the effectiveness of Q-learning, we first need to know the ground truth of the optimal policies and optimal state values. Here, the ground truth is obtained by the model-based policy iteration algorithm. The ground truth is given in Figures 7.4(a) and (b).</p>
<p><strong>Experience samples</strong>: The behavior policy has a uniform distribution: the probability of taking any action at any state is 0.2 (Figure 7.4(c)). A single episode with 100,000 steps is generated (Figure 7.4(d)). Due to the good exploration ability of the behavior policy, the episode visits every state-action pair many times.</p>
<p><strong>Learned results</strong>: Based on the episode generated by the behavior policy, the final target policy learned by Q-learning is shown in Figure 7.4(e). This policy is optimal because the estimated state value error (root-mean-square error) converges to zero as shown in Figure 7.4(f). In addition, one may notice that the learned optimal policy is not exactly the same as that in Figure 7.4(a). In fact, there exist multiple optimal policies that have the same optimal state values.</p>
<p><strong>Different initial values</strong>: Since Q-learning bootstraps, the performance of the algorithm depends on the initial guess for the action values. As shown in Figure 7.4(g), <u>when the initial guess is close to the true value, the estimate converges within approximately 10,000 steps.</u> Otherwise, the convergence requires more steps (Figure 7.4(h)). Nevertheless, these figures demonstrate that Q-learning can still converge rapidly even though the initial value is not accurate.</p>
<p><strong>Different behavior policies</strong>: <u>When the behavior policy is not exploratory, the learning performance drops significantly.</u> For example, consider the behavior policies shown in Figure 7.5. They are $\epsilon$-greedy policies with = 0.5 or 0.1 (the uniform policy in Figure 7.4(c) can be viewed as $\epsilon$-greedy with = 1). It is shown that, when decreases from 1 to 0.5 and then to 0.1, the learning speed drops significantly. That is because the exploration ability of the policy is weak and hence the experience samples are insufficient.</p>
<p><img src="/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-05-22-19-55-35.png" alt=""></p>
<p><img src="/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-05-22-19-55-45.png" alt=""></p>
<h3 id="7-6-A-unified-viewpoint"><a href="#7-6-A-unified-viewpoint" class="headerlink" title="7.6 A unified viewpoint"></a>7.6 A unified viewpoint</h3><p>In this section, we introduce a uni ed framework to accommodate all these<br> algorithms and MC learning.<br>In particular, the TD algorithms (for action value estimation) can be expressed in a unified expression:</p>
<script type="math/tex; mode=display">
\begin{equation}
\textcolor{blue}{q_{t+1}(s_t,a_t) =q_{t}(s_t,a_t)-\alpha_t(s_t,a_t)[q_{t}(s_t,a_t)-\bar{q_t}]}  , \end{equation}</script><p>where $\bar{q_t}$ is the TD target. Different TD algorithms have different $\bar{q_t}$.<br><img src="/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/2024-05-22-20-00-27.png" alt=""><br>As can be seen, all of the algorithms aim to solve the Bellman equation except Q-learning, which aims to solve the Bellman optimality equation.</p>
<h3 id="7-7-Summary"><a href="#7-7-Summary" class="headerlink" title="7.7 Summary"></a>7.7 Summary</h3><p>we introduced include Sarsa, n-step Sarsa, and Q-learning. All these algorithms can be viewed as stochastic approximation algorithms for solving Bellman or Bellman optimality equations.<br>The TD algorithms introduced in this chapter, except Q-learning, are used to evaluate a given policy. That is to estimate a given policy’s state/action values from some experience samples. Together with policy improvement, they can be used to learn optimal policies. Moreover, these algorithms are on-policy: the target policy is used as the behavior policy to generate experience samples.<br>Q-learning is slightly special compared to the other TD algorithms in the sense that it is o-policy. The target policy can be different from the behavior policy in Q-learning. The fundamental reason why Q-learning is o-policy is that Q-learning aims to solve the Bellman optimality equation rather than the Bellman equation of a given policy.<br>It is worth mentioning that there are some methods that can convert an on-policy algorithm to be off-policy. Importance sampling is a widely used one and will be introduced in Chapter 10. Finally, there are some variants and extensions of the TD algorithms introduced in this chapter <sup><a href="#fn_5" id="reffn_5">5</a></sup>. For example, the $TD(\lambda)$ method provides a more general and uni ed framework for TD learning. More information can be found in paper<sup><a href="#fn_6" id="reffn_6">6</a></sup>.</p>
<p><sup><a href="#fn_5" id="reffn_5">5</a></sup>:H. Van Hasselt, A. Guez, and D. Silver, Deep reinforcement learning with double Q-learning, in AAAI Conference on Artificial Intelligence, vol. 30, 2016.<br><sup><a href="#fn_6" id="reffn_6">6</a></sup>:R. S. Sutton and A. G. Barto, Reinforcement learning: An introduction (2nd Edition). MIT Press, 2018.</p>
<p>Q: What does the term learning in TD learning mean?<br> A: From a mathematical point of view, learning simply means estimation . That is to estimate state/action values from some samples and then obtain policies based on the estimated values.</p>
<p>Q: Why does Sarsa update policies to be-greedy?<br>A: That is because the policy is also used to generate samples for value estimation. Hence, it should be exploratory to generate sufficient experience samples.</p>
<p>Q: While Theorems 7.1 and 7.2 require that the learning rate t converges to zero gradually, why is it often set to be a small constant in practice?<br>A: The fundamental reason is that the policy to be evaluated keeps changing (or called nonstationary). In particular, a TD learning algorithm like Sarsa aims to estimate the action values of a given policy. If the policy is fixed, using a decaying learning rate is acceptable. However, in the optimal policy learning process, the policy that Sarsa aims to evaluate keeps changing after every iteration. We need a constant learning rate in this case; otherwise, a decaying learning rate may be too small to effectively evaluate policies. Although a drawback of constant learning rates is that the value estimate may fluctuate eventually, the fluctuation is negatable as long as the constant learning rate is sufficiently small.</p>
<p>Q: Why does the o-policy version of Q-learning update policies to be greedy instead of $\epsilon$-greedy?<br>A: That is because the target policy is not required to generate experience samples. Hence, it is not required to be exploratory.</p>
<h2 id="Approximation-Q-learning-Steps"><a href="#Approximation-Q-learning-Steps" class="headerlink" title="Approximation (Q-learning Steps)"></a>Approximation (Q-learning Steps)</h2><p>The Q-learning algorithm uses a Q-table of State-Action Values (also called Q-values). This Q-table has a row for each state and a column for each action. Each cell contains the estimated Q-value for the corresponding state-action pair.<br>Steps:</p>
<p>1.We start by initializing all the Q-values to zero. As the agent interacts with the environment and gets feedback, the algorithm iteratively improves these Q-values until they converge to the Optimal Q-values. It updates them using the Bellman equation.</p>
<p>2.Then we use the ε-greedy policy to pick the current action from the current state.</p>
<p>3.Execute this action tointhe environment to execute, and gets feedback in the form of a reward and the next state.</p>
<p>4.Now, for step #4, the algorithm has to use a Q-value from the next state in order to update its estimated Q-value (Q1) for the current state and selected action.</p>
<p>And here is where the Q-Learning algorithm uses its clever trick. The next state has several actions, so which Q-value does it use? It uses the action (a4) from the next state which has the highest Q-value (Q4). What is critical to note is that it treats this action as a target action to be used only for the update to Q1. It is not necessarily the action that it will actually end up executing from the next state when it reaches the next time step. </p>
<script type="math/tex; mode=display">Q(s_t, a_t) \gets (1-\alpha)Q(s_t, a_t) + \alpha \left( r_t + (1-d_t) \gamma \max_{a_{t+1}} Q(s_{t+1}, a_{t+1}) \right)</script><p> Here $d_t$ is the done flag. If $d_t=1$, then it means the state $s_{t+1}$ is a terminal state. Adding the $\left(1-d_t\right)$ term implies that the discounted return $G_t$ is just going to be $r_t$, since all rewards after that are going to be 0 . In some situations this can help accelerate learning.</p>
<p>In other words, there are two actions involved:</p>
<ul>
<li>Current action — the action from the current state that is actually executed in the environment, and whose Q-value is updated.</li>
<li>Target action — has the highest Q-value from the next state, and used to update the current action’s Q value.</li>
</ul>

    </div>

    
    
    

<div> 
<div class="my_post_copyright">
  <script src="//cdn.bootcss.com/clipboard.js/1.5.10/clipboard.min.js"></script>

  <!-- JS库 sweetalert 可修改路径 -->
  <script type="text/javascript" src="http://jslibs.wuxubj.cn/sweetalert_mini/jquery-1.7.1.min.js"></script>
  <script src="http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.min.js"></script>
  <link rel="stylesheet" type="text/css" href="http://jslibs.wuxubj.cn/sweetalert_mini/sweetalert.mini.css">

  <p><span>Article:</span>Mathematical Foundation of Reinforcement Learning-TD algorithms</a></p>
  <p><span>Author:</span>Ming Huang</a></p>
  <p><span>Release time：</span>2024-08-09   11:39:09</p>
  <p><span>Updat time:</span>2024-10-23   11:56:44</p>
  <p><span>Original link:</span><a href="/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/" title="Mathematical Foundation of Reinforcement Learning-TD algorithms">https://www.huangm.cn/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/</a>
    <span class="copy-path"  title="Click to copy the article link"><i class="fa fa-clipboard" data-clipboard-text="https://www.huangm.cn/2024/08/09/Mathmatical-Foundation-of-Reinforcement-Learning-TD-algorithms/"  aria-label="Copy successful!"></i></span>
  </p>
  <p><span>license agreement:</span><i class="fa fa-creative-commons" /></i> <a rel="license" href="https://creativecommons.org/licenses/by-nc-nd/4.0/" target="_blank" title="Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0)"> (CC BY-NC-ND 4.0)</a> Please keep the original link and author when reprinting.</p>
</div>
<script>
    var clipboard = new Clipboard('.fa-clipboard');
    clipboard.on('success', $(function(){
      $(".fa-clipboard").click(function(){
        swal({
          title: "",
          text: 'Copy successful',
          html: false,
          timer: 500,
          showConfirmButton: false
        });
      });
    }));
</script>
 </div>

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/08/08/Mathmatical-Foundation-of-Reinforcement-Learning-Algorithms-model-based-non-incremental/" rel="prev" title="Mathematical Foundation of Reinforcement Learning - Algorithms (model-based & non-incremental)">
      <i class="fa fa-chevron-left"></i> Mathematical Foundation of Reinforcement Learning - Algorithms (model-based & non-incremental)
    </a></div>
      <div class="post-nav-item">
    <a href="/2024/08/10/Mathmatical-Foundation-of-Reinforcement-Learning-DQN-algorithm/" rel="next" title="Mathematical Foundation of Reinforcement Learning-DQN algorithm">
      Mathematical Foundation of Reinforcement Learning-DQN algorithm <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#6-Stochastic-Approximation"><span class="nav-text">6 Stochastic Approximation</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#6-1-Mean-estimation"><span class="nav-text">6.1 Mean estimation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-2-Robbins-Monro-RM-algorithm"><span class="nav-text">6.2 Robbins-Monro (RM) algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-1-Problem-statement"><span class="nav-text">6.2.1 Problem statement</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-2-An-illustrative-example"><span class="nav-text">6.2.2 An illustrative example</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-3-RM-algorithm%E2%80%99s-proof"><span class="nav-text">6.2.3 RM algorithm’s proof</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-2-4-Mean-estimation-is-a-special-example-of-RM-algorithm"><span class="nav-text">6.2.4 Mean estimation is a special example of RM algorithm</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-3-Stochastic-gradient-descent"><span class="nav-text">6.3 Stochastic gradient descent</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#6-3-1-Mean-estimation-is-a-special-example-of-SGD"><span class="nav-text">6.3.1 Mean estimation is a special example of SGD</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-3-2-SGD-is-a-special-example-of-RM-algorithm-Convergence-analysis"><span class="nav-text">6.3.2 SGD is a special example of RM algorithm (Convergence analysis)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-3-3-Convergence-pattern"><span class="nav-text">6.3.3 Convergence pattern</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-3-4-BGD-SGD-and-mini-batch-GD"><span class="nav-text">6.3.4 BGD, SGD, and mini-batch GD</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#6-4-Summary"><span class="nav-text">6.4 Summary</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#7-Temporal-Difference-Methods"><span class="nav-text">7 Temporal-Difference Methods</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#7-1-TD-learning-of-state-values"><span class="nav-text">7.1 TD learning of state values</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-1-1-Algorithm-description"><span class="nav-text">7.1.1 Algorithm description</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-1-2-Property-analysis-TD-target-and-TD-error"><span class="nav-text">7.1.2 Property analysis (TD target and TD error)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-1-3-Compare-TD-Learning-with-MC-Learning"><span class="nav-text">7.1.3 Compare TD Learning with MC Learning</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-2-TD-learning-of-action-values-Sarsa"><span class="nav-text">7.2 TD learning of action values : Sarsa</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-2-1-Algorithm-description"><span class="nav-text">7.2.1 Algorithm description</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-2-2-Optimal-policy-learning-via-Sarsa"><span class="nav-text">7.2.2 Optimal policy learning via Sarsa</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-3-TD-learning-of-action-values-Expectded-Sarsa"><span class="nav-text">7.3 TD learning of action values : Expectded Sarsa</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-4-TD-learning-of-action-values-n-step-Sarsa"><span class="nav-text">7.4 TD learning of action values : n-step Sarsa</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-5-TD-learning-of-optimal-action-values-Q-Learning"><span class="nav-text">7.5 TD learning of optimal action values : Q-Learning</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#7-5-1-Algorithm-description"><span class="nav-text">7.5.1 Algorithm description</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-5-2-Off-policy-and-on-policy"><span class="nav-text">7.5.2 Off-policy and on-policy</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#7-5-3-Implementation-and-Illustrative-examples"><span class="nav-text">7.5.3 Implementation and Illustrative examples</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-6-A-unified-viewpoint"><span class="nav-text">7.6 A unified viewpoint</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#7-7-Summary"><span class="nav-text">7.7 Summary</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Approximation-Q-learning-Steps"><span class="nav-text">Approximation (Q-learning Steps)</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Ming Huang"
      src="/images/pic.png">
  <p class="site-author-name" itemprop="name">Ming Huang</p>
  <div class="site-description" itemprop="description">Let’s do something extraordinary together.</div>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://bit.edu.cn/" title="Beijing Institute of Technology → https:&#x2F;&#x2F;bit.edu.cn&#x2F;" rel="noopener" target="_blank"><i class="fa fa-university fa-fw"></i>Beijing Institute of Technology</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://scholar.google.com.hk/citations?user=nDvJ6BUAAAAJ&hl=en/" title="Google Scholar → https:&#x2F;&#x2F;scholar.google.com.hk&#x2F;citations?user&#x3D;nDvJ6BUAAAAJ&amp;hl&#x3D;en&#x2F;" rel="noopener" target="_blank"><i class="fa fa-graduation-cap fa-fw"></i>Google Scholar</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:huangming98@163.com" title="E-Mail → mailto:huangming98@163.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://orcid.org/0000-0001-9146-4721" title="ORCID → https:&#x2F;&#x2F;orcid.org&#x2F;0000-0001-9146-4721" rel="noopener" target="_blank"><i class="fa fa-id-card fa-fw"></i>ORCID</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://github.com/huangming98" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;huangming98" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>




<script src="/js/switch_language.js"></script>
      </div>

<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="https://music.163.com/outchain/player?type=2&id=1941963749&auto=1&height=66"></iframe>

    </div>
  </aside>

  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2022.09— 
  <span itemprop="copyrightYear">2026.01</span>
  
   
</div>
  <div class="powered-by"><p style="color:rgb(0,0,255,0.8);font-size:1.2em;font-weight:bold;font-family:Arial">Do not be afraid of making mistakes, only of missing opportunities!</p>
  </div>

<!-- Insert clustrmaps.com -->
<!-- <a target="_blank" rel="noopener" href='https://clustrmaps.com/site/1bqbj' ><img src='//clustrmaps.com/map_v2.png?cl=555555&w=350&t=m&d=b6-cOzsyxmN07VPEXlJO1PaWKRLFhE6feh1y5oiN1Hw&co=f5f5f5&ct=555555'/></a> -->


<!-- 网站运行时间的设置 -->
<span id="timeDate">载入天数...</span>
<span id="times">载入时分秒...</span>
<script>
    var now = new Date();
    function createtime() {
        var grt= new Date("03/01/2023 10:00:00"); //此处修改你的建站时间或者网站上线时间
        now.setTime(now.getTime()+250);
        days = (now - grt ) / 1000 / 60 / 60 / 24; dnum = Math.floor(days);
        hours = (now - grt ) / 1000 / 60 / 60 - (24 * dnum); hnum = Math.floor(hours);
        if(String(hnum).length ==1 ){hnum = "0" + hnum;} minutes = (now - grt ) / 1000 /60 - (24 * 60 * dnum) - (60 * hnum);
        mnum = Math.floor(minutes); if(String(mnum).length ==1 ){mnum = "0" + mnum;}
        seconds = (now - grt ) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
        snum = Math.round(seconds); if(String(snum).length ==1 ){snum = "0" + snum;}
        document.getElementById("timeDate").innerHTML = "<br />This website is as worthless as hundreds of daily chores, but I still hope it will be helpful to you! <br /> It's been running quietly for  "+dnum+" days ";
        document.getElementById("times").innerHTML = hnum + " hours " + mnum + " minutes " + snum + " seconds.";
    } 
setInterval("createtime()",250);
</script>



        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item" id="busuanzi_container_site_uv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-user"></i>
      </span>
      <span class="site-uv" title="Total Visitors">
        <span id="busuanzi_value_site_uv"></span>
      </span>
    </span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item" id="busuanzi_container_site_pv" style="display: none;">
      <span class="post-meta-item-icon">
        <i class="fa fa-eye"></i>
      </span>
      <span class="site-pv" title="Total Views">
        <span id="busuanzi_value_site_pv"></span>
      </span>
    </span>
</div>








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/medium-zoom@1/dist/medium-zoom.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
